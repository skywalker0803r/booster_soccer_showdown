# -*- coding: utf-8 -*-
import numpy as np

def calculate_potential(state_45: np.ndarray) -> float:
    """
    è¨ˆç®—åŸºæ–¼ä¿æŒç›´ç«‹ã€ç«™ç©©å’Œçƒæ§åˆ¶çš„å‹¢èƒ½å‡½æ•¸ Phi(s)ã€‚
    
    45ç¶­ç‹€æ…‹çµæ§‹ (ä¾†è‡ª utils.py Preprocessor.modify_state):
    [0:12]   Joint Positions (12)
    [12:24]  Joint Velocities (12) 
    [24:27]  Projected Gravity (3)     â† ç›´ç«‹æ§åˆ¶
    [27:30]  Robot Gyro (3)           â† è§’é€Ÿåº¦ç©©å®š
    [30:33]  Robot Accelerometer (3)   â† åŠ é€Ÿåº¦ç©©å®š
    [33:36]  Robot Velocimeter (3)     â† ç·šé€Ÿåº¦æ§åˆ¶
    [36:39]  Ball Position (3)         â† çƒè·é›¢æ§åˆ¶
    [39:42]  Ball Velocity (3)         â† çƒé€Ÿåº¦æ§åˆ¶
    [42:45]  Task One-Hot (3)
    
    è¼¸å‡º: æµ®é»æ•¸å‹¢èƒ½å€¼ [-1.0, 1.0]
    """
    
    # 1. ç›´ç«‹ç©©å®šå‹¢èƒ½ (Projected Gravity)
    proj_grav = state_45[24:27]  # âœ… ä¿®æ­£ç´¢å¼•
    target_grav = np.array([0.0, 0.0, -1.0])
    # é»ç©: è¶Šç›´ç«‹è¶Šæ¥è¿‘1.0
    grav_potential = np.dot(proj_grav, target_grav) 
    
    # 2. ç·šé€Ÿåº¦ç©©å®šå‹¢èƒ½ (Robot Velocimeter) 
    robot_velo = state_45[33:36]  # âœ… ä¿®æ­£ç´¢å¼•
    # æ‡²ç½°éåº¦ç§»å‹•ï¼Œé¼“å‹µç©©å®šæ§åˆ¶
    velo_penalty = -0.03 * np.sum(robot_velo**2)  # èª¿æ•´ä¿‚æ•¸
    
    # 3. ğŸ†• è§’é€Ÿåº¦ç©©å®šå‹¢èƒ½ (Robot Gyro)
    robot_gyro = state_45[27:30]  
    # æ‡²ç½°éåº¦æ—‹è½‰ï¼Œé¼“å‹µå¹³è¡¡
    gyro_penalty = -0.02 * np.sum(robot_gyro**2)
    
    # 4. ğŸ†• çƒè·é›¢æ§åˆ¶å‹¢èƒ½ (Ball Position)
    ball_pos = state_45[36:39]
    ball_distance = np.linalg.norm(ball_pos[:2])  # åªè€ƒæ…®xyè·é›¢
    # é¼“å‹µæ¥è¿‘çƒï¼Œä½†ä¸è¦å¤ªè¿‘
    optimal_distance = 1.0  # æœ€ä½³è¸¢çƒè·é›¢
    distance_reward = -0.1 * abs(ball_distance - optimal_distance)
    
    # 5. ğŸ†• é—œç¯€é€Ÿåº¦æ‡²ç½° (Joint Velocities)
    joint_velo = state_45[12:24]
    # é¿å…é—œç¯€åŠ‡çƒˆé‹å‹•
    joint_penalty = -0.01 * np.sum(joint_velo**2)
    
    
    # 6. ç¸½å‹¢èƒ½çµ„åˆ - åˆ†å±¤æ¬Šé‡è¨­è¨ˆ
    stability_component = grav_potential + velo_penalty + gyro_penalty + joint_penalty  # ç©©å®šæ€§
    ball_component = distance_reward  # çƒæ§åˆ¶
    
    # ğŸ¯ éšæ®µæ€§æ¬Šé‡: å…ˆå­¸ç©©å®šï¼Œå†å­¸çƒæ§åˆ¶
    stability_weight = 0.7  # ç©©å®šæ€§ä½”70%
    ball_weight = 0.3      # çƒæ§åˆ¶ä½”30%
    
    total_potential = stability_weight * stability_component + ball_weight * ball_component
    
    # 7. è¦æ¨¡èª¿æ•´ - åŒ¹é…åŸå§‹çå‹µè¦æ¨¡
    K = 0.4  # èª¿æ•´ä¿‚æ•¸ï¼Œé¿å…éåº¦å½±éŸ¿åŸå§‹çå‹µ
    scaled_potential = K * total_potential
    
    # 8. æœ€çµ‚å‹¢èƒ½è£å‰ª - é˜²æ­¢æ•¸å€¼æº¢å‡º
    final_potential = np.clip(scaled_potential, -1.0, 0.8)
    
    return final_potential


def create_pbrs_wrapper(env, gamma=0.99, debug=False):
    """
    å‰µå»º PBRS åŒ…è£å™¨çš„ä¾¿åˆ©å‡½æ•¸
    
    Args:
        env: åŸå§‹ç’°å¢ƒ
        gamma: æŠ˜æ‰£å› å­
        debug: æ˜¯å¦è¼¸å‡ºèª¿è©¦ä¿¡æ¯
    
    Returns:
        åŒ…è£å¾Œçš„ç’°å¢ƒ
    """
    return PBRSWrapper(env, gamma=gamma, debug=debug)


class PBRSWrapper:
    """
    PBRS (Potential-Based Reward Shaping) ç’°å¢ƒåŒ…è£å™¨
    
    æ ¹æ“š Ng, Harada & Russell (1999) çš„ç†è«–ï¼Œ
    ä½¿ç”¨å‹¢èƒ½å‡½æ•¸é€²è¡Œçå‹µå¡‘å½¢ï¼Œä¿è­‰æœ€å„ªç­–ç•¥ä¸è®Šæ€§
    """
    
    def __init__(self, env, gamma=0.99, debug=False):
        self.env = env
        self.gamma = gamma
        self.debug = debug
        self.prev_potential = 0.0
        self.step_count = 0
        self.total_shaped_reward = 0.0
        
    def reset(self, **kwargs):
        """é‡ç½®ç’°å¢ƒ"""
        obs, info = self.env.reset(**kwargs)
        
        # ä½¿ç”¨ Preprocessor è™•ç†è§€æ¸¬
        from utils import Preprocessor
        preprocessor = Preprocessor()
        processed_obs = preprocessor.modify_state(obs, info)
        
        # è¨ˆç®—åˆå§‹å‹¢èƒ½
        self.prev_potential = calculate_potential(processed_obs[0])
        self.step_count = 0
        self.total_shaped_reward = 0.0
        
        if self.debug:
            print(f"ğŸ”„ PBRS Reset - Initial potential: {self.prev_potential:.3f}")
            
        return obs, info
    
    def step(self, action):
        """ç’°å¢ƒæ­¥é€²"""
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # è™•ç†è§€æ¸¬
        from utils import Preprocessor
        preprocessor = Preprocessor()
        processed_obs = preprocessor.modify_state(obs, info)
        
        # è¨ˆç®—æ–°å‹¢èƒ½
        current_potential = calculate_potential(processed_obs[0])
        
        # PBRS çå‹µå¡‘å½¢: R' = R + Î³*Î¦(s') - Î¦(s)
        if terminated or truncated:
            # EpisodeçµæŸæ™‚ï¼ŒÎ¦(s') = 0
            shaped_reward = reward - self.prev_potential
        else:
            shaped_reward = reward + self.gamma * current_potential - self.prev_potential
        
        # æ›´æ–°ç‹€æ…‹
        self.prev_potential = current_potential
        self.step_count += 1
        self.total_shaped_reward += (shaped_reward - reward)
        
        if self.debug and self.step_count % 100 == 0:
            print(f"ğŸ“Š Step {self.step_count}: Original={reward:.3f}, "
                  f"Shaped={shaped_reward:.3f}, Potential={current_potential:.3f}")
        
        return obs, shaped_reward, terminated, truncated, info
    
    def __getattr__(self, name):
        """ä»£ç†åˆ°åŸå§‹ç’°å¢ƒ"""
        return getattr(self.env, name)