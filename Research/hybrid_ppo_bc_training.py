"""
æ··åˆè¨“ç·´ç³»çµ±ï¼šçµåˆè¡Œç‚ºå…‹éš†é è¨“ç·´ + PPO-CMAåœ¨ç·šå¾®èª¿
æ•´åˆå°ˆå®¶æ•¸æ“šé è¨“ç·´èˆ‡æ‚¨ç¾æœ‰çš„PPO-CMAç³»çµ±
"""

import numpy as np
import torch
import torch.nn as nn
import copy
from sai_rl import SAIClient 
from ppo_cma_model import PPOCMA
from utils import Preprocessor
from logger import TensorBoardLogger
from curiosity_module import CuriosityDrivenExploration
import sys
sys.path.append('..')
from llm_coach import LLMCoach
from reward_shaper import RewardShaper

class ExpertDataConverter:
    """å°‡imitation learningæ•¸æ“šè½‰æ›ç‚ºResearchç³»çµ±æ ¼å¼"""
    
    def __init__(self):
        self.il_preprocessor = self._create_il_preprocessor()
        self.research_preprocessor = Preprocessor()
    
    def _create_il_preprocessor(self):
        """å‰µå»ºILé è™•ç†å™¨"""
        # å°å…¥ILç³»çµ±çš„é è™•ç†å™¨
        import sys
        sys.path.append('../imitation_learning/scripts')
        from preprocessor import Preprocessor as ILPreprocessor
        return ILPreprocessor()
    
    def convert_observations(self, il_observations):
        """
        å°‡89ç¶­ILè§€æ¸¬è½‰æ›ç‚º45ç¶­Researchæ ¼å¼
        
        ILæ ¼å¼(89ç¶­): [æ©Ÿå™¨äººç‹€æ…‹42 + çƒä¿¡æ¯6 + ç›®æ¨™ä¿¡æ¯38 + ä»»å‹™ç·¨ç¢¼3]
        Researchæ ¼å¼(45ç¶­): [é—œç¯€12 + é€Ÿåº¦12 + é‡åŠ›3 + å‚³æ„Ÿå™¨9 + çƒ6 + ä»»å‹™3]
        """
        converted_obs = []
        
        for obs in il_observations:
            # å¾ILè§€æ¸¬ä¸­æå–åŸºç¤ä¿¡æ¯
            robot_qpos = obs[:12]      # é—œç¯€ä½ç½® 
            robot_qvel = obs[12:24]    # é—œç¯€é€Ÿåº¦
            
            # æå–å…¶ä»–å¿…è¦ä¿¡æ¯ï¼ˆéœ€è¦é‡æ§‹æˆ–è¿‘ä¼¼ï¼‰
            # ç”±æ–¼ILé è™•ç†å™¨åŒ…å«æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å€‘éœ€è¦é¸æ“‡æ€§æå–
            proj_gravity = obs[24:27]   # å‡è¨­ä½ç½®3-6æ˜¯é‡åŠ›æŠ•å½±
            ball_pos = obs[30:33]       # çƒçš„ç›¸å°ä½ç½®
            ball_vel = obs[33:36]       # çƒçš„é€Ÿåº¦
            task_onehot = obs[-3:]      # ä»»å‹™ç·¨ç¢¼
            
            # æ§‹é€ 45ç¶­å‘é‡ (éœ€è¦è£œå……ç¼ºå¤±çš„å‚³æ„Ÿå™¨æ•¸æ“š)
            # å°æ–¼ç¼ºå¤±çš„å‚³æ„Ÿå™¨æ•¸æ“šï¼Œä½¿ç”¨é›¶å¡«å……æˆ–å¾ç¾æœ‰æ•¸æ“šæ¨æ–·
            gyro = np.zeros(3)         # é™€èºå„€æ•¸æ“š - éœ€è¦å¾åŸå§‹ç’°å¢ƒç²å–
            accel = np.zeros(3)        # åŠ é€Ÿåº¦è¨ˆæ•¸æ“š
            velo = np.zeros(3)         # é€Ÿåº¦è¨ˆæ•¸æ“š
            
            research_obs = np.concatenate([
                robot_qpos,    # 12ç¶­
                robot_qvel,    # 12ç¶­
                proj_gravity,  # 3ç¶­
                gyro,          # 3ç¶­
                accel,         # 3ç¶­  
                velo,          # 3ç¶­
                ball_pos,      # 3ç¶­
                ball_vel,      # 3ç¶­
                task_onehot    # 3ç¶­
            ])
            
            converted_obs.append(research_obs)
            
        return np.array(converted_obs)

class BehavioralCloningPretrainer:
    """è¡Œç‚ºå…‹éš†é è¨“ç·´å™¨"""
    
    def __init__(self, ppo_agent, expert_data_path):
        self.ppo_agent = ppo_agent
        self.expert_data = self._load_expert_data(expert_data_path)
        self.converter = ExpertDataConverter()
        self.bc_loss_fn = nn.MSELoss()
        
        # å‰µå»ºBCå°ˆç”¨å„ªåŒ–å™¨
        self.bc_optimizer = torch.optim.Adam(
            ppo_agent.actor.parameters(), 
            lr=1e-4  # BCå­¸ç¿’ç‡
        )
        
    def _load_expert_data(self, data_path):
        """è¼‰å…¥ä¸¦è½‰æ›å°ˆå®¶æ•¸æ“š"""
        print(f"ğŸ“š è¼‰å…¥å°ˆå®¶æ•¸æ“š: {data_path}")
        data = np.load(data_path, allow_pickle=True)
        
        # è½‰æ›è§€æ¸¬æ ¼å¼
        il_observations = data['observations']
        converted_obs = self.converter.convert_observations(il_observations)
        
        expert_actions = np.array(data['actions'])
        
        print(f"âœ… å°ˆå®¶æ•¸æ“šè¼‰å…¥å®Œæˆ:")
        print(f"   è§€æ¸¬æ•¸: {len(converted_obs)} (å¾89ç¶­è½‰æ›ç‚º45ç¶­)")
        print(f"   å‹•ä½œæ•¸: {len(expert_actions)}")
        print(f"   Episodes: {np.sum(data['done'])}")
        
        return {
            'observations': converted_obs,
            'actions': expert_actions,
            'done': data['done']
        }
    
    def pretrain(self, epochs=100, batch_size=256):
        """åŸ·è¡Œè¡Œç‚ºå…‹éš†é è¨“ç·´"""
        print(f"ğŸ¯ é–‹å§‹è¡Œç‚ºå…‹éš†é è¨“ç·´ ({epochs} epochs)")
        
        observations = torch.tensor(self.expert_data['observations'], dtype=torch.float32)
        actions = torch.tensor(self.expert_data['actions'], dtype=torch.float32)
        
        dataset_size = len(observations)
        device = next(self.ppo_agent.actor.parameters()).device
        observations = observations.to(device)
        actions = actions.to(device)
        
        best_bc_loss = float('inf')
        
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            
            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            indices = torch.randperm(dataset_size)
            
            for i in range(0, dataset_size, batch_size):
                batch_indices = indices[i:i+batch_size]
                batch_obs = observations[batch_indices]
                batch_actions = actions[batch_indices]
                
                # å‰å‘å‚³æ’­
                predicted_actions = self.ppo_agent.actor(batch_obs)
                
                # è¨ˆç®—BCæå¤±
                bc_loss = self.bc_loss_fn(predicted_actions, batch_actions)
                
                # åå‘å‚³æ’­
                self.bc_optimizer.zero_grad()
                bc_loss.backward()
                self.bc_optimizer.step()
                
                epoch_loss += bc_loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            
            if avg_loss < best_bc_loss:
                best_bc_loss = avg_loss
                # ä¿å­˜æœ€ä½³BCæ¨¡å‹
                torch.save(self.ppo_agent.actor.state_dict(), 'best_bc_pretrained_actor.pth')
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch:3d}: BC Loss = {avg_loss:.6f} (Best: {best_bc_loss:.6f})")
        
        print(f"âœ… è¡Œç‚ºå…‹éš†é è¨“ç·´å®Œæˆ! æœ€ä½³æå¤±: {best_bc_loss:.6f}")
        return best_bc_loss

class HybridTrainer:
    """æ··åˆè¨“ç·´å™¨ï¼šBCé è¨“ç·´ + PPOå¾®èª¿"""
    
    def __init__(self, expert_data_path):
        self.expert_data_path = expert_data_path
        self.setup_environment()
        self.setup_agents()
        
    def setup_environment(self):
        """è¨­ç½®ç’°å¢ƒï¼ˆæ²¿ç”¨æ‚¨åŸæœ‰çš„è¨­ç½®ï¼‰"""
        self.sai = SAIClient(
            comp_id="booster-soccer-showdown", 
            api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
        )
        self.env = self.sai.make_env()
        print(f"ç’°å¢ƒå·²å‰µå»ºã€‚è§€å¯Ÿç©ºé–“: {self.env.observation_space} | å‹•ä½œç©ºé–“: {self.env.action_space}")
        
    def setup_agents(self):
        """è¨­ç½®æ™ºèƒ½é«”ï¼ˆæ²¿ç”¨æ‚¨åŸæœ‰çš„é…ç½®ï¼‰"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # PPO-CMAé…ç½®ï¼ˆå¾æ‚¨çš„main.pyï¼‰
        self.ppo_cma_agent = PPOCMA(
            state_dim=45,
            action_dim=self.env.action_space.shape[0],
            lr_actor=3e-4,
            lr_critic=1e-3,
            gamma=0.99,
            gae_lambda=0.95,
            clip_epsilon=0.2,
            entropy_coef=0.01,
            hidden_layers=[512, 512, 256],
            buffer_capacity=8192,
            batch_size=1024,
            ppo_epochs=10,
            max_grad_norm=0.5,
            cma_population_size=64,
            cma_sigma=0.1,
            device=device
        )
        
        # å…¶ä»–çµ„ä»¶
        self.curiosity_explorer = CuriosityDrivenExploration(
            state_dim=45,
            action_dim=self.env.action_space.shape[0],
            device=device
        )
        
        self.llm_coach = LLMCoach()
        self.reward_shaper = RewardShaper()
        
        # BCé è¨“ç·´å™¨
        self.bc_pretrainer = BehavioralCloningPretrainer(
            self.ppo_cma_agent, 
            self.expert_data_path
        )
    
    def train(self, bc_epochs=100, rl_timesteps=1000000):
        """åŸ·è¡Œæ··åˆè¨“ç·´"""
        print(f"ğŸš€ é–‹å§‹æ··åˆè¨“ç·´ï¼šBCé è¨“ç·´ + PPO-CMAå¾®èª¿")
        
        # éšæ®µ1ï¼šè¡Œç‚ºå…‹éš†é è¨“ç·´
        print(f"\nğŸ“š === éšæ®µ1ï¼šè¡Œç‚ºå…‹éš†é è¨“ç·´ ===")
        bc_loss = self.bc_pretrainer.pretrain(epochs=bc_epochs)
        
        # éšæ®µ2ï¼šPPO-CMAåœ¨ç·šå¾®èª¿ï¼ˆæ²¿ç”¨æ‚¨çš„è¨“ç·´å¾ªç’°ï¼‰
        print(f"\nğŸ¯ === éšæ®µ2ï¼šPPO-CMAåœ¨ç·šå¾®èª¿ ===")
        self.run_ppo_training(rl_timesteps, initial_bc_loss=bc_loss)
    
    def run_ppo_training(self, total_timesteps, initial_bc_loss):
        """é‹è¡ŒPPOè¨“ç·´ï¼ˆåŸºæ–¼æ‚¨çš„main.pyé‚è¼¯ï¼‰"""
        # è¨­ç½®logger
        logger = TensorBoardLogger(f"hybrid_bc_ppo_training")
        
        # åˆå§‹åŒ–è®Šé‡
        episode_count = 0
        best_reward = -np.inf
        device = self.ppo_cma_agent.device
        
        # æ¬Šé‡é…ç½®
        current_weights = {
            'extrinsic': 0.6,   # é™ä½å¤–åœ¨çå‹µæ¬Šé‡ï¼Œå› ç‚ºæœ‰BCæŒ‡å°
            'intrinsic': 0.3,   # å¥½å¥‡å¿ƒçå‹µ
            'shaped': 0.1       # LLMå¡‘å½¢çå‹µ
        }
        
        print(f"ğŸ¯ é–‹å§‹PPO-CMAå¾®èª¿ï¼Œåˆå§‹BCæå¤±: {initial_bc_loss:.6f}")
        print(f"âš–ï¸ æ¬Šé‡é…ç½®: {current_weights}")
        
        # è¨“ç·´å¾ªç’°
        current_obs, info = self.env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state).float().to(device)
        
        for t in range(1, total_timesteps + 1):
            # PPOå‹•ä½œé¸æ“‡ï¼ˆå·²æœ‰BCé è¨“ç·´çš„åŸºç¤ï¼‰
            action_probs, values, log_probs = self.ppo_cma_agent.forward(state)
            action = self.ppo_cma_agent.get_action(state.unsqueeze(0)).squeeze()
            
            # åŸ·è¡Œå‹•ä½œ
            bounded_action = self.action_function(action.cpu().numpy())
            next_obs, extrinsic_reward, done, _, info = self.env.step(bounded_action)
            
            # çå‹µå¡‘å½¢
            intrinsic_reward = self.curiosity_explorer.compute_intrinsic_reward(
                state.unsqueeze(0), action.unsqueeze(0)
            )
            
            shaped_reward = self.reward_shaper.shape_reward(
                extrinsic_reward, state.cpu().numpy(), action.cpu().numpy(), info
            )
            
            # çµ„åˆçå‹µ
            total_reward = (
                current_weights['extrinsic'] * extrinsic_reward + 
                current_weights['intrinsic'] * intrinsic_reward +
                current_weights['shaped'] * shaped_reward
            )
            
            # è™•ç†ä¸‹ä¸€ç‹€æ…‹
            if not done:
                next_state = Preprocessor().modify_state(next_obs, info)[0]
                next_state = torch.tensor(next_state).float().to(device)
            else:
                next_state = None
            
            # å­˜å„²ç¶“é©—
            self.ppo_cma_agent.store_transition(
                state, action, total_reward, next_state, done, log_probs, values
            )
            
            # æ›´æ–°æ™ºèƒ½é«”
            if self.ppo_cma_agent.should_update():
                ppo_info = self.ppo_cma_agent.update()
                curiosity_info = self.curiosity_explorer.update(
                    self.ppo_cma_agent.buffer.states,
                    self.ppo_cma_agent.buffer.actions
                )
                
                # è¨˜éŒ„è¨“ç·´ä¿¡æ¯
                logger.log({
                    'ppo/policy_loss': ppo_info.get('policy_loss', 0),
                    'ppo/value_loss': ppo_info.get('value_loss', 0),
                    'curiosity/intrinsic_reward': intrinsic_reward,
                    'reward/extrinsic': extrinsic_reward,
                    'reward/shaped': shaped_reward,
                    'reward/total': total_reward,
                    'training/bc_initialization': initial_bc_loss
                }, step=t)
            
            # è™•ç†episodeçµæŸ
            if done:
                episode_count += 1
                current_obs, info = self.env.reset()
                state = Preprocessor().modify_state(current_obs, info)[0]
                state = torch.tensor(state).float().to(device)
                
                print(f"Episode {episode_count}: ç¸½çå‹µ = {total_reward:.2f}")
            else:
                state = next_state
            
            # å®šæœŸå ±å‘Š
            if t % 10000 == 0:
                print(f"æ­¥æ•¸ {t}: Episode {episode_count}, çå‹µæ¬Šé‡ {current_weights}")
        
        logger.close()
        print(f"ğŸ æ··åˆè¨“ç·´å®Œæˆï¼")
    
    def action_function(self, policy):
        """å‹•ä½œè½‰æ›å‡½æ•¸ï¼ˆæ²¿ç”¨æ‚¨çš„å¯¦ç¾ï¼‰"""
        expected_bounds = [-1, 1]
        action_percent = (policy - expected_bounds[0]) / (
            expected_bounds[1] - expected_bounds[0]
        )
        bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
        return (
            self.env.action_space.low
            + (self.env.action_space.high - self.env.action_space.low) * bounded_percent
        )

def main():
    """ä¸»å‡½æ•¸"""
    expert_data_path = "../data/dataset_kick.npz"
    
    # æª¢æŸ¥æ•¸æ“šå­˜åœ¨
    if not os.path.exists(expert_data_path):
        print(f"âŒ å°ˆå®¶æ•¸æ“šä¸å­˜åœ¨: {expert_data_path}")
        print(f"è«‹ç¢ºä¿å·²æ”¶é›†å°ˆå®¶æ•¸æ“š")
        return
    
    # å‰µå»ºæ··åˆè¨“ç·´å™¨
    trainer = HybridTrainer(expert_data_path)
    
    # åŸ·è¡Œè¨“ç·´
    trainer.train(
        bc_epochs=100,        # BCé è¨“ç·´100å€‹epoch
        rl_timesteps=1000000  # PPOå¾®èª¿100è¬æ­¥
    )

if __name__ == "__main__":
    import os
    main()