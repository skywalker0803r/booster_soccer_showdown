# -*- coding: utf-8 -*-
# main_td3_curiosity.py
# ä½¿ç”¨TD3æ”¹é€²çš„ç´”å¥½å¥‡å¿ƒé©…å‹•è¨“ç·´è…³æœ¬ + LLMè¼”åŠ©çå‹µå¡‘å½¢

import numpy as np
import torch
from sai_rl import SAIClient 
from td3_model import TD3_FF, ReplayBuffer  # ä½¿ç”¨TD3æ›¿ä»£DDPG
from utils import Preprocessor
from logger import TensorBoardLogger
from curiosity_module import CuriosityDrivenExploration
from gdrive_utils import SimpleGDriveSync
# [AI-Integrate] å°å…¥LLMè¼”åŠ©æ¨¡çµ„
import sys
sys.path.append('..')  # æ·»åŠ ä¸Šç´šç›®éŒ„åˆ°è·¯å¾‘
from llm_coach import LLMCoach
from reward_shaper import RewardShaper

# =================================================================
# 1. åˆå§‹åŒ– SAIClient å’Œç’°å¢ƒ
# =================================================================
sai = SAIClient(
    comp_id="booster-soccer-showdown", 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)

# ğŸ¯ å‰µå»ºç„¡æ™‚é–“æ‡²ç½°çš„ç’°å¢ƒ
env = sai.make_env()

# ğŸš« ç§»é™¤æ™‚é–“æ‡²ç½° - ä¿®æ”¹çå‹µé…ç½®
print("ğŸ¯ æ­£åœ¨ç§»é™¤æ™‚é–“æ‡²ç½°...")
try:
    # å˜—è©¦è¨ªå•å’Œä¿®æ”¹çå‹µé…ç½®
    if hasattr(env, 'reward_config') or hasattr(env.unwrapped, 'reward_config'):
        reward_config = getattr(env, 'reward_config', None) or getattr(env.unwrapped, 'reward_config', None)
        if reward_config and isinstance(reward_config, dict):
            # ç§»é™¤æ‰€æœ‰æ™‚é–“ç›¸é—œæ‡²ç½°
            if 'steps' in reward_config:
                original_steps = reward_config['steps']
                reward_config['steps'] = 0.0  # è¨­ç‚º0ç§»é™¤æ™‚é–“æ‡²ç½°
                print(f"âœ… æ™‚é–“æ‡²ç½°å·²ç§»é™¤: {original_steps} â†’ 0.0")
            if 'step_penalty' in reward_config:
                reward_config['step_penalty'] = 0.0
                print(f"âœ… æ­¥æ•¸æ‡²ç½°å·²ç§»é™¤")
        else:
            print("âš ï¸ ç„¡æ³•è¨ªå•reward_configï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
    else:
        print("âš ï¸ ç’°å¢ƒä¸æ”¯æŒreward_configä¿®æ”¹ï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
except Exception as e:
    print(f"âš ï¸ ä¿®æ”¹çå‹µé…ç½®å¤±æ•—: {e}ï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
print(f"ç’°å¢ƒå·²å‰µå»ºã€‚è§€å¯Ÿç©ºé–“: {env.observation_space} | å‹•ä½œç©ºé–“: {env.action_space}")

N_FEATURES = 45 
N_ACTIONS = env.action_space.shape[0]

# =================================================================
# 2. è¼”åŠ©å‡½æ•¸ï¼šå‹•ä½œè½‰æ› (ä¿æŒä¸è®Š)
# =================================================================
def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (
        expected_bounds[1] - expected_bounds[0]
    )
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return (
        env.action_space.low
        + (env.action_space.high - env.action_space.low) * bounded_percent
    )

# =================================================================
# 3. ğŸš€ A100æœ€ä½³åŒ–è¶…åƒæ•¸è¨­ç½® (TD3 + ç´”å¥½å¥‡å¿ƒç‰ˆ)
# =================================================================
TOTAL_TIMESTEPS = 2000000          # å¢åŠ ç¸½è¨“ç·´æ­¥æ•¸ï¼Œå……åˆ†åˆ©ç”¨A100
MODEL_NAME = "Booster-TD3-A100-PureOriginal-v1"
BUFFER_CAPACITY = 2000000          # 2M bufferï¼Œåˆ©ç”¨A100å¤§VRAM
BATCH_SIZE = 1024                  # 4å€batch sizeï¼Œå¤§å¹…åŠ é€Ÿè¨“ç·´
LEARNING_RATE = 1e-3               # æé«˜å­¸ç¿’ç‡é…åˆå¤§batch
NEURONS = [512, 512, 256]          # æ›´å¤§æ›´æ·±çš„ç¶²çµ¡æ¶æ§‹
UPDATE_FREQ = 1
SAVE_FREQ = 25                     # æ›´é »ç¹ä¿å­˜

# TD3 ç‰¹æœ‰åƒæ•¸
POLICY_DELAY = 2      # ç­–ç•¥å»¶é²æ›´æ–°é »ç‡
POLICY_NOISE = 0.1    # é™ä½å™ªéŸ³æé«˜ç©©å®šæ€§
NOISE_CLIP = 0.3      # èª¿æ•´å™ªéŸ³ç¯„åœ

# å¥½å¥‡å¿ƒæ¨¡çµ„åƒæ•¸ (A100å„ªåŒ–è¨­ç½®)
INTRINSIC_REWARD_SCALE = 0.8      # ç¨å¾®é™ä½ä»¥å¹³è¡¡å¤§batchæ•ˆæ‡‰
CURIOSITY_UPDATE_FREQ = 1

# åˆå§‹åŒ–TD3æ¨¡å‹
td3_agent = TD3_FF(
    N_FEATURES, 
    env.action_space, 
    NEURONS, 
    torch.nn.functional.relu,
    LEARNING_RATE,
    policy_delay=POLICY_DELAY,
    policy_noise=POLICY_NOISE,
    noise_clip=NOISE_CLIP
)
replay_buffer = ReplayBuffer(BUFFER_CAPACITY, (N_FEATURES,), N_ACTIONS)

# åˆå§‹åŒ–ç´”å¥½å¥‡å¿ƒæ¨¡çµ„
curiosity_explorer = CuriosityDrivenExploration(
    state_dim=N_FEATURES,
    action_dim=N_ACTIONS, 
    intrinsic_reward_scale=INTRINSIC_REWARD_SCALE
)

# [AI-Integrate] åˆå§‹åŒ–LLMè¼”åŠ©æ¨¡çµ„
# ä½¿ç”¨ Gemini API é€²è¡Œæ™ºèƒ½æ±ºç­–
GEMINI_API_KEY = "AIzaSyDUOIGCWDJkY98gi5QcrKtWkxxB61Qhmi0"
llm_coach = LLMCoach(api_key=GEMINI_API_KEY, use_llm=True)
reward_shaper = RewardShaper()
current_weights = llm_coach.current_weights
episode_stats_buffer = []  # ç”¨æ–¼å­˜å„²æœ€è¿‘å¹¾å€‹å›åˆçš„è¡¨ç¾

# =================================================================
# ğŸ”„ æ¨¡å‹è¼‰å…¥é¸æ“‡å’ŒGoogle Driveè¨­ç½®
# =================================================================

# åˆå§‹åŒ–Google DriveåŒæ­¥ (å¸¶éŒ¯èª¤è™•ç†)
try:
    gdrive_sync = SimpleGDriveSync()
    gdrive_available = gdrive_sync.gdrive_path is not None
    print(f"ğŸ”— Google Driveç‹€æ…‹: {'âœ… å·²é€£æ¥' if gdrive_available else 'âŒ æœªé€£æ¥ (åƒ…æœ¬åœ°ä¿å­˜)'}")
except Exception as e:
    print(f"âš ï¸ Google Driveåˆå§‹åŒ–å¤±æ•—: {e}")
    gdrive_sync = None
    gdrive_available = False

# è©¢å•æ˜¯å¦è¼‰å…¥èˆŠæ¨¡å‹
def choose_model_loading():
    print("\n" + "="*50)
    print("ğŸ¤” TD3è¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°å·²æœ‰æ¨¡å‹
    import glob
    local_models = glob.glob(f"*{MODEL_NAME}*.pth") + glob.glob(f"best_*.pth") + glob.glob(f"final_*.pth")
    
    # æª¢æŸ¥Google Driveæ¨¡å‹
    gdrive_models = gdrive_sync.list_saved_models(MODEL_NAME.replace("-", "_")) if gdrive_sync else []
    
    if local_models or gdrive_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹:")
        
        all_models = []
        if local_models:
            print("\næœ¬åœ°æ¨¡å‹:")
            for i, model in enumerate(local_models):
                print(f"  {i+1}. {model}")
                all_models.append(('local', model))
        
        if gdrive_models:
            print(f"\nGoogle Driveæ¨¡å‹ (å‰5å€‹):")
            for i, model in enumerate(gdrive_models[:5]):
                print(f"  {len(local_models)+i+1}. {model['name']} ({model['modified'].strftime('%Y-%m-%d %H:%M')})")
                all_models.append(('gdrive', model['path']))
        
        print(f"\n{len(all_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(all_models) + 1:
                    return None, None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(all_models):
                    model_type, model_path = all_models[choice_num - 1]
                    return model_type, model_path
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None, None

model_type, model_path = choose_model_loading()

# ğŸš€ A100 GPUè¨­ç½®èˆ‡æ··åˆç²¾åº¦
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
td3_agent.to(device)
curiosity_explorer.to(device)

# A100æ··åˆç²¾åº¦åŠ é€Ÿ
scaler = torch.cuda.amp.GradScaler()
print(f"âœ… A100æ··åˆç²¾åº¦è¨“ç·´å·²å•Ÿç”¨ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ”¥ GPUè¨˜æ†¶é«”å„ªåŒ–ï¼šæ··åˆç²¾åº¦å¯ç¯€çœç´„40% VRAM")

# è¼‰å…¥æ¨¡å‹ (å¦‚æœé¸æ“‡äº†)
start_episode = 0
if model_path:
    try:
        if model_type == 'gdrive':
            # å¾Google Driveè¤‡è£½åˆ°æœ¬åœ°
            import shutil
            local_path = f"loaded_{MODEL_NAME}.pth"
            shutil.copy2(model_path, local_path)
            model_path = local_path
        
        print(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            td3_agent.load_state_dict(checkpoint['model_state_dict'])
            start_episode = checkpoint.get('episode', 0)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (å¾Episode {start_episode}ç¹¼çºŒ)")
        else:
            td3_agent.load_state_dict(checkpoint)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (ç‹€æ…‹dictæ ¼å¼)")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ å°‡å¾é ­é–‹å§‹è¨“ç·´")
        start_episode = 0

print(f"ğŸš€ é–‹å§‹è¨“ç·´ (èµ·å§‹Episode: {start_episode})")

# åˆå§‹åŒ–è¨˜éŒ„å™¨
logger = TensorBoardLogger(model_name=MODEL_NAME) 

# è¿½è¹¤è®Šé‡
episode_reward_sum = 0
episode_intrinsic_reward_sum = 0
episode_extrinsic_reward_sum = 0  # åˆ†åˆ¥è¿½è¹¤åŸå§‹çå‹µ
episode_shaped_reward_sum = 0     # [AI-Integrate] è¿½è¹¤LLMå¡‘å½¢çå‹µ
episode_count = 0
episode_steps = 0
best_reward = -np.inf
best_model_path = f"best_{MODEL_NAME}.pth"

print(f"ğŸš€ A100æœ€ä½³åŒ– TD3 + ç´”åŸå§‹çå‹µè¨“ç·´é–‹å§‹ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ¯ TD3æ”¹é€²ç‰¹æ€§ï¼š")
print(f"   â€¢ Double Q-Learning: âœ…")
print(f"   â€¢ Delayed Policy Updates: âœ… (æ¯{POLICY_DELAY}æ¬¡)")
print(f"   â€¢ Target Policy Smoothing: âœ… (å™ªéŸ³Ïƒ={POLICY_NOISE})")
print(f"ğŸ”¥ A100å„ªåŒ–é…ç½®ï¼š")
print(f"   â€¢ Batch Size: {BATCH_SIZE} (4å€æå‡)")
print(f"   â€¢ Buffer Capacity: {BUFFER_CAPACITY//1000}K (2å€æå‡)")
print(f"   â€¢ Network Size: {NEURONS} (æ›´å¤§æ›´æ·±)")
print(f"   â€¢ Learning Rate: {LEARNING_RATE} (é…åˆå¤§batch)")
print(f"   â€¢ å…§åœ¨çå‹µç¸®æ”¾: {INTRINSIC_REWARD_SCALE}")
print(f"   â€¢ æ··åˆç²¾åº¦: âœ… (A100å°ˆç”¨)")
print(f"âŒ OUå™ªéŸ³ï¼šå·²ç¦ç”¨")
print(f"âŒ PBRSçå‹µï¼šå·²ç¦ç”¨")
print(f"âŒ çå‹µå·¥ç¨‹ï¼šå·²ç§»é™¤") 
print(f"âŒ æ™‚é–“æ‡²ç½°ï¼šå·²ç§»é™¤")
print(f"âœ… ç´”åŸå§‹ç’°å¢ƒçå‹µï¼šå·²å•Ÿç”¨")
print(f"âœ… å¥½å¥‡å¿ƒè¼”åŠ©æ¢ç´¢ï¼šå·²å•Ÿç”¨")

# =================================================================
# 4. TD3 + ç´”å¥½å¥‡å¿ƒ è¨“ç·´å¾ªç’°
# =================================================================
current_obs, info = env.reset()
state = Preprocessor().modify_state(current_obs, info)[0] 
state = torch.tensor(state).float().to(device)

for t in range(1, TOTAL_TIMESTEPS + 1):
    # 1. æ¡é›†å‹•ä½œ (ä¸æ·»åŠ OUå™ªéŸ³ï¼Œç´”ä¾è³´å¥½å¥‡å¿ƒæ¢ç´¢)
    with torch.no_grad():
        raw_action_tensor = td3_agent(state.unsqueeze(0))
    raw_action = raw_action_tensor.cpu().numpy().flatten()
    
    # ğŸš« ä¸æ·»åŠ OUå™ªéŸ³ - ç´”ä¾è³´å¥½å¥‡å¿ƒé©…å‹•çš„æ¢ç´¢
    
    # åŸ·è¡Œå‹•ä½œ
    action = action_function(raw_action)
    next_obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    
    # ç‹€æ…‹è½‰æ›
    next_state_np = Preprocessor().modify_state(next_obs, info)[0]
    next_state = torch.tensor(next_state_np).float().to(device)

    # =================================================================
    # ğŸ§  LLMè¼”åŠ©çå‹µå¡‘å½¢ + å¥½å¥‡å¿ƒçå‹µè¨ˆç®—
    # =================================================================
    
    # ğŸš« å¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½° (å¦‚æœç’°å¢ƒé…ç½®ä¿®æ”¹å¤±æ•—)
    processed_reward = reward
    
    # æª¢æ¸¬ä¸¦ç§»é™¤å¯èƒ½çš„æ™‚é–“æ‡²ç½°æ¨¡å¼
    if episode_steps > 10:  # é¿å…åˆæœŸèª¤åˆ¤
        # å¦‚æœrewardæ˜¯å›ºå®šçš„å°è² å€¼ï¼Œå¯èƒ½æ˜¯æ™‚é–“æ‡²ç½°
        if -1.5 <= reward <= -0.1:  # å…¸å‹çš„æ™‚é–“æ‡²ç½°ç¯„åœ
            # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ™‚é–“æ‡²ç½°ï¼ˆæ²’æœ‰å…¶ä»–äº‹ä»¶ï¼‰
            if not any(keyword in str(info).lower() for keyword in ['goal', 'fallen', 'success', 'offside']):
                processed_reward = 0.0  # ç§»é™¤æ™‚é–“æ‡²ç½°
                if t % 10000 == 0:  # å¶çˆ¾æç¤º
                    print(f"ğŸš« æª¢æ¸¬åˆ°æ™‚é–“æ‡²ç½° {reward:.3f}ï¼Œå·²ç§»é™¤")
    
    # [AI-Integrate] è¨ˆç®—LLMå¼•å°çš„Shaped Reward
    shaped_reward = reward_shaper.compute_reward(info, next_obs, current_weights)
    
    # [AI-Integrate] èåˆçå‹µï¼šåŸå§‹çå‹µ + LLMå¡‘å½¢çå‹µ
    # æ ¹æ“šprompt.txtå»ºè­°èª¿æ•´æ¯”ä¾‹
    total_step_reward = processed_reward + shaped_reward
    
    # ğŸ¯ LLMå¢å¼·çå‹µ + å¥½å¥‡å¿ƒæ¨¡çµ„
    final_reward, intrinsic_reward = curiosity_explorer.get_enhanced_reward(
        state.cpu().numpy(),
        raw_action,
        next_state_np,
        total_step_reward  # ä½¿ç”¨LLMå¢å¼·å¾Œçš„çå‹µ
    )
    
    # ç´¯ç©çµ±è¨ˆ
    episode_extrinsic_reward_sum += reward
    episode_intrinsic_reward_sum += intrinsic_reward
    episode_reward_sum += final_reward
    episode_steps += 1
    
    # [AI-Integrate] ç´¯ç©LLMå¡‘å½¢çå‹µçµ±è¨ˆ
    if 'episode_shaped_reward_sum' not in locals():
        episode_shaped_reward_sum = 0.0
    episode_shaped_reward_sum += shaped_reward

    # =================================================================
    # ğŸ“š ç¶“é©—å„²å­˜å’Œæ¨¡å‹æ›´æ–°
    # =================================================================
    
    # å„²å­˜ç¶“é©— (ä½¿ç”¨å¥½å¥‡å¿ƒå¢å¼·çå‹µ)
    replay_buffer.add(
        state.cpu().numpy(), 
        raw_action, 
        final_reward,
        next_state_np, 
        done
    )

    # ğŸš€ A100å„ªåŒ– TD3 æ¨¡å‹æ›´æ–°ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
    if replay_buffer.size > BATCH_SIZE and t % UPDATE_FREQ == 0:
        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)
        
        states = torch.tensor(states).float().to(device)
        actions = torch.tensor(actions).float().to(device)
        rewards = torch.tensor(rewards).float().to(device)
        next_states = torch.tensor(next_states).float().to(device)
        dones = torch.tensor(dones).float().to(device)
        
        # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿè¨“ç·´
        with torch.cuda.amp.autocast():
            critic_loss, actor_loss = td3_agent.model_update(states, actions, rewards, next_states, dones)
        
        # æ›´æ–°å¥½å¥‡å¿ƒæ¨¡çµ„
        if t % CURIOSITY_UPDATE_FREQ == 0:
            curiosity_stats = curiosity_explorer.update_curiosity(states, actions, next_states)
            
            # è¨˜éŒ„å¥½å¥‡å¿ƒæŒ‡æ¨™
            logger.set_step(t)
            logger.log_scalar("Curiosity/Forward_Loss", curiosity_stats['forward_loss'])
            logger.log_scalar("Curiosity/Inverse_Loss", curiosity_stats['inverse_loss'])
            logger.log_scalar("Curiosity/Avg_Intrinsic_Reward", curiosity_stats['avg_intrinsic_reward'])
        
        # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        logger.set_step(t) 
        logger.log_scalar("Loss/Critic_Loss", critic_loss) 
        if actor_loss is not None and actor_loss != 0.0:  # TD3çš„å»¶é²æ›´æ–°
            logger.log_scalar("Loss/Actor_Loss", actor_loss)
        
        # è¨˜éŒ„TD3ç‰¹å®šæŒ‡æ¨™
        td3_stats = td3_agent.get_statistics()
        logger.log_scalar("TD3/Update_Counter", td3_stats['update_counter'])
        logger.log_scalar("TD3/Next_Actor_Update", td3_stats['next_actor_update'])

    # =================================================================
    # ğŸ”„ å›åˆçµæŸè™•ç†
    # =================================================================
    if done:
        episode_count += 1

        # [AI-Integrate] æ”¶é›†æ•¸æ“šçµ¦LLMæ•™ç·´
        # æª¢æ¸¬æ˜¯å¦è·Œå€’ï¼ˆæ ¹æ“šæ­¥æ•¸å’Œçå‹µåˆ¤æ–·ï¼‰
        fell_down = episode_steps < 20 or episode_extrinsic_reward_sum < -5.0
        episode_stats_buffer.append({
            'steps': episode_steps,
            'reward': episode_reward_sum,
            'extrinsic_reward': episode_extrinsic_reward_sum,
            'shaped_reward': episode_shaped_reward_sum,
            'fell_down': fell_down
        })

        # è©³ç´°è¨˜éŒ„åˆ†è§£çå‹µ
        logger.log_scalar("Train/Episode_Total_Reward", episode_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Extrinsic_Reward", episode_extrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Intrinsic_Reward", episode_intrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Shaped_Reward", episode_shaped_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Steps", episode_steps, step=t)
        
        # è¨ˆç®—å¥½å¥‡å¿ƒè²¢ç»æ¯”ä¾‹
        if episode_reward_sum != 0:
            curiosity_ratio = episode_intrinsic_reward_sum / abs(episode_reward_sum)
            logger.log_scalar("Train/Curiosity_Contribution_Ratio", curiosity_ratio, step=t)
        
        # [AI-Integrate] æ¯50å€‹Episodeè®“LLMæ•™ç·´èª¿æ•´ç­–ç•¥
        if episode_count % 50 == 0 and len(episode_stats_buffer) >= 10:
            # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
            recent_episodes = episode_stats_buffer[-50:] if len(episode_stats_buffer) >= 50 else episode_stats_buffer
            
            avg_steps = np.mean([ep['steps'] for ep in recent_episodes])
            avg_reward = np.mean([ep['reward'] for ep in recent_episodes])
            fall_rate = np.mean([ep['fell_down'] for ep in recent_episodes])
            avg_shaped_reward = np.mean([ep['shaped_reward'] for ep in recent_episodes])
            
            stats_summary = {
                'avg_steps': avg_steps,
                'avg_reward': avg_reward,
                'fall_rate': fall_rate,
                'avg_shaped_reward': avg_shaped_reward
            }
            
            # æ›´æ–°æ¬Šé‡
            previous_weights = current_weights.copy()
            current_weights = llm_coach.consult(stats_summary)
            
            # è¨˜éŒ„æ•™ç·´æ±ºç­–
            print(f"ğŸ§  LLM Coach ç¬¬{episode_count}å›åˆæ›´æ–°:")
            print(f"   ç•¶å‰éšæ®µ: {llm_coach.phase}")
            print(f"   çµ±è¨ˆæ•¸æ“š: æ­¥æ•¸={float(avg_steps):.1f}, è·Œå€’ç‡={float(fall_rate):.3f}, å¹³å‡çå‹µ={float(avg_reward):.2f}")
            print(f"   æ¬Šé‡è®ŠåŒ–: {previous_weights} â†’ {current_weights}")
            
            # è¨˜éŒ„åˆ° TensorBoard
            logger.log_scalar("Coach/Weight_Balance", current_weights.get('balance', 0), step=t)
            logger.log_scalar("Coach/Weight_Progress", current_weights.get('progress', 0), step=t)
            logger.log_scalar("Coach/Weight_Energy", current_weights.get('energy', 0), step=t)
            logger.log_scalar("Coach/Avg_Steps", avg_steps, step=t)
            logger.log_scalar("Coach/Fall_Rate", fall_rate, step=t)
            logger.log_scalar("Coach/Phase_ID", hash(llm_coach.phase) % 1000, step=t)  # ç°¡å–®çš„ç›¸ä½ç·¨ç¢¼
            
            # è¨˜éŒ„ LLM API çµ±è¨ˆ
            api_stats = llm_coach.get_api_statistics()
            logger.log_scalar("LLM_API/Total_Calls", api_stats['total_calls'], step=t)
            logger.log_scalar("LLM_API/Success_Rate", api_stats['success_rate'], step=t)
            logger.log_scalar("LLM_API/Errors", api_stats['errors'], step=t)
            
            # æ¸…ç©ºéƒ¨åˆ†ç·©è¡ä»¥ä¿æŒè¨˜æ†¶é«”æ•ˆç‡
            episode_stats_buffer = episode_stats_buffer[-100:]  # ä¿ç•™æœ€è¿‘100å€‹å›åˆ
        
        # æª¢æŸ¥æœ€ä½³æ¨¡å‹ä¸¦è‡ªå‹•ä¿å­˜åˆ°Google Drive
        if episode_reward_sum > best_reward:
            best_reward = episode_reward_sum
            
            # ä¿å­˜æ¨¡å‹ç‹€æ…‹ (åŒ…å«å…ƒæ•¸æ“š)
            checkpoint = {
                'model_state_dict': td3_agent.state_dict(),
                'episode': episode_count + start_episode,
                'timestep': t,
                'best_reward': best_reward,
                'total_reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'episode_steps': episode_steps,
                'td3_update_counter': td3_agent.update_counter
            }
            
            # æœ¬åœ°ä¿å­˜
            torch.save(checkpoint, best_model_path)
            
            # è‡ªå‹•ä¿å­˜åˆ°Google Drive
            metadata = {
                'episode': episode_count + start_episode,
                'timestep': t,
                'reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'steps': episode_steps,
                'algorithm': 'TD3'
            }
            if gdrive_sync and gdrive_available:
                gdrive_sync.save_model(checkpoint, f"best_{MODEL_NAME}", metadata)
            else:
                print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œåƒ…æœ¬åœ°ä¿å­˜")
            
            print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹!")
            print(f"   ç¸½çå‹µ: {episode_reward_sum:.2f}")
            print(f"   åŸå§‹çå‹µ: {episode_extrinsic_reward_sum:.2f}")
            print(f"   å¥½å¥‡å¿ƒçå‹µ: {episode_intrinsic_reward_sum:.2f}")
            print(f"   å›åˆæ­¥æ•¸: {episode_steps}")
            print(f"   è¨“ç·´æ­¥æ•¸: {t}")
            print(f"   ğŸ“¤ å·²è‡ªå‹•å‚™ä»½åˆ°Google Drive")
        
        # å®šæœŸé€²åº¦å ±å‘Š
        if episode_count % 5 == 0:
            ratio = episode_intrinsic_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            shaped_ratio = episode_shaped_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            td3_stats = td3_agent.get_statistics()
            print(f"ğŸ¯ Episode {episode_count:3d} | "
                  f"ç¸½çå‹µ: {episode_reward_sum:6.2f} | "
                  f"åŸå§‹: {episode_extrinsic_reward_sum:6.2f} | "
                  f"å¡‘å½¢: {episode_shaped_reward_sum:5.2f} | "
                  f"å¥½å¥‡å¿ƒ: {episode_intrinsic_reward_sum:5.2f} | "
                  f"æ­¥æ•¸: {episode_steps:3d} | "
                  f"éšæ®µ: {llm_coach.phase[:8]} | "
                  f"TD3æ›´æ–°: {td3_stats['update_counter']}")
        
        # é‡ç½®ç’°å¢ƒ
        current_obs, info = env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state).float().to(device)
        
        # é‡è¨­è®Šé‡
        episode_reward_sum = 0
        episode_intrinsic_reward_sum = 0
        episode_extrinsic_reward_sum = 0
        episode_shaped_reward_sum = 0  # [AI-Integrate] é‡è¨­LLMå¡‘å½¢çå‹µ
        episode_steps = 0
    else:
        state = next_state
    
    # å¤§é€²åº¦å ±å‘Šå’Œå®šæœŸå‚™ä»½ (ğŸš€ A100å„ªåŒ–: æ›´é »ç¹å‚™ä»½)
    if t % 10000 == 0:
        curiosity_stats = curiosity_explorer.get_statistics()
        td3_stats = td3_agent.get_statistics()
        print(f"\nğŸš€ === TD3è¨“ç·´é€²åº¦å ±å‘Š (æ­¥æ•¸: {t}) ===")
        print(f"ğŸ“Š å›åˆç¸½æ•¸: {episode_count}")
        print(f"ğŸ’¾ Bufferå¤§å°: {replay_buffer.size}")
        print(f"ğŸ† æœ€ä½³ç¸½çå‹µ: {best_reward:.2f}")
        print(f"ğŸ§  ç´¯è¨ˆå¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['total_intrinsic_reward']:.2f}")
        print(f"ğŸ“ˆ å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['average_intrinsic_reward']:.4f}")
        print(f"ğŸ”„ å¥½å¥‡å¿ƒæ›´æ–°æ¬¡æ•¸: {curiosity_stats['update_count']}")
        print(f"ğŸ¯ TD3æ›´æ–°æ¬¡æ•¸: {td3_stats['update_counter']}")
        print(f"â° ä¸‹æ¬¡Actoræ›´æ–°: {td3_stats['next_actor_update']}æ­¥å¾Œ")
        
        # å®šæœŸè‡ªå‹•å‚™ä»½åˆ°Google Drive
        checkpoint_name = f"checkpoint_{t//1000}k"
        checkpoint_data = {
            'model_state_dict': td3_agent.state_dict(),
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'td3_update_counter': td3_agent.update_counter
        }
        checkpoint_meta = {
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'checkpoint': True,
            'algorithm': 'TD3'
        }
        
        if gdrive_sync and gdrive_available:
            if gdrive_sync.save_model(checkpoint_data, checkpoint_name, checkpoint_meta):
                print(f"ğŸ“¤ å®šæœŸå‚™ä»½å·²ä¿å­˜åˆ° Google Drive")
        else:
            print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œè·³éé›²ç«¯å‚™ä»½")
        
        print("=" * 50)

# =================================================================
# 5. è¨“ç·´å®Œæˆå’Œç¸½çµ
# =================================================================
final_model_path = f"final_{MODEL_NAME}.pth"

# ä¿å­˜æœ€çµ‚æ¨¡å‹ (åŒ…å«å®Œæ•´ç‹€æ…‹)
final_checkpoint = {
    'model_state_dict': td3_agent.state_dict(),
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS,
    'best_reward': best_reward,
    'final_training': True,
    'td3_update_counter': td3_agent.update_counter
}
torch.save(final_checkpoint, final_model_path)

# è‡ªå‹•ä¿å­˜æœ€çµ‚æ¨¡å‹åˆ°Google Drive
final_metadata = {
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS, 
    'best_reward': best_reward,
    'training_completed': True,
    'algorithm': 'TD3'
}
if gdrive_sync and gdrive_available:
    gdrive_sync.save_model(final_checkpoint, f"final_{MODEL_NAME}", final_metadata)
    print(f"ğŸ“¤ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ° Google Drive")
else:
    print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œæœ€çµ‚æ¨¡å‹åƒ…æœ¬åœ°ä¿å­˜")

curiosity_final_stats = curiosity_explorer.get_statistics()
td3_final_stats = td3_agent.get_statistics()

print(f"\nğŸ‰ TD3 + LLMè¼”åŠ© + å¥½å¥‡å¿ƒè¨“ç·´å®Œæˆï¼")
print(f"ğŸ† æœ€ä½³å›åˆçå‹µ: {best_reward:.2f}")
print(f"ğŸ§  ç¸½å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['total_intrinsic_reward']:.2f}")
print(f"ğŸ“Š å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['average_intrinsic_reward']:.4f}")
print(f"ğŸ”„ ç¸½å›åˆæ•¸: {episode_count}")
print(f"ğŸ¯ TD3ç¸½æ›´æ–°æ¬¡æ•¸: {td3_final_stats['update_counter']}")
print(f"ğŸ§  LLMæ•™ç·´æœ€çµ‚éšæ®µ: {llm_coach.phase}")
print(f"âš–ï¸ æœ€çµ‚æ¬Šé‡é…ç½®: {current_weights}")
print(f"ğŸ“ˆ éšæ®µè®ŠåŒ–æ¬¡æ•¸: {len(llm_coach.phase_history)}")

# LLM API çµ±è¨ˆå ±å‘Š
llm_api_stats = llm_coach.get_api_statistics()
print(f"ğŸ¤– LLM APIçµ±è¨ˆ:")
print(f"   ç¸½èª¿ç”¨æ¬¡æ•¸: {llm_api_stats['total_calls']}")
print(f"   éŒ¯èª¤æ¬¡æ•¸: {llm_api_stats['errors']}")
print(f"   æˆåŠŸç‡: {llm_api_stats['success_rate']:.2%}")
print(f"   LLMå•Ÿç”¨: {'âœ…' if llm_api_stats['llm_enabled'] else 'âŒ'}")

print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {best_model_path}, {final_model_path}")

# æ¸…ç†
env.close()
logger.close()
print("ğŸ TD3ç´”å¥½å¥‡å¿ƒå¯¦é©—å®Œæˆï¼")