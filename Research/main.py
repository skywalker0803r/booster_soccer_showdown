# -*- coding: utf-8 -*-
# main_pure_curiosity.py
# ç´”å¥½å¥‡å¿ƒé©…å‹•çš„DDPGè¨“ç·´è…³æœ¬ (ä¸ä½¿ç”¨OUå™ªéŸ³å’ŒPBRS)

import numpy as np
import torch
from sai_rl import SAIClient 
from ddpg_model import DDPG_FF, ReplayBuffer
from utils import Preprocessor  # ä¸å°å…¥calculate_potential
from logger import TensorBoardLogger
from curiosity_module import CuriosityDrivenExploration
from gdrive_utils import SimpleGDriveSync

# =================================================================
# 1. åˆå§‹åŒ– SAIClient å’Œç’°å¢ƒ
# =================================================================
sai = SAIClient(
    comp_id="booster-soccer-showdown" , 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)

env = sai.make_env()
print(f"ç’°å¢ƒå·²å‰µå»ºã€‚è§€å¯Ÿç©ºé–“: {env.observation_space} | å‹•ä½œç©ºé–“: {env.action_space}")

N_FEATURES = 45 
N_ACTIONS = env.action_space.shape[0]

# =================================================================
# 2. è¼”åŠ©å‡½æ•¸ï¼šå‹•ä½œè½‰æ› (ä¿æŒä¸è®Š)
# =================================================================
def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (
        expected_bounds[1] - expected_bounds[0]
    )
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return (
        env.action_space.low
        + (env.action_space.high - env.action_space.low) * bounded_percent
    )

# =================================================================
# 3. è¶…åƒæ•¸å’Œåˆå§‹åŒ– (ç´”å¥½å¥‡å¿ƒç‰ˆ)
# =================================================================
TOTAL_TIMESTEPS = 1000000
MODEL_NAME = "Booster-DDPG-PureCuriosity-v1"
BUFFER_CAPACITY = 1000000
BATCH_SIZE = 256
LEARNING_RATE = 1e-4
NEURONS = [256, 256] 
UPDATE_FREQ = 1
SAVE_FREQ = 50

# å¥½å¥‡å¿ƒæ¨¡çµ„åƒæ•¸ (é—œéµè¨­ç½®)
INTRINSIC_REWARD_SCALE = 1.0  # å¢å¤§ä¿‚æ•¸ï¼Œå› ç‚ºåªä¾è³´å¥½å¥‡å¿ƒ
CURIOSITY_UPDATE_FREQ = 1

# åˆå§‹åŒ–æ¨¡å‹
ddpg_agent = DDPG_FF(
    N_FEATURES, 
    env.action_space, 
    NEURONS, 
    torch.nn.functional.relu,
    LEARNING_RATE
)
replay_buffer = ReplayBuffer(BUFFER_CAPACITY, (N_FEATURES,), N_ACTIONS)

# åˆå§‹åŒ–ç´”å¥½å¥‡å¿ƒæ¨¡çµ„
curiosity_explorer = CuriosityDrivenExploration(
    state_dim=N_FEATURES,
    action_dim=N_ACTIONS, 
    intrinsic_reward_scale=INTRINSIC_REWARD_SCALE
)

# =================================================================
# ğŸ”„ æ¨¡å‹è¼‰å…¥é¸æ“‡å’ŒGoogle Driveè¨­ç½®
# =================================================================

# åˆå§‹åŒ–Google DriveåŒæ­¥
gdrive_sync = SimpleGDriveSync()

# è©¢å•æ˜¯å¦è¼‰å…¥èˆŠæ¨¡å‹
def choose_model_loading():
    print("\n" + "="*50)
    print("ğŸ¤” è¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°å·²æœ‰æ¨¡å‹
    import glob
    local_models = glob.glob(f"*{MODEL_NAME}*.pth") + glob.glob(f"best_*.pth") + glob.glob(f"final_*.pth")
    
    # æª¢æŸ¥Google Driveæ¨¡å‹
    gdrive_models = gdrive_sync.list_saved_models(MODEL_NAME.replace("-", "_"))
    
    if local_models or gdrive_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹:")
        
        all_models = []
        if local_models:
            print("\næœ¬åœ°æ¨¡å‹:")
            for i, model in enumerate(local_models):
                print(f"  {i+1}. {model}")
                all_models.append(('local', model))
        
        if gdrive_models:
            print(f"\nGoogle Driveæ¨¡å‹ (å‰5å€‹):")
            for i, model in enumerate(gdrive_models[:5]):
                print(f"  {len(local_models)+i+1}. {model['name']} ({model['modified'].strftime('%Y-%m-%d %H:%M')})")
                all_models.append(('gdrive', model['path']))
        
        print(f"\n{len(all_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(all_models) + 1:
                    return None, None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(all_models):
                    model_type, model_path = all_models[choice_num - 1]
                    return model_type, model_path
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None, None

model_type, model_path = choose_model_loading()

# GPUè¨­ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ddpg_agent.to(device)
curiosity_explorer.to(device)

# è¼‰å…¥æ¨¡å‹ (å¦‚æœé¸æ“‡äº†)
start_episode = 0
if model_path:
    try:
        if model_type == 'gdrive':
            # å¾Google Driveè¤‡è£½åˆ°æœ¬åœ°
            import shutil
            local_path = f"loaded_{MODEL_NAME}.pth"
            shutil.copy2(model_path, local_path)
            model_path = local_path
        
        print(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            ddpg_agent.load_state_dict(checkpoint['model_state_dict'])
            start_episode = checkpoint.get('episode', 0)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (å¾Episode {start_episode}ç¹¼çºŒ)")
        else:
            ddpg_agent.load_state_dict(checkpoint)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (ç‹€æ…‹dictæ ¼å¼)")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ å°‡å¾é ­é–‹å§‹è¨“ç·´")
        start_episode = 0

print(f"ğŸš€ é–‹å§‹è¨“ç·´ (èµ·å§‹Episode: {start_episode})")

# åˆå§‹åŒ–è¨˜éŒ„å™¨
logger = TensorBoardLogger(model_name=MODEL_NAME) 

# è¿½è¹¤è®Šé‡
episode_reward_sum = 0
episode_intrinsic_reward_sum = 0
episode_extrinsic_reward_sum = 0  # åˆ†åˆ¥è¿½è¹¤åŸå§‹çå‹µ
episode_count = 0
episode_steps = 0
best_reward = -np.inf
best_model_path = f"best_{MODEL_NAME}.pth"

print(f"ğŸ§  ç´”å¥½å¥‡å¿ƒDDPGè¨“ç·´é–‹å§‹ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ”¥ å…§åœ¨çå‹µç¸®æ”¾ä¿‚æ•¸ï¼š{INTRINSIC_REWARD_SCALE}")
print(f"âŒ OUå™ªéŸ³ï¼šå·²ç¦ç”¨")
print(f"âŒ PBRSçå‹µï¼šå·²ç¦ç”¨") 
print(f"âœ… ç´”å¥½å¥‡å¿ƒæ¢ç´¢ï¼šå·²å•Ÿç”¨")

# =================================================================
# 4. ç´”å¥½å¥‡å¿ƒ DDPG è¨“ç·´å¾ªç’°
# =================================================================
current_obs, info = env.reset()
state = Preprocessor().modify_state(current_obs, info)[0] 
state = torch.tensor(state).float().to(device)

for t in range(1, TOTAL_TIMESTEPS + 1):
    # 1. æ¡é›†å‹•ä½œ (ä¸æ·»åŠ OUå™ªéŸ³ï¼Œç´”ä¾è³´å¥½å¥‡å¿ƒæ¢ç´¢)
    with torch.no_grad():
        raw_action_tensor = ddpg_agent(state.unsqueeze(0))
    raw_action = raw_action_tensor.cpu().numpy().flatten()
    
    # ğŸš« ä¸æ·»åŠ OUå™ªéŸ³ - ç´”ä¾è³´å¥½å¥‡å¿ƒé©…å‹•çš„æ¢ç´¢
    
    # åŸ·è¡Œå‹•ä½œ
    action = action_function(raw_action)
    next_obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    
    # ç‹€æ…‹è½‰æ›
    next_state_np = Preprocessor().modify_state(next_obs, info)[0]
    next_state = torch.tensor(next_state_np).float().to(device)

    # =================================================================
    # ğŸ§  ç´”å¥½å¥‡å¿ƒçå‹µè¨ˆç®—
    # =================================================================
    
    # ğŸš« ä¸ä½¿ç”¨PBRSçå‹µ
    # åªä½¿ç”¨ï¼šåŸå§‹çå‹µ + å¥½å¥‡å¿ƒçå‹µ
    
    final_reward, intrinsic_reward = curiosity_explorer.get_enhanced_reward(
        state.cpu().numpy(),
        raw_action,
        next_state_np,
        reward  # ç›´æ¥ä½¿ç”¨åŸå§‹çå‹µï¼Œä¸åŠ PBRS
    )
    
    # ç´¯ç©çµ±è¨ˆ
    episode_extrinsic_reward_sum += reward
    episode_intrinsic_reward_sum += intrinsic_reward
    episode_reward_sum += final_reward
    episode_steps += 1

    # =================================================================
    # ğŸ“š ç¶“é©—å„²å­˜å’Œæ¨¡å‹æ›´æ–°
    # =================================================================
    
    # å„²å­˜ç¶“é©— (ä½¿ç”¨å¥½å¥‡å¿ƒå¢å¼·çå‹µ)
    replay_buffer.add(
        state.cpu().numpy(), 
        raw_action, 
        final_reward,
        next_state_np, 
        done
    )

    # DDPG æ¨¡å‹æ›´æ–°
    if replay_buffer.size > BATCH_SIZE and t % UPDATE_FREQ == 0:
        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)
        
        states = torch.tensor(states).float().to(device)
        actions = torch.tensor(actions).float().to(device)
        rewards = torch.tensor(rewards).float().to(device)
        next_states = torch.tensor(next_states).float().to(device)
        dones = torch.tensor(dones).float().to(device)
        
        critic_loss, actor_loss = ddpg_agent.model_update(states, actions, rewards, next_states, dones)
        
        # æ›´æ–°å¥½å¥‡å¿ƒæ¨¡çµ„
        if t % CURIOSITY_UPDATE_FREQ == 0:
            curiosity_stats = curiosity_explorer.update_curiosity(states, actions, next_states)
            
            # è¨˜éŒ„å¥½å¥‡å¿ƒæŒ‡æ¨™
            logger.set_step(t)
            logger.log_scalar("Curiosity/Forward_Loss", curiosity_stats['forward_loss'])
            logger.log_scalar("Curiosity/Inverse_Loss", curiosity_stats['inverse_loss'])
            logger.log_scalar("Curiosity/Avg_Intrinsic_Reward", curiosity_stats['avg_intrinsic_reward'])
        
        # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        logger.set_step(t) 
        logger.log_scalar("Loss/Critic_Loss", critic_loss) 
        logger.log_scalar("Loss/Actor_Loss", actor_loss) 

    # =================================================================
    # ğŸ”„ å›åˆçµæŸè™•ç†
    # =================================================================
    if done:
        episode_count += 1

        # è©³ç´°è¨˜éŒ„åˆ†è§£çå‹µ
        logger.log_scalar("Train/Episode_Total_Reward", episode_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Extrinsic_Reward", episode_extrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Intrinsic_Reward", episode_intrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Steps", episode_steps, step=t)
        
        # è¨ˆç®—å¥½å¥‡å¿ƒè²¢ç»æ¯”ä¾‹
        if episode_reward_sum != 0:
            curiosity_ratio = episode_intrinsic_reward_sum / abs(episode_reward_sum)
            logger.log_scalar("Train/Curiosity_Contribution_Ratio", curiosity_ratio, step=t)
        
        # æª¢æŸ¥æœ€ä½³æ¨¡å‹ä¸¦è‡ªå‹•ä¿å­˜åˆ°Google Drive
        if episode_reward_sum > best_reward:
            best_reward = episode_reward_sum
            
            # ä¿å­˜æ¨¡å‹ç‹€æ…‹ (åŒ…å«å…ƒæ•¸æ“š)
            checkpoint = {
                'model_state_dict': ddpg_agent.state_dict(),
                'episode': episode_count + start_episode,
                'timestep': t,
                'best_reward': best_reward,
                'total_reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'episode_steps': episode_steps
            }
            
            # æœ¬åœ°ä¿å­˜
            torch.save(checkpoint, best_model_path)
            
            # è‡ªå‹•ä¿å­˜åˆ°Google Drive
            metadata = {
                'episode': episode_count + start_episode,
                'timestep': t,
                'reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'steps': episode_steps
            }
            gdrive_sync.save_model(checkpoint, f"best_{MODEL_NAME}", metadata)
            
            print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹!")
            print(f"   ç¸½çå‹µ: {episode_reward_sum:.2f}")
            print(f"   åŸå§‹çå‹µ: {episode_extrinsic_reward_sum:.2f}")
            print(f"   å¥½å¥‡å¿ƒçå‹µ: {episode_intrinsic_reward_sum:.2f}")
            print(f"   å›åˆæ­¥æ•¸: {episode_steps}")
            print(f"   è¨“ç·´æ­¥æ•¸: {t}")
            print(f"   ğŸ“¤ å·²è‡ªå‹•å‚™ä»½åˆ°Google Drive")
        
        # å®šæœŸé€²åº¦å ±å‘Š
        if episode_count % 5 == 0:
            ratio = episode_intrinsic_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            print(f"ğŸ§  Episode {episode_count:3d} | "
                  f"ç¸½çå‹µ: {episode_reward_sum:6.2f} | "
                  f"åŸå§‹: {episode_extrinsic_reward_sum:6.2f} | "
                  f"å¥½å¥‡å¿ƒ: {episode_intrinsic_reward_sum:5.2f} | "
                  f"æ­¥æ•¸: {episode_steps:3d} | "
                  f"æ¯”ä¾‹: {ratio:.2f}")
        
        # é‡ç½®ç’°å¢ƒ
        current_obs, info = env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state).float().to(device)
        
        # é‡è¨­è®Šé‡
        episode_reward_sum = 0
        episode_intrinsic_reward_sum = 0
        episode_extrinsic_reward_sum = 0
        episode_steps = 0
    else:
        state = next_state
    
    # å¤§é€²åº¦å ±å‘Šå’Œå®šæœŸå‚™ä»½
    if t % 100000 == 0:
        curiosity_stats = curiosity_explorer.get_statistics()
        print(f"\nğŸš€ === è¨“ç·´é€²åº¦å ±å‘Š (æ­¥æ•¸: {t}) ===")
        print(f"ğŸ“Š å›åˆç¸½æ•¸: {episode_count}")
        print(f"ğŸ’¾ Bufferå¤§å°: {replay_buffer.size}")
        print(f"ğŸ† æœ€ä½³ç¸½çå‹µ: {best_reward:.2f}")
        print(f"ğŸ§  ç´¯è¨ˆå¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['total_intrinsic_reward']:.2f}")
        print(f"ğŸ“ˆ å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['average_intrinsic_reward']:.4f}")
        print(f"ğŸ”„ å¥½å¥‡å¿ƒæ›´æ–°æ¬¡æ•¸: {curiosity_stats['update_count']}")
        
        # å®šæœŸè‡ªå‹•å‚™ä»½åˆ°Google Drive
        checkpoint_name = f"checkpoint_{t//1000}k"
        checkpoint_data = {
            'model_state_dict': ddpg_agent.state_dict(),
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward
        }
        checkpoint_meta = {
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'checkpoint': True
        }
        
        if gdrive_sync.save_model(checkpoint_data, checkpoint_name, checkpoint_meta):
            print(f"ğŸ“¤ å®šæœŸå‚™ä»½å·²ä¿å­˜åˆ° Google Drive")
        
        print("=" * 50)

# =================================================================
# 5. è¨“ç·´å®Œæˆå’Œç¸½çµ
# =================================================================
final_model_path = f"final_{MODEL_NAME}.pth"

# ä¿å­˜æœ€çµ‚æ¨¡å‹ (åŒ…å«å®Œæ•´ç‹€æ…‹)
final_checkpoint = {
    'model_state_dict': ddpg_agent.state_dict(),
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS,
    'best_reward': best_reward,
    'final_training': True
}
torch.save(final_checkpoint, final_model_path)

# è‡ªå‹•ä¿å­˜æœ€çµ‚æ¨¡å‹åˆ°Google Drive
final_metadata = {
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS, 
    'best_reward': best_reward,
    'training_completed': True
}
gdrive_sync.save_model(final_checkpoint, f"final_{MODEL_NAME}", final_metadata)

curiosity_final_stats = curiosity_explorer.get_statistics()

print(f"\nğŸ‰ ç´”å¥½å¥‡å¿ƒè¨“ç·´å®Œæˆï¼")
print(f"ğŸ† æœ€ä½³å›åˆçå‹µ: {best_reward:.2f}")
print(f"ğŸ§  ç¸½å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['total_intrinsic_reward']:.2f}")
print(f"ğŸ“Š å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['average_intrinsic_reward']:.4f}")
print(f"ğŸ”„ ç¸½å›åˆæ•¸: {episode_count}")
print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {best_model_path}, {final_model_path}")

# æ¸…ç†
env.close()
logger.close()
print("ğŸ ç´”å¥½å¥‡å¿ƒå¯¦é©—å®Œæˆï¼")