#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BCé è¨“ç·´ + ç´”PPOè¨“ç·´ç³»çµ±
ç°¡åŒ–ç‰ˆæœ¬ï¼šç§»é™¤å¥½å¥‡å¿ƒã€çå‹µå¡‘å½¢ã€LLMæ•™ç·´ã€CMA-ESç­‰è¤‡é›œçµ„ä»¶
å°ˆæ³¨æ–¼æ¸¬è©¦å°ˆå®¶æ•¸æ“šé è¨“ç·´çš„æ•ˆæœ
"""

import numpy as np
import torch
import os
import sys
from sai_rl import SAIClient 
from ppo_cma_model import PPOCMA
from utils import Preprocessor
from logger import TensorBoardLogger
from simple_bc_integration import BCPretrainer
from gdrive_utils import SimpleGDriveSync
import glob

def safe_float(value):
    """å®‰å…¨åœ°å°‡numpy arrayæˆ–æ¨™é‡è½‰æ›ç‚ºfloat"""
    if hasattr(value, 'item'):
        return value.item()
    else:
        return float(value)

# =================================================================
# 1. ç’°å¢ƒè¨­ç½®
# =================================================================
print("ğŸš€ åˆå§‹åŒ–BCé è¨“ç·´ + ç´”PPOç³»çµ±")

# å‰µå»ºSAIç’°å¢ƒ
sai = SAIClient(
    comp_id="booster-soccer-showdown", 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)
env = sai.make_env()
print(f"âœ… ç’°å¢ƒå·²å‰µå»º | è§€å¯Ÿ: {env.observation_space} | å‹•ä½œ: {env.action_space}")

# =================================================================
# 2. è¶…åƒæ•¸é…ç½®
# =================================================================
TOTAL_TIMESTEPS = 1000000         # è¨“ç·´æ­¥æ•¸ (ç°¡åŒ–ç‰ˆæ¸›å°‘50%)
MODEL_NAME = "BC-PPO-Simple"
BATCH_SIZE = 512                  # ç°¡åŒ–ç‰ˆæ¸›å°batch size
BUFFER_CAPACITY = 4096            # å°æ‡‰æ¸›å°buffer
LEARNING_RATE_ACTOR = 3e-4
LEARNING_RATE_CRITIC = 1e-3
HIDDEN_DIMS = [256, 256]        # ç°¡åŒ–ç¶²çµ¡çµæ§‹
SAVE_FREQ = 50

# PPOåƒæ•¸
GAMMA = 0.99
GAE_LAMBDA = 0.95
CLIP_EPSILON = 0.2
ENTROPY_COEF = 0.01
PPO_EPOCHS = 5                    # æ¸›å°‘PPO epochs
MAX_GRAD_NORM = 0.5

# =================================================================
# 3. å‹•ä½œè½‰æ›å‡½æ•¸
# =================================================================
def action_function(policy):
    """å°‡ç­–ç•¥è¼¸å‡ºè½‰æ›ç‚ºç’°å¢ƒå‹•ä½œ"""
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (expected_bounds[1] - expected_bounds[0])
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return env.action_space.low + (env.action_space.high - env.action_space.low) * bounded_percent

# =================================================================
# 4. æ¨¡å‹åˆå§‹åŒ–
# =================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ ä½¿ç”¨è¨­å‚™: {device}")

# å‰µå»ºç°¡åŒ–ç‰ˆPPOæ¨¡å‹ (ç§»é™¤CMA-ESåƒæ•¸)
ppo_agent = PPOCMA(
    state_dim=45,
    action_dim=env.action_space.shape[0],
    lr_actor=LEARNING_RATE_ACTOR,
    lr_critic=LEARNING_RATE_CRITIC,
    gamma=GAMMA,
    gae_lambda=GAE_LAMBDA,
    clip_epsilon=CLIP_EPSILON,
    entropy_coef=ENTROPY_COEF,
    hidden_dims=HIDDEN_DIMS,
    buffer_capacity=BUFFER_CAPACITY,
    batch_size=BATCH_SIZE,
    ppo_epochs=PPO_EPOCHS,
    max_grad_norm=MAX_GRAD_NORM,
    cma_population_size=0,  # ç¦ç”¨CMA-ES
    cma_sigma=0.0,          # ç¦ç”¨CMA-ES
    cma_update_freq=999999  # ç¦ç”¨CMA-ES
)

# æ‰‹å‹•å°‡æ¨¡å‹ç§»åˆ°æ­£ç¢ºçš„è¨­å‚™
ppo_agent = ppo_agent.to(device)

# æ‰‹å‹•ç¦ç”¨CMA-ESæ©Ÿåˆ¶
ppo_agent.use_cma = False
print("âœ… CMA-ESæ©Ÿåˆ¶å·²ç¦ç”¨ï¼Œä½¿ç”¨ç´”PPO")

print("âœ… PPOæ¨¡å‹å·²å‰µå»º")

# =================================================================
# 4.5. Google Driveè¨­ç½®å’Œæ¨¡å‹é¸æ“‡
# =================================================================

# åˆå§‹åŒ–Google DriveåŒæ­¥
try:
    gdrive_sync = SimpleGDriveSync()
    gdrive_available = gdrive_sync.gdrive_path is not None
    print(f"ğŸ”— Google Drive: {'âœ… å·²é€£æ¥' if gdrive_available else 'âŒ æœªé€£æ¥'}")
except Exception as e:
    print(f"âš ï¸ Google Driveåˆå§‹åŒ–å¤±æ•—: {e}")
    gdrive_sync = None
    gdrive_available = False

def choose_model_loading():
    """é¸æ“‡è¼‰å…¥æ¨¡å‹æˆ–é‡æ–°é–‹å§‹"""
    print("\n" + "="*50)
    print("ğŸ¤” BC-PPOè¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°æ¨¡å‹
    local_models = (glob.glob(f"*{MODEL_NAME}*.pth") + 
                   glob.glob(f"best_*.pth") + 
                   glob.glob(f"final_*.pth") +
                   glob.glob(f"checkpoint_*.pth"))
    
    # æª¢æŸ¥Google Driveæ¨¡å‹
    gdrive_models = []
    if gdrive_sync and gdrive_available:
        try:
            gdrive_models = gdrive_sync.list_saved_models(MODEL_NAME.replace("-", "_"))
        except:
            gdrive_models = []
    
    if local_models or gdrive_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹:")
        
        all_models = []
        if local_models:
            print("\næœ¬åœ°æ¨¡å‹:")
            for i, model in enumerate(local_models):
                print(f"  {i+1}. {model}")
                all_models.append(('local', model))
        
        if gdrive_models:
            print(f"\nGoogle Driveæ¨¡å‹ (å‰5å€‹):")
            for i, model in enumerate(gdrive_models[:5]):
                print(f"  {len(local_models)+i+1}. {model['name']} ({model['modified'].strftime('%Y-%m-%d %H:%M')})")
                all_models.append(('gdrive', model['path']))
        
        print(f"\n{len(all_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´ (åŒ…å«BCé è¨“ç·´)")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(all_models) + 1:
                    return None, None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(all_models):
                    model_type, model_path = all_models[choice_num - 1]
                    return model_type, model_path
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None, None

# é¸æ“‡æ¨¡å‹è¼‰å…¥æ–¹å¼
model_type, model_path = choose_model_loading()

# è¼‰å…¥æ¨¡å‹ (å¦‚æœé¸æ“‡äº†)
start_episode = 0
if model_path:
    try:
        if model_type == 'gdrive':
            print(f"â¬‡ï¸ å¾Google Driveè¼‰å…¥æ¨¡å‹: {model_path}")
            checkpoint = gdrive_sync.load_model(model_path)
        else:
            print(f"ğŸ“‚ è¼‰å…¥æœ¬åœ°æ¨¡å‹: {model_path}")
            checkpoint = torch.load(model_path, map_location=device)
        
        if checkpoint:
            ppo_agent.load_state_dict(checkpoint['model_state_dict'])
            start_episode = checkpoint.get('episode', 0)
            start_timestep = checkpoint.get('timestep', 0)
            previous_best = checkpoint.get('best_reward', -np.inf)
            
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
            print(f"   èµ·å§‹Episode: {start_episode}")
            print(f"   èµ·å§‹æ­¥æ•¸: {start_timestep}")
            print(f"   æ­·å²æœ€ä½³çå‹µ: {previous_best:.2f}")
            
            # å¦‚æœè¼‰å…¥æ¨¡å‹ï¼Œè·³éBCé è¨“ç·´
            skip_bc = True
        else:
            print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå°‡å¾é ­é–‹å§‹")
            skip_bc = False
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
        print("å°‡å¾é ­é–‹å§‹è¨“ç·´")
        skip_bc = False
else:
    skip_bc = False

# =================================================================
# 5. BCé è¨“ç·´ 
# =================================================================
expert_data_path = "../data/dataset_kick.npz"
if not skip_bc and os.path.exists(expert_data_path):
    print("ğŸ¯ é–‹å§‹BCé è¨“ç·´...")
    bc_pretrainer = BCPretrainer(ppo_agent, expert_data_path, device)
    bc_loss = bc_pretrainer.pretrain(epochs=50)
    
    # è©•ä¼°BCæ€§èƒ½
    bc_performance = bc_pretrainer.evaluate_bc_performance()
    if bc_performance:
        print(f"ğŸ“Š BCé è¨“ç·´å®Œæˆ:")
        print(f"   æœ€çµ‚æå¤±: {bc_loss:.6f}")
        print(f"   MSE: {bc_performance['mse']:.6f}")
        print(f"   MAE: {bc_performance['mae']:.6f}")  
        print(f"   å¹³å‡ç›¸é—œä¿‚æ•¸: {bc_performance['avg_correlation']:.4f}")
    
    print("âœ… BCé è¨“ç·´å®Œæˆï¼Œé–‹å§‹PPOå¾®èª¿...")
elif skip_bc:
    print("ğŸ”„ è¼‰å…¥å·²è¨“ç·´æ¨¡å‹ï¼Œè·³éBCé è¨“ç·´")
else:
    print("âš ï¸ æœªæ‰¾åˆ°å°ˆå®¶æ•¸æ“šï¼Œåƒ…ä½¿ç”¨PPOè¨“ç·´")

# =================================================================
# 6. è¨“ç·´è¨­ç½®
# =================================================================
logger = TensorBoardLogger(f"simplified_bc_ppo_{MODEL_NAME}")
episode_count = 0
best_reward = -np.inf
t = 0

# åˆå§‹åŒ–ç’°å¢ƒ
current_obs, info = env.reset()
state = Preprocessor().modify_state(current_obs, info)[0]
state = torch.tensor(state, dtype=torch.float32).to(device)

# è¨“ç·´è®Šæ•¸
episode_reward = 0
episode_steps = 0

# åˆå§‹åŒ–æœ€ä½³çå‹µ (è€ƒæ…®è¼‰å…¥çš„æ¨¡å‹)
if model_path and 'previous_best' in locals():
    best_reward = previous_best
    episode_count = start_episode

print(f"ğŸš€ é–‹å§‹PPOè¨“ç·´ ({TOTAL_TIMESTEPS:,} æ­¥)")
print("="*60)

# =================================================================
# 7. ä¸»è¨“ç·´å¾ªç’°
# =================================================================
for t in range(1, TOTAL_TIMESTEPS + 1):
    # PPOå‹•ä½œé¸æ“‡
    with torch.no_grad():
        action, log_prob, value = ppo_agent.get_action(state.cpu().numpy())
    
    # åŸ·è¡Œå‹•ä½œ
    bounded_action = action_function(action)
    next_obs, reward, done, _, info = env.step(bounded_action)
    
    # è™•ç†ä¸‹ä¸€ç‹€æ…‹
    if not done:
        next_state = Preprocessor().modify_state(next_obs, info)[0]
        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)
    else:
        next_state = None
    
    # å­˜å„²ç¶“é©— (åªä½¿ç”¨ç’°å¢ƒåŸç”Ÿçå‹µ)
    ppo_agent.store_transition(
        state.cpu().numpy(), action, reward, 
        next_state.cpu().numpy() if next_state is not None else None, 
        done, log_prob, value
    )
    
    # ç´¯è¨ˆçµ±è¨ˆ
    episode_reward += reward
    episode_steps += 1
    
    # æ›´æ–°æ¨¡å‹ (ç•¶ç·©è¡å€æœ‰è¶³å¤ æ•¸æ“šæ™‚)
    if ppo_agent.buffer.size >= BATCH_SIZE:
        actor_loss, critic_loss, candidate_params = ppo_agent.update()
        
        # è¨˜éŒ„è¨“ç·´ä¿¡æ¯
        if actor_loss is not None:
            logger.log({
                'ppo/policy_loss': actor_loss,
                'ppo/value_loss': critic_loss,
                'training/learning_rate': LEARNING_RATE_ACTOR,
                'environment/episode_length': episode_steps,
                'environment/episode_reward': episode_reward,
                'ppo/update_counter': ppo_agent.update_counter
            }, step=t)
    
    # EpisodeçµæŸè™•ç†
    if done:
        episode_count += 1
        
        # æ›´æ–°æœ€ä½³çå‹µ
        if episode_reward > best_reward:
            best_reward = episode_reward
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_data = {
                'model_state_dict': ppo_agent.state_dict(),
                'episode': episode_count,
                'timestep': t,
                'best_reward': best_reward,
                'algorithm': 'BC-PPO'
            }
            
            # æœ¬åœ°ä¿å­˜
            torch.save(best_model_data, f"best_{MODEL_NAME}.pth")
            
            # Google Driveä¿å­˜
            if gdrive_sync and gdrive_available:
                try:
                    gdrive_sync.save_model(
                        best_model_data, 
                        f"best_{MODEL_NAME}", 
                        {
                            'episode': episode_count,
                            'timestep': t,
                            'best_reward': best_reward,
                            'model_type': 'best',
                            'algorithm': 'BC-PPO'
                        }
                    )
                    print(f"ğŸ“¤ æœ€ä½³æ¨¡å‹å·²ä¸Šå‚³Google Drive (çå‹µ: {best_reward:.2f})")
                except Exception as e:
                    print(f"âš ï¸ Google Driveä¸Šå‚³å¤±æ•—: {e}")
        
        # è¼¸å‡ºé€²åº¦
        if episode_count % 10 == 0:
            print(f"Episode {episode_count:4d} | "
                  f"çå‹µ: {episode_reward:7.2f} | "
                  f"æœ€ä½³: {best_reward:7.2f} | "
                  f"æ­¥æ•¸: {episode_steps:3d} | "
                  f"æ™‚é–“æ­¥: {t:7d}")
        
        # é‡ç½®ç’°å¢ƒ
        current_obs, info = env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state, dtype=torch.float32).to(device)
        
        # é‡ç½®è®Šæ•¸
        episode_reward = 0
        episode_steps = 0
    else:
        state = next_state
    
    # å®šæœŸä¿å­˜å’Œå ±å‘Š
    if t % 50000 == 0:
        print(f"\nğŸ“Š === è¨“ç·´é€²åº¦å ±å‘Š (æ­¥æ•¸: {t:,}) ===")
        print(f"å›åˆç¸½æ•¸: {episode_count}")
        print(f"æœ€ä½³çå‹µ: {best_reward:.2f}")
        print(f"PPOæ›´æ–°æ¬¡æ•¸: {ppo_agent.update_counter}")
        print("="*50)
        
        # å®šæœŸä¿å­˜checkpoint
        checkpoint_data = {
            'model_state_dict': ppo_agent.state_dict(),
            'episode': episode_count,
            'timestep': t,
            'best_reward': best_reward,
            'algorithm': 'BC-PPO'
        }
        
        # æœ¬åœ°ä¿å­˜
        checkpoint_name = f"checkpoint_{MODEL_NAME}_step_{t}"
        torch.save(checkpoint_data, f"{checkpoint_name}.pth")
        
        # Google Driveä¿å­˜
        if gdrive_sync and gdrive_available:
            try:
                gdrive_sync.save_model(
                    checkpoint_data,
                    checkpoint_name,
                    {
                        'episode': episode_count,
                        'timestep': t,
                        'best_reward': best_reward,
                        'model_type': 'checkpoint',
                        'algorithm': 'BC-PPO'
                    },
                    add_timestamp=False
                )
                print(f"ğŸ“¤ Checkpointå·²ä¸Šå‚³Google Drive")
            except Exception as e:
                print(f"âš ï¸ Google Driveä¸Šå‚³å¤±æ•—: {e}")

# =================================================================
# 8. è¨“ç·´å®Œæˆ
# =================================================================
print(f"\nğŸ‰ è¨“ç·´å®Œæˆ!")
print(f"ğŸ† æœ€ä½³çå‹µ: {best_reward:.2f}")
print(f"ğŸ“Š ç¸½å›åˆæ•¸: {episode_count}")
print(f"ğŸ¯ PPOæ›´æ–°æ¬¡æ•¸: {ppo_agent.update_counter}")

# ä¿å­˜æœ€çµ‚æ¨¡å‹
final_checkpoint = {
    'model_state_dict': ppo_agent.state_dict(),
    'episode': episode_count,
    'timestep': TOTAL_TIMESTEPS,
    'best_reward': best_reward,
    'final_training': True,
    'algorithm': 'BC-PPO'
}

# æœ¬åœ°ä¿å­˜
torch.save(final_checkpoint, f"final_{MODEL_NAME}.pth")

# Google Driveä¿å­˜
if gdrive_sync and gdrive_available:
    try:
        gdrive_sync.save_model(
            final_checkpoint,
            f"final_{MODEL_NAME}",
            {
                'episode': episode_count,
                'timestep': TOTAL_TIMESTEPS,
                'best_reward': best_reward,
                'training_completed': True,
                'model_type': 'final',
                'algorithm': 'BC-PPO'
            }
        )
        print(f"ğŸ“¤ æœ€çµ‚æ¨¡å‹å·²ä¸Šå‚³Google Drive")
    except Exception as e:
        print(f"âš ï¸ æœ€çµ‚æ¨¡å‹Google Driveä¸Šå‚³å¤±æ•—: {e}")

logger.close()
print(f"âœ… æ¨¡å‹å·²ä¿å­˜: final_{MODEL_NAME}.pth")
print("ğŸ BCé è¨“ç·´ + ç´”PPOè¨“ç·´å®Œæˆ!")