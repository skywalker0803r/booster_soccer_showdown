# -*- coding: utf-8 -*-
# main_ppo_cma_curiosity.py
# ä½¿ç”¨PPO-CMAæ”¹é€²çš„ç´”å¥½å¥‡å¿ƒé©…å‹•è¨“ç·´è…³æœ¬ + LLMè¼”åŠ©çå‹µå¡‘å½¢

import numpy as np
import torch
from sai_rl import SAIClient 
from ppo_cma_model import PPOCMA  # ä½¿ç”¨PPO-CMAæ›¿ä»£TD3
from utils import Preprocessor
from logger import TensorBoardLogger
from curiosity_module import CuriosityDrivenExploration
from gdrive_utils import SimpleGDriveSync
# [AI-Integrate] å°å…¥LLMè¼”åŠ©æ¨¡çµ„
import sys
sys.path.append('..')  # æ·»åŠ ä¸Šç´šç›®éŒ„åˆ°è·¯å¾‘
from llm_coach import LLMCoach
from reward_shaper import RewardShaper

# [AI-Integrate] å¹«åŠ©å‡½æ•¸ï¼šå®‰å…¨æå–æ¨™é‡å€¼
def safe_float(value):
    """å®‰å…¨åœ°å°‡ numpy array æˆ–æ¨™é‡è½‰æ›ç‚º floatï¼Œé¿å… deprecation è­¦å‘Š"""
    if hasattr(value, 'item'):
        return value.item()  # numpy array -> scalar
    else:
        return float(value)  # å·²ç¶“æ˜¯æ¨™é‡

# =================================================================
# 1. åˆå§‹åŒ– SAIClient å’Œç’°å¢ƒ
# =================================================================
sai = SAIClient(
    comp_id="booster-soccer-showdown", 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)

# ğŸ¯ å‰µå»ºç„¡æ™‚é–“æ‡²ç½°çš„ç’°å¢ƒ
env = sai.make_env()

# # ğŸš« ç§»é™¤æ™‚é–“æ‡²ç½° - ä¿®æ”¹çå‹µé…ç½®
# print("ğŸ¯ æ­£åœ¨ç§»é™¤æ™‚é–“æ‡²ç½°...")
# try:
#     # å˜—è©¦è¨ªå•å’Œä¿®æ”¹çå‹µé…ç½®
#     if hasattr(env, 'reward_config') or hasattr(env.unwrapped, 'reward_config'):
#         reward_config = getattr(env, 'reward_config', None) or getattr(env.unwrapped, 'reward_config', None)
#         if reward_config and isinstance(reward_config, dict):
#             # ç§»é™¤æ‰€æœ‰æ™‚é–“ç›¸é—œæ‡²ç½°
#             if 'steps' in reward_config:
#                 original_steps = reward_config['steps']
#                 reward_config['steps'] = 0.0  # è¨­ç‚º0ç§»é™¤æ™‚é–“æ‡²ç½°
#                 print(f"âœ… æ™‚é–“æ‡²ç½°å·²ç§»é™¤: {original_steps} â†’ 0.0")
#             if 'step_penalty' in reward_config:
#                 reward_config['step_penalty'] = 0.0
#                 print(f"âœ… æ­¥æ•¸æ‡²ç½°å·²ç§»é™¤")
#         else:
#             print("âš ï¸ ç„¡æ³•è¨ªå•reward_configï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
#     else:
#         print("âš ï¸ ç’°å¢ƒä¸æ”¯æŒreward_configä¿®æ”¹ï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
# except Exception as e:
#     print(f"âš ï¸ ä¿®æ”¹çå‹µé…ç½®å¤±æ•—: {e}ï¼Œå°‡é€šéå¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½°")
print(f"ç’°å¢ƒå·²å‰µå»ºã€‚è§€å¯Ÿç©ºé–“: {env.observation_space} | å‹•ä½œç©ºé–“: {env.action_space}")

N_FEATURES = 45 
N_ACTIONS = env.action_space.shape[0]

# =================================================================
# 2. è¼”åŠ©å‡½æ•¸ï¼šå‹•ä½œè½‰æ› (ä¿æŒä¸è®Š)
# =================================================================
def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (
        expected_bounds[1] - expected_bounds[0]
    )
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return (
        env.action_space.low
        + (env.action_space.high - env.action_space.low) * bounded_percent
    )

# =================================================================
# 3. ğŸš€ A100æœ€ä½³åŒ–è¶…åƒæ•¸è¨­ç½® (PPO-CMA + ç´”å¥½å¥‡å¿ƒç‰ˆ)
# =================================================================
TOTAL_TIMESTEPS = 2000000          # å¢åŠ ç¸½è¨“ç·´æ­¥æ•¸ï¼Œå……åˆ†åˆ©ç”¨A100
MODEL_NAME = "Booster-PPOCMA-A100-PureOriginal-v1"
BUFFER_CAPACITY = 8192             # PPOç·©è¡å€ï¼Œç‚ºç™¼æ®A100æ•ˆèƒ½ï¼Œè¨­ç‚ºBATCH_SIZEçš„å€æ•¸
BATCH_SIZE = 1024                  # åŠ å¤§æ‰¹æ¬¡ä»¥æœ€å¤§åŒ–A100 GPUåˆ©ç”¨ç‡
LEARNING_RATE_ACTOR = 3e-4         # Actorå­¸ç¿’ç‡
LEARNING_RATE_CRITIC = 1e-3        # Criticå­¸ç¿’ç‡
NEURONS = [512, 512, 256]          # æ›´å¤§æ›´æ·±çš„ç¶²çµ¡æ¶æ§‹
UPDATE_FREQ = BUFFER_CAPACITY      # PPOåœ¨bufferæ»¿æ™‚æ›´æ–°
SAVE_FREQ = 25                     # æ›´é »ç¹ä¿å­˜

# PPO-CMA ç‰¹æœ‰åƒæ•¸
GAMMA = 0.99                       # æŠ˜æ‰£å› å­
GAE_LAMBDA = 0.95                  # GAE lambdaåƒæ•¸
CLIP_EPSILON = 0.2                 # PPOè£åˆ‡ä¿‚æ•¸
ENTROPY_COEF = 0.01               # ç†µæ­£å‰‡åŒ–ä¿‚æ•¸
PPO_EPOCHS = 15                    # æ¯æ¬¡æ›´æ–°çš„PPO epochæ•¸
MAX_GRAD_NORM = 0.5               # æ¢¯åº¦è£åˆ‡
CMA_POPULATION_SIZE = 64           # CMA-ESç¨®ç¾¤å¤§å°ï¼ˆæ‰‹å‹•è¨­ç½®ä»¥åŠ å¼·æ¢ç´¢ï¼‰
CMA_SIGMA = 0.1                    # CMA-ESåˆå§‹æ­¥é•·
CMA_UPDATE_FREQ = 10               # CMA-ESæ›´æ–°é »ç‡

# å¥½å¥‡å¿ƒæ¨¡çµ„åƒæ•¸ (A100å„ªåŒ–è¨­ç½®)
INTRINSIC_REWARD_SCALE = 0.8      # ç¨å¾®é™ä½ä»¥å¹³è¡¡å¤§batchæ•ˆæ‡‰
CURIOSITY_UPDATE_FREQ = 1

# åˆå§‹åŒ–PPO-CMAæ¨¡å‹
ppo_cma_agent = PPOCMA(
    state_dim=N_FEATURES,
    action_dim=N_ACTIONS,
    hidden_dims=NEURONS,
    lr_actor=LEARNING_RATE_ACTOR,
    lr_critic=LEARNING_RATE_CRITIC,
    gamma=GAMMA,
    gae_lambda=GAE_LAMBDA,
    clip_epsilon=CLIP_EPSILON,
    entropy_coef=ENTROPY_COEF,
    max_grad_norm=MAX_GRAD_NORM,
    ppo_epochs=PPO_EPOCHS,
    batch_size=BATCH_SIZE,
    buffer_capacity=BUFFER_CAPACITY,
    cma_population_size=CMA_POPULATION_SIZE,
    cma_sigma=CMA_SIGMA,
    cma_update_freq=CMA_UPDATE_FREQ
)

# åˆå§‹åŒ–ç´”å¥½å¥‡å¿ƒæ¨¡çµ„
curiosity_explorer = CuriosityDrivenExploration(
    state_dim=N_FEATURES,
    action_dim=N_ACTIONS, 
    intrinsic_reward_scale=INTRINSIC_REWARD_SCALE
)

# [AI-Integrate] åˆå§‹åŒ–LLMè¼”åŠ©æ¨¡çµ„
# ä½¿ç”¨ Gemini API é€²è¡Œæ™ºèƒ½æ±ºç­–
GEMINI_API_KEY = "AIzaSyDUOIGCWDJkY98gi5QcrKtWkxxB61Qhmi0"
llm_coach = LLMCoach(api_key=GEMINI_API_KEY, use_llm=True)
reward_shaper = RewardShaper()
current_weights = llm_coach.current_weights
episode_stats_buffer = []  # ç”¨æ–¼å­˜å„²æœ€è¿‘å¹¾å€‹å›åˆçš„è¡¨ç¾

# =================================================================
# ğŸ”„ æ¨¡å‹è¼‰å…¥é¸æ“‡å’ŒGoogle Driveè¨­ç½®
# =================================================================

# åˆå§‹åŒ–Google DriveåŒæ­¥ (å¸¶éŒ¯èª¤è™•ç†)
try:
    gdrive_sync = SimpleGDriveSync()
    gdrive_available = gdrive_sync.gdrive_path is not None
    print(f"ğŸ”— Google Driveç‹€æ…‹: {'âœ… å·²é€£æ¥' if gdrive_available else 'âŒ æœªé€£æ¥ (åƒ…æœ¬åœ°ä¿å­˜)'}")
except Exception as e:
    print(f"âš ï¸ Google Driveåˆå§‹åŒ–å¤±æ•—: {e}")
    gdrive_sync = None
    gdrive_available = False

# è©¢å•æ˜¯å¦è¼‰å…¥èˆŠæ¨¡å‹
def choose_model_loading():
    print("\n" + "="*50)
    print("ğŸ¤” PPO-CMAè¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°å·²æœ‰æ¨¡å‹
    import glob
    local_models = glob.glob(f"*{MODEL_NAME}*.pth") + glob.glob(f"best_*.pth") + glob.glob(f"final_*.pth")
    
    # æª¢æŸ¥Google Driveæ¨¡å‹
    gdrive_models = gdrive_sync.list_saved_models(MODEL_NAME.replace("-", "_")) if gdrive_sync else []
    
    if local_models or gdrive_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹:")
        
        all_models = []
        if local_models:
            print("\næœ¬åœ°æ¨¡å‹:")
            for i, model in enumerate(local_models):
                print(f"  {i+1}. {model}")
                all_models.append(('local', model))
        
        if gdrive_models:
            print(f"\nGoogle Driveæ¨¡å‹ (å‰5å€‹):")
            for i, model in enumerate(gdrive_models[:5]):
                print(f"  {len(local_models)+i+1}. {model['name']} ({model['modified'].strftime('%Y-%m-%d %H:%M')})")
                all_models.append(('gdrive', model['path']))
        
        print(f"\n{len(all_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(all_models) + 1:
                    return None, None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(all_models):
                    model_type, model_path = all_models[choice_num - 1]
                    return model_type, model_path
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None, None

model_type, model_path = choose_model_loading()

# ğŸš€ A100 GPUè¨­ç½®èˆ‡æ··åˆç²¾åº¦
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ppo_cma_agent.to(device)
curiosity_explorer.to(device)

# A100æ··åˆç²¾åº¦åŠ é€Ÿ
scaler = torch.cuda.amp.GradScaler()
print(f"âœ… A100æ··åˆç²¾åº¦è¨“ç·´å·²å•Ÿç”¨ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ”¥ GPUè¨˜æ†¶é«”å„ªåŒ–ï¼šæ··åˆç²¾åº¦å¯ç¯€çœç´„40% VRAM")

# è¼‰å…¥æ¨¡å‹ (å¦‚æœé¸æ“‡äº†)
start_episode = 0
if model_path:
    try:
        if model_type == 'gdrive':
            # å¾Google Driveè¤‡è£½åˆ°æœ¬åœ°
            import shutil
            local_path = f"loaded_{MODEL_NAME}.pth"
            shutil.copy2(model_path, local_path)
            model_path = local_path
        
        print(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            ppo_cma_agent.load_state_dict(checkpoint['model_state_dict'])
            start_episode = checkpoint.get('episode', 0)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (å¾Episode {start_episode}ç¹¼çºŒ)")
        else:
            ppo_cma_agent.load_state_dict(checkpoint)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (ç‹€æ…‹dictæ ¼å¼)")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ å°‡å¾é ­é–‹å§‹è¨“ç·´")
        start_episode = 0

print(f"ğŸš€ é–‹å§‹è¨“ç·´ (èµ·å§‹Episode: {start_episode})")

# åˆå§‹åŒ–è¨˜éŒ„å™¨
logger = TensorBoardLogger(model_name=MODEL_NAME) 

# è¿½è¹¤è®Šé‡
episode_reward_sum = 0
episode_intrinsic_reward_sum = 0
episode_extrinsic_reward_sum = 0  # åˆ†åˆ¥è¿½è¹¤åŸå§‹çå‹µ
episode_shaped_reward_sum = 0     # [AI-Integrate] è¿½è¹¤LLMå¡‘å½¢çå‹µ
episode_count = 0
episode_steps = 0
best_reward = -np.inf
best_model_path = f"best_{MODEL_NAME}.pth"

print(f"ğŸš€ A100æœ€ä½³åŒ– PPO-CMA + ç´”åŸå§‹çå‹µè¨“ç·´é–‹å§‹ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ¯ PPO-CMAæ”¹é€²ç‰¹æ€§ï¼š")
print(f"   â€¢ PPO Clipped Surrogate: âœ… (Îµ={CLIP_EPSILON})")
print(f"   â€¢ CMA-ES Parameter Evolution: âœ… (Ïƒ={CMA_SIGMA})")
print(f"   â€¢ Generalized Advantage Estimation: âœ… (Î»={GAE_LAMBDA})")
print(f"   â€¢ Entropy Regularization: âœ… (Î²={ENTROPY_COEF})")
print(f"ğŸ”¥ A100å„ªåŒ–é…ç½®ï¼š")
print(f"   â€¢ Batch Size: {BATCH_SIZE}")
print(f"   â€¢ Buffer Capacity: {BUFFER_CAPACITY}")
print(f"   â€¢ Network Size: {NEURONS} (æ›´å¤§æ›´æ·±)")
print(f"   â€¢ Actor LR: {LEARNING_RATE_ACTOR}, Critic LR: {LEARNING_RATE_CRITIC}")
print(f"   â€¢ PPO Epochs: {PPO_EPOCHS}")
print(f"   â€¢ CMA Update Freq: {CMA_UPDATE_FREQ}")
print(f"   â€¢ å…§åœ¨çå‹µç¸®æ”¾: {INTRINSIC_REWARD_SCALE}")
print(f"   â€¢ æ··åˆç²¾åº¦: âœ… (A100å°ˆç”¨)")
print(f"âŒ PBRSçå‹µï¼šå·²ç¦ç”¨")
print(f"âŒ çå‹µå·¥ç¨‹ï¼šå·²ç§»é™¤") 
print(f"âŒ æ™‚é–“æ‡²ç½°ï¼šå·²ç§»é™¤")
print(f"âœ… ç´”åŸå§‹ç’°å¢ƒçå‹µï¼šå·²å•Ÿç”¨")
print(f"âœ… å¥½å¥‡å¿ƒè¼”åŠ©æ¢ç´¢ï¼šå·²å•Ÿç”¨")

# =================================================================
# 4. PPO-CMA + ç´”å¥½å¥‡å¿ƒ è¨“ç·´å¾ªç’°
# =================================================================
current_obs, info = env.reset()
state = Preprocessor().modify_state(current_obs, info)[0] 
state = torch.tensor(state).float().to(device)

for t in range(1, TOTAL_TIMESTEPS + 1):
    # 1. PPO-CMAå‹•ä½œæ¡æ¨£ (åŒ…å«éš¨æ©Ÿæ¢ç´¢)
    raw_action, log_prob, value = ppo_cma_agent.get_action(state)
    
    # åŸ·è¡Œå‹•ä½œ
    action = action_function(raw_action)
    next_obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    
    # ç‹€æ…‹è½‰æ›
    next_state_np = Preprocessor().modify_state(next_obs, info)[0]
    next_state = torch.tensor(next_state_np).float().to(device)

    # =================================================================
    # ğŸ§  LLMè¼”åŠ©çå‹µå¡‘å½¢ + å¥½å¥‡å¿ƒçå‹µè¨ˆç®—
    # =================================================================
    
    # ğŸš« å¾Œè™•ç†ç§»é™¤æ™‚é–“æ‡²ç½° (å¦‚æœç’°å¢ƒé…ç½®ä¿®æ”¹å¤±æ•—)
    processed_reward = reward
    
    # [ä½¿ç”¨è€…è‡ªè¨‚è¦å‰‡] ç•¶åŸå§‹çå‹µéä½æ™‚ï¼Œçµ¦äºˆä¸€å€‹å°çš„æ­¥æ•¸çå‹µä»¥é¼“å‹µæ¢ç´¢
    # ç•¶çå‹µéƒ½å°æ–¼0çš„æ™‚å€™ å¤§æ­¥å¥”è·‘å§
    if reward <= 0:
        step_encouragement_reward = 1  # å¯ä»¥èª¿æ•´çš„æ­¥æ•¸çå‹µå€¼
        processed_reward += step_encouragement_reward
    
    # # æª¢æ¸¬ä¸¦ç§»é™¤å¯èƒ½çš„æ™‚é–“æ‡²ç½°æ¨¡å¼
    # if episode_steps > 10:  # é¿å…åˆæœŸèª¤åˆ¤
    #     # å¦‚æœrewardæ˜¯å›ºå®šçš„å°è² å€¼ï¼Œå¯èƒ½æ˜¯æ™‚é–“æ‡²ç½°
    #     if -1.5 <= reward <= -0.1:  # å…¸å‹çš„æ™‚é–“æ‡²ç½°ç¯„åœ
    #         # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ™‚é–“æ‡²ç½°ï¼ˆæ²’æœ‰å…¶ä»–äº‹ä»¶ï¼‰
    #         if not any(keyword in str(info).lower() for keyword in ['goal', 'fallen', 'success', 'offside']):
    #             processed_reward = 0.0  # ç§»é™¤æ™‚é–“æ‡²ç½°
    #             if t % 10000 == 0:  # å¶çˆ¾æç¤º
    #                 print(f"ğŸš« æª¢æ¸¬åˆ°æ™‚é–“æ‡²ç½° {reward:.3f}ï¼Œå·²ç§»é™¤")
    
    # [AI-Integrate] è¨ˆç®—LLMå¼•å°çš„Shaped Reward (å·²ç¦ç”¨)
    shaped_reward = 0.0
    
    # [AI-Integrate] èåˆçå‹µï¼šåŸå§‹çå‹µ + LLMå¡‘å½¢çå‹µ (Shaped Reward å·²ç¦ç”¨)
    # æ ¹æ“šprompt.txtå»ºè­°èª¿æ•´æ¯”ä¾‹
    total_step_reward = processed_reward
    
    # ğŸ¯ LLMå¢å¼·çå‹µ + å¥½å¥‡å¿ƒæ¨¡çµ„
    final_reward, intrinsic_reward = curiosity_explorer.get_enhanced_reward(
        state.cpu().numpy(),
        raw_action,
        next_state_np,
        total_step_reward  # ä½¿ç”¨LLMå¢å¼·å¾Œçš„çå‹µ
    )
    
    # ç´¯ç©çµ±è¨ˆ
    episode_extrinsic_reward_sum += reward
    episode_intrinsic_reward_sum += intrinsic_reward
    episode_reward_sum += final_reward
    episode_steps += 1
    
    # [AI-Integrate] ç´¯ç©LLMå¡‘å½¢çå‹µçµ±è¨ˆ
    if 'episode_shaped_reward_sum' not in locals():
        episode_shaped_reward_sum = 0.0
    episode_shaped_reward_sum += shaped_reward

    # =================================================================
    # ğŸ“š ç¶“é©—å„²å­˜å’Œæ¨¡å‹æ›´æ–°
    # =================================================================
    
    # å„²å­˜ç¶“é©—åˆ°PPO-CMAç·©è¡å€ (ä½¿ç”¨å¥½å¥‡å¿ƒå¢å¼·çå‹µ)
    ppo_cma_agent.store_transition(
        state.cpu().numpy(), 
        raw_action, 
        final_reward,
        next_state_np, 
        done,
        log_prob,
        value
    )

    # ğŸš€ A100å„ªåŒ– PPO-CMA æ¨¡å‹æ›´æ–°ï¼ˆbufferæ»¿æ™‚æ›´æ–°ï¼‰
    if t % UPDATE_FREQ == 0:
        # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿè¨“ç·´ (autocast å·²æš«æ™‚ç¦ç”¨ä»¥ä¿®å¾©æ¢¯åº¦éŒ¯èª¤)
        actor_loss, critic_loss = ppo_cma_agent.update()
        
        # æ›´æ–°å¥½å¥‡å¿ƒæ¨¡çµ„
        if t % CURIOSITY_UPDATE_FREQ == 0:
            # å¾PPO-CMA bufferä¸­ç²å–ä¸€äº›æ¨£æœ¬ç”¨æ–¼å¥½å¥‡å¿ƒæ›´æ–°
            buffer_data = ppo_cma_agent.buffer.get_all_data()
            if buffer_data is not None:
                states = buffer_data['states'].to(device)
                actions = buffer_data['actions'].to(device)
                next_states = torch.FloatTensor(ppo_cma_agent.buffer.next_states[:ppo_cma_agent.buffer.size]).to(device)
                
                curiosity_stats = curiosity_explorer.update_curiosity(states, actions, next_states)
                
                # è¨˜éŒ„å¥½å¥‡å¿ƒæŒ‡æ¨™
                logger.set_step(t)
                logger.log_scalar("Curiosity/Forward_Loss", curiosity_stats['forward_loss'])
                logger.log_scalar("Curiosity/Inverse_Loss", curiosity_stats['inverse_loss'])
                logger.log_scalar("Curiosity/Avg_Intrinsic_Reward", curiosity_stats['avg_intrinsic_reward'])
        
        # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        if actor_loss is not None and critic_loss is not None:
            logger.set_step(t) 
            logger.log_scalar("Loss/Actor_Loss", actor_loss)
            logger.log_scalar("Loss/Critic_Loss", critic_loss)
        
        # è¨˜éŒ„PPO-CMAç‰¹å®šæŒ‡æ¨™
        ppo_cma_stats = ppo_cma_agent.get_statistics()
        logger.log_scalar("PPOCMA/Update_Counter", ppo_cma_stats['update_counter'])
        logger.log_scalar("PPOCMA/CMA_Updates", ppo_cma_stats['cma_updates'])
        logger.log_scalar("PPOCMA/CMA_Sigma", ppo_cma_stats['cma_sigma'])
        logger.log_scalar("PPOCMA/CMA_Generation", ppo_cma_stats['cma_generation'])

    # =================================================================
    # ğŸ”„ å›åˆçµæŸè™•ç†
    # =================================================================
    if done:
        episode_count += 1

        # [AI-Integrate] æ”¶é›†æ•¸æ“šçµ¦LLMæ•™ç·´
        # æª¢æ¸¬æ˜¯å¦è·Œå€’ï¼ˆæ ¹æ“šæ­¥æ•¸å’Œçå‹µåˆ¤æ–·ï¼‰
        fell_down = episode_steps < 20 or episode_extrinsic_reward_sum < -5.0
        episode_stats_buffer.append({
            'steps': episode_steps,
            'reward': episode_reward_sum,
            'extrinsic_reward': episode_extrinsic_reward_sum,
            'shaped_reward': episode_shaped_reward_sum,
            'fell_down': fell_down
        })

        # è©³ç´°è¨˜éŒ„åˆ†è§£çå‹µ
        logger.log_scalar("Train/Episode_Total_Reward", episode_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Extrinsic_Reward", episode_extrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Intrinsic_Reward", episode_intrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Shaped_Reward", episode_shaped_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Steps", episode_steps, step=t)
        
        # è¨ˆç®—å¥½å¥‡å¿ƒè²¢ç»æ¯”ä¾‹
        if episode_reward_sum != 0:
            curiosity_ratio = episode_intrinsic_reward_sum / abs(episode_reward_sum)
            logger.log_scalar("Train/Curiosity_Contribution_Ratio", curiosity_ratio, step=t)
        
        # [AI-Integrate] æ¯50å€‹Episodeè®“LLMæ•™ç·´èª¿æ•´ç­–ç•¥
        if episode_count % 50 == 0 and len(episode_stats_buffer) >= 10:
            # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
            recent_episodes = episode_stats_buffer[-50:] if len(episode_stats_buffer) >= 50 else episode_stats_buffer
            
            avg_steps = np.mean([ep['steps'] for ep in recent_episodes])
            avg_reward = np.mean([ep['reward'] for ep in recent_episodes])
            fall_rate = np.mean([ep['fell_down'] for ep in recent_episodes])
            avg_shaped_reward = np.mean([ep['shaped_reward'] for ep in recent_episodes])
            
            stats_summary = {
                'avg_steps': avg_steps,
                'avg_reward': avg_reward,
                'fall_rate': fall_rate,
                'avg_shaped_reward': avg_shaped_reward
            }
            
            # æ›´æ–°æ¬Šé‡
            previous_weights = current_weights.copy()
            current_weights = llm_coach.consult(stats_summary)
            
            # è¨˜éŒ„æ•™ç·´æ±ºç­–
            print(f"ğŸ§  LLM Coach ç¬¬{episode_count}å›åˆæ›´æ–°:")
            print(f"   ç•¶å‰éšæ®µ: {llm_coach.phase}")
            print(f"   çµ±è¨ˆæ•¸æ“š: æ­¥æ•¸={safe_float(avg_steps):.1f}, è·Œå€’ç‡={safe_float(fall_rate):.3f}, å¹³å‡çå‹µ={safe_float(avg_reward):.2f}")
            print(f"   æ¬Šé‡è®ŠåŒ–: {previous_weights} â†’ {current_weights}")
            
            # è¨˜éŒ„åˆ° TensorBoard
            logger.log_scalar("Coach/Weight_Balance", current_weights.get('balance', 0), step=t)
            logger.log_scalar("Coach/Weight_Progress", current_weights.get('progress', 0), step=t)
            logger.log_scalar("Coach/Weight_Energy", current_weights.get('energy', 0), step=t)
            logger.log_scalar("Coach/Avg_Steps", avg_steps, step=t)
            logger.log_scalar("Coach/Fall_Rate", fall_rate, step=t)
            logger.log_scalar("Coach/Phase_ID", hash(llm_coach.phase) % 1000, step=t)  # ç°¡å–®çš„ç›¸ä½ç·¨ç¢¼
            
            # è¨˜éŒ„ LLM API çµ±è¨ˆ
            api_stats = llm_coach.get_api_statistics()
            logger.log_scalar("LLM_API/Total_Calls", api_stats['total_calls'], step=t)
            logger.log_scalar("LLM_API/Success_Rate", api_stats['success_rate'], step=t)
            logger.log_scalar("LLM_API/Errors", api_stats['errors'], step=t)
            
            # æ¸…ç©ºéƒ¨åˆ†ç·©è¡ä»¥ä¿æŒè¨˜æ†¶é«”æ•ˆç‡
            episode_stats_buffer = episode_stats_buffer[-100:]  # ä¿ç•™æœ€è¿‘100å€‹å›åˆ
        
        # æª¢æŸ¥æœ€ä½³æ¨¡å‹ä¸¦è‡ªå‹•ä¿å­˜åˆ°Google Drive
        if episode_reward_sum > best_reward:
            best_reward = episode_reward_sum
            
            # ä¿å­˜æ¨¡å‹ç‹€æ…‹ (åŒ…å«å…ƒæ•¸æ“š)
            checkpoint = {
                'model_state_dict': ppo_cma_agent.state_dict(),
                'episode': episode_count + start_episode,
                'timestep': t,
                'best_reward': best_reward,
                'total_reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'episode_steps': episode_steps,
                'ppo_cma_update_counter': ppo_cma_agent.update_counter,
                'cma_updates': ppo_cma_agent.cma_updates
            }
            
            # æœ¬åœ°ä¿å­˜
            torch.save(checkpoint, best_model_path)
            
            # è‡ªå‹•ä¿å­˜åˆ°Google Drive
            metadata = {
                'episode': episode_count + start_episode,
                'timestep': t,
                'reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'steps': episode_steps,
                'algorithm': 'PPO-CMA'
            }
            if gdrive_sync and gdrive_available:
                gdrive_sync.save_model(checkpoint, f"best_{MODEL_NAME}", metadata)
            else:
                print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œåƒ…æœ¬åœ°ä¿å­˜")
            
            print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹!")
            print(f"   ç¸½çå‹µ: {safe_float(episode_reward_sum):.2f}")
            print(f"   åŸå§‹çå‹µ: {safe_float(episode_extrinsic_reward_sum):.2f}")
            print(f"   å¥½å¥‡å¿ƒçå‹µ: {safe_float(episode_intrinsic_reward_sum):.2f}")
            print(f"   å›åˆæ­¥æ•¸: {episode_steps}")
            print(f"   è¨“ç·´æ­¥æ•¸: {t}")
            print(f"   ğŸ“¤ å·²è‡ªå‹•å‚™ä»½åˆ°Google Drive")
        
        # å®šæœŸé€²åº¦å ±å‘Š
        if episode_count % 5 == 0:
            ratio = episode_intrinsic_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            shaped_ratio = episode_shaped_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            ppo_cma_stats = ppo_cma_agent.get_statistics()
            print(f"ğŸ¯ Episode {episode_count:3d} | "
                  f"ç¸½çå‹µ: {safe_float(episode_reward_sum):6.2f} | "
                  f"åŸå§‹: {safe_float(episode_extrinsic_reward_sum):6.2f} | "
                  f"å¡‘å½¢: {safe_float(episode_shaped_reward_sum):5.2f} | "
                  f"å¥½å¥‡å¿ƒ: {safe_float(episode_intrinsic_reward_sum):5.2f} | "
                  f"æ­¥æ•¸: {episode_steps:3d} | "
                  f"éšæ®µ: {llm_coach.phase[:8]} | "
                  f"PPOæ›´æ–°: {ppo_cma_stats['update_counter']} | "
                  f"CMA: {ppo_cma_stats['cma_updates']}")
        
        # é‡ç½®ç’°å¢ƒ
        current_obs, info = env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state).float().to(device)
        
        # é‡è¨­è®Šé‡
        episode_reward_sum = 0
        episode_intrinsic_reward_sum = 0
        episode_extrinsic_reward_sum = 0
        episode_shaped_reward_sum = 0  # [AI-Integrate] é‡è¨­LLMå¡‘å½¢çå‹µ
        episode_steps = 0
    else:
        state = next_state
    
    # å¤§é€²åº¦å ±å‘Šå’Œå®šæœŸå‚™ä»½ (ğŸš€ A100å„ªåŒ–: æ›´é »ç¹å‚™ä»½)
    if t % 10000 == 0:
        curiosity_stats = curiosity_explorer.get_statistics()
        ppo_cma_stats = ppo_cma_agent.get_statistics()
        print(f"\nğŸš€ === PPO-CMAè¨“ç·´é€²åº¦å ±å‘Š (æ­¥æ•¸: {t}) ===")
        print(f"ğŸ“Š å›åˆç¸½æ•¸: {episode_count}")
        print(f"ğŸ’¾ Bufferå¤§å°: {ppo_cma_stats['buffer_size']}")
        print(f"ğŸ† æœ€ä½³ç¸½çå‹µ: {safe_float(best_reward):.2f}")
        print(f"ğŸ§  ç´¯è¨ˆå¥½å¥‡å¿ƒçå‹µ: {safe_float(curiosity_stats['total_intrinsic_reward']):.2f}")
        print(f"ğŸ“ˆ å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {safe_float(curiosity_stats['average_intrinsic_reward']):.4f}")
        print(f"ğŸ”„ å¥½å¥‡å¿ƒæ›´æ–°æ¬¡æ•¸: {curiosity_stats['update_count']}")
        print(f"ğŸ¯ PPOæ›´æ–°æ¬¡æ•¸: {ppo_cma_stats['update_counter']}")
        print(f"ğŸ§¬ CMA-ESæ›´æ–°æ¬¡æ•¸: {ppo_cma_stats['cma_updates']}")
        print(f"ğŸ“ CMA-ESæ­¥é•·Ïƒ: {ppo_cma_stats['cma_sigma']:.6f}")
        print(f"ğŸŒ± CMA-ESä¸–ä»£: {ppo_cma_stats['cma_generation']}")
        
        # å®šæœŸè‡ªå‹•å‚™ä»½åˆ°Google Drive
        checkpoint_name = f"checkpoint_{t//1000}k"
        checkpoint_data = {
            'model_state_dict': ppo_cma_agent.state_dict(),
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'ppo_cma_update_counter': ppo_cma_agent.update_counter,
            'cma_updates': ppo_cma_agent.cma_updates
        }
        checkpoint_meta = {
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'checkpoint': True,
            'algorithm': 'PPO-CMA'
        }
        
        if gdrive_sync and gdrive_available:
            if gdrive_sync.save_model(checkpoint_data, checkpoint_name, checkpoint_meta):
                print(f"ğŸ“¤ å®šæœŸå‚™ä»½å·²ä¿å­˜åˆ° Google Drive")
        else:
            print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œè·³éé›²ç«¯å‚™ä»½")
        
        print("=" * 50)

# =================================================================
# 5. è¨“ç·´å®Œæˆå’Œç¸½çµ
# =================================================================
final_model_path = f"final_{MODEL_NAME}.pth"

# ä¿å­˜æœ€çµ‚æ¨¡å‹ (åŒ…å«å®Œæ•´ç‹€æ…‹)
final_checkpoint = {
    'model_state_dict': ppo_cma_agent.state_dict(),
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS,
    'best_reward': best_reward,
    'final_training': True,
    'ppo_cma_update_counter': ppo_cma_agent.update_counter,
    'cma_updates': ppo_cma_agent.cma_updates
}
torch.save(final_checkpoint, final_model_path)

# è‡ªå‹•ä¿å­˜æœ€çµ‚æ¨¡å‹åˆ°Google Drive
final_metadata = {
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS, 
    'best_reward': best_reward,
    'training_completed': True,
    'algorithm': 'PPO-CMA'
}
if gdrive_sync and gdrive_available:
    gdrive_sync.save_model(final_checkpoint, f"final_{MODEL_NAME}", final_metadata)
    print(f"ğŸ“¤ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ° Google Drive")
else:
    print(f"âš ï¸ Google Driveä¸å¯ç”¨ï¼Œæœ€çµ‚æ¨¡å‹åƒ…æœ¬åœ°ä¿å­˜")

curiosity_final_stats = curiosity_explorer.get_statistics()
ppo_cma_final_stats = ppo_cma_agent.get_statistics()

print(f"\nğŸ‰ PPO-CMA + LLMè¼”åŠ© + å¥½å¥‡å¿ƒè¨“ç·´å®Œæˆï¼")
print(f"ğŸ† æœ€ä½³å›åˆçå‹µ: {safe_float(best_reward):.2f}")
print(f"ğŸ§  ç¸½å¥½å¥‡å¿ƒçå‹µ: {safe_float(curiosity_final_stats['total_intrinsic_reward']):.2f}")
print(f"ğŸ“Š å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {safe_float(curiosity_final_stats['average_intrinsic_reward']):.4f}")
print(f"ğŸ”„ ç¸½å›åˆæ•¸: {episode_count}")
print(f"ğŸ¯ PPOç¸½æ›´æ–°æ¬¡æ•¸: {ppo_cma_final_stats['update_counter']}")
print(f"ğŸ§¬ CMA-ESç¸½æ›´æ–°æ¬¡æ•¸: {ppo_cma_final_stats['cma_updates']}")
print(f"ğŸ“ æœ€çµ‚CMA-ESæ­¥é•·Ïƒ: {ppo_cma_final_stats['cma_sigma']:.6f}")
print(f"ğŸŒ± æœ€çµ‚CMA-ESä¸–ä»£: {ppo_cma_final_stats['cma_generation']}")
print(f"ğŸ§  LLMæ•™ç·´æœ€çµ‚éšæ®µ: {llm_coach.phase}")
print(f"âš–ï¸ æœ€çµ‚æ¬Šé‡é…ç½®: {current_weights}")
print(f"ğŸ“ˆ éšæ®µè®ŠåŒ–æ¬¡æ•¸: {len(llm_coach.phase_history)}")

# LLM API çµ±è¨ˆå ±å‘Š
llm_api_stats = llm_coach.get_api_statistics()
print(f"ğŸ¤– LLM APIçµ±è¨ˆ:")
print(f"   ç¸½èª¿ç”¨æ¬¡æ•¸: {llm_api_stats['total_calls']}")
print(f"   éŒ¯èª¤æ¬¡æ•¸: {llm_api_stats['errors']}")
print(f"   æˆåŠŸç‡: {safe_float(llm_api_stats['success_rate']):.2%}")
print(f"   LLMå•Ÿç”¨: {'âœ…' if llm_api_stats['llm_enabled'] else 'âŒ'}")

print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {best_model_path}, {final_model_path}")

# æ¸…ç†
env.close()
logger.close()
print("ğŸ PPO-CMAç´”å¥½å¥‡å¿ƒå¯¦é©—å®Œæˆï¼")