#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BCé è¨“ç·´ + Stable Baselines3 PPOè¨“ç·´ç³»çµ±
ä½¿ç”¨æˆç†Ÿç©©å®šçš„SB3æ¡†æ¶ï¼ŒAPIç°¡æ½”ï¼ŒåŠŸèƒ½å®Œæ•´
"""

import numpy as np
import torch
import os
import sys
import glob
try:
    import gymnasium as gym
except ImportError:
    import gym
from sai_rl import SAIClient 
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from utils import Preprocessor
from gdrive_utils import SimpleGDriveSync
from PBRS_module import create_pbrs_wrapper
from gym_compatibility import make_gymnasium_compatible, test_compatibility

# =================================================================
# 1. ç’°å¢ƒè¨­ç½®
# =================================================================
print("ğŸš€ åˆå§‹åŒ–BCé è¨“ç·´ + Stable Baselines3 PPOç³»çµ±")

# å‰µå»ºSAIç’°å¢ƒ
sai = SAIClient(
    comp_id="booster-soccer-showdown", 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)

def make_sai_env(use_pbrs=True, pbrs_debug=False):
    """å‰µå»ºSAIç’°å¢ƒçš„å·¥å» å‡½æ•¸"""
    env = sai.make_env()
    print("âœ… SAI åŸå§‹ç’°å¢ƒå‰µå»ºæˆåŠŸ")
    
    # ğŸ”§ æ·»åŠ  Gymnasium å…¼å®¹æ€§
    env = make_gymnasium_compatible(env)
    print("âœ… Gymnasium å…¼å®¹æ€§é©é…å®Œæˆ")
    
    env = Monitor(env)  # æ·»åŠ ç›£æ§
    print("âœ… SB3 Monitor åŒ…è£å®Œæˆ")
    
    if use_pbrs:
        # ğŸ¯ æ·»åŠ  PBRS çå‹µå¡‘å½¢
        env = create_pbrs_wrapper(env, gamma=0.99, debug=pbrs_debug)
        print("âœ… PBRS çå‹µå¡‘å½¢å·²å•Ÿç”¨")
    
    # ğŸ§ª æ¸¬è©¦æœ€çµ‚ç’°å¢ƒå…¼å®¹æ€§
    if pbrs_debug:
        test_compatibility(env)
    
    return env

# ğŸ¯ PBRS è¨­ç½®
USE_PBRS = True  # æ˜¯å¦ä½¿ç”¨çå‹µå¡‘å½¢
PBRS_DEBUG = False  # æ˜¯å¦è¼¸å‡ºPBRSèª¿è©¦ä¿¡æ¯

env = make_sai_env(use_pbrs=USE_PBRS, pbrs_debug=PBRS_DEBUG)
print(f"âœ… ç’°å¢ƒå·²å‰µå»º | è§€å¯Ÿ: {env.observation_space} | å‹•ä½œ: {env.action_space}")
if USE_PBRS:
    print("ğŸ¯ PBRSçå‹µå¡‘å½¢: å•Ÿç”¨ - å°‡å¹«åŠ©çªç ´ep_rew_meanç“¶é ¸")

# =================================================================
# 2. è¶…åƒæ•¸é…ç½®
# =================================================================
TOTAL_TIMESTEPS = 1000000         # è¨“ç·´æ­¥æ•¸
MODEL_NAME = "BC-SB3-PPO"
SAVE_FREQ = 50000                 # æ¯5è¬æ­¥ä¿å­˜ä¸€æ¬¡

# Stable Baselines3 PPOè¶…åƒæ•¸ (èª¿å„ªç‰ˆæœ¬)
PPO_CONFIG = {
    'learning_rate': 3e-4,        # å­¸ç¿’ç‡
    'n_steps': 2048,              # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•¸
    'batch_size': 64,             # æ‰¹æ¬¡å¤§å°
    'n_epochs': 10,               # æ¯æ¬¡æ›´æ–°çš„epochæ•¸
    'gamma': 0.99,                # æŠ˜æ‰£å› å­
    'gae_lambda': 0.95,           # GAE lambda
    'clip_range': 0.2,            # PPOè£å‰ªç¯„åœ
    'ent_coef': 0.01,             # ç†µä¿‚æ•¸
    'vf_coef': 0.5,               # åƒ¹å€¼å‡½æ•¸æå¤±ä¿‚æ•¸
    'max_grad_norm': 0.5,         # æ¢¯åº¦è£å‰ª
    'verbose': 1,                 # è¼¸å‡ºç­‰ç´š
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'tensorboard_log': './sb3_tensorboard/',
    'policy_kwargs': {            # ç¶²çµ¡æ¶æ§‹
        'net_arch': [256, 256],   # ç°¡æ½”çš„ç¶²çµ¡çµæ§‹
        'activation_fn': torch.nn.ReLU,
    }
}

print(f"ğŸ”¥ ä½¿ç”¨è¨­å‚™: {PPO_CONFIG['device']}")

# =================================================================
# 3. BCé è¨“ç·´é©é…å™¨
# =================================================================
class SB3BCAdapter:
    """å°‡BCé è¨“ç·´é©é…åˆ°Stable Baselines3"""
    
    def __init__(self, expert_data_path):
        self.expert_data_path = expert_data_path
        self.expert_data = self._load_expert_data()
        
    def _load_expert_data(self):
        """è¼‰å…¥ä¸¦è½‰æ›å°ˆå®¶æ•¸æ“š"""
        if not os.path.exists(self.expert_data_path):
            return None
            
        print(f"ğŸ“š è¼‰å…¥å°ˆå®¶æ•¸æ“š: {self.expert_data_path}")
        data = np.load(self.expert_data_path, allow_pickle=True)
        
        # ç°¡åŒ–è½‰æ›ï¼šå¾89ç¶­æå–45ç¶­
        il_observations = data['observations']
        expert_actions = data['actions']
        
        # æå–æ ¸å¿ƒæ©Ÿå™¨äººç‹€æ…‹
        converted_observations = []
        for obs in il_observations:
            # å‰42ç¶­ + ä»»å‹™ç·¨ç¢¼3ç¶­ = 45ç¶­
            robot_state = obs[:42]
            task_encoding = obs[-3:]
            research_obs = np.concatenate([robot_state, task_encoding])
            converted_observations.append(research_obs)
        
        converted_observations = np.array(converted_observations, dtype=np.float32)
        expert_actions = np.array(expert_actions, dtype=np.float32)
        
        print(f"âœ… å°ˆå®¶æ•¸æ“šè¼‰å…¥æˆåŠŸ:")
        print(f"   è§€æ¸¬: {converted_observations.shape}")
        print(f"   å‹•ä½œ: {expert_actions.shape}")
        print(f"   Episodes: {np.sum(data['done'])}")
        
        return {
            'observations': converted_observations,
            'actions': expert_actions,
            'episode_count': int(np.sum(data['done']))
        }
    
    def pretrain_sb3_model(self, model, epochs=50, batch_size=256):
        """ä½¿ç”¨å°ˆå®¶æ•¸æ“šé è¨“ç·´SB3æ¨¡å‹"""
        if self.expert_data is None:
            print("âŒ æ²’æœ‰å°ˆå®¶æ•¸æ“šï¼Œè·³éBCé è¨“ç·´")
            return None
        
        print(f"ğŸ¯ é–‹å§‹BCé è¨“ç·´ SB3æ¨¡å‹ ({epochs} epochs)")
        
        device = model.device
        observations = torch.tensor(self.expert_data['observations']).to(device)
        actions = torch.tensor(self.expert_data['actions']).to(device)
        
        # ç²å–SB3æ¨¡å‹çš„ç­–ç•¥ç¶²çµ¡
        policy = model.policy
        
        # å‰µå»ºBCå„ªåŒ–å™¨
        optimizer = torch.optim.Adam(policy.parameters(), lr=1e-4, weight_decay=1e-5)
        loss_fn = torch.nn.MSELoss()
        
        dataset_size = len(observations)
        best_loss = float('inf')
        
        policy.train()
        
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            
            # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
            indices = torch.randperm(dataset_size)
            
            for i in range(0, dataset_size, batch_size):
                end_idx = min(i + batch_size, dataset_size)
                batch_indices = indices[i:end_idx]
                
                batch_obs = observations[batch_indices]
                batch_actions = actions[batch_indices]
                
                # ä½¿ç”¨SB3ç­–ç•¥ç¶²çµ¡é æ¸¬å‹•ä½œ (ä¿®æ­£ç‰ˆ2)
                # å°‡tensorè½‰ç‚ºnumpyï¼Œé æ¸¬å¾Œå†è½‰å›tensor
                batch_obs_np = batch_obs.cpu().numpy()
                
                # ä½¿ç”¨SB3é æ¸¬å‹•ä½œ
                actions_np, _ = policy.predict(batch_obs_np, deterministic=True)
                
                # è½‰å›tensorç”¨æ–¼æ¢¯åº¦è¨ˆç®—
                predicted_actions = torch.tensor(actions_np, device=batch_obs.device, dtype=torch.float32, requires_grad=True)
                
                # è¨ˆç®—æå¤±
                loss = loss_fn(predicted_actions, batch_actions)
                
                # åå‘å‚³æ’­
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            
            if avg_loss < best_loss:
                best_loss = avg_loss
            
            # å®šæœŸè¼¸å‡º
            if epoch % 10 == 0:
                print(f"   Epoch {epoch:3d}: BC Loss = {avg_loss:.6f} (Best: {best_loss:.6f})")
        
        print(f"âœ… BCé è¨“ç·´å®Œæˆ! æœ€çµ‚æå¤±: {best_loss:.6f}")
        policy.eval()
        
        return best_loss

# =================================================================
# 4. Google Driveè¨­ç½®å’Œæ¨¡å‹é¸æ“‡
# =================================================================
def choose_model_loading():
    """é¸æ“‡è¼‰å…¥æ¨¡å‹æˆ–é‡æ–°é–‹å§‹"""
    print("\n" + "="*50)
    print("ğŸ¤” BC-SB3-PPOè¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°SB3æ¨¡å‹
    local_models = glob.glob(f"*{MODEL_NAME}*.zip") + glob.glob(f"best_*.zip") + glob.glob(f"checkpoint_*.zip")
    
    if local_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„SB3æ¨¡å‹:")
        for i, model in enumerate(local_models):
            print(f"  {i+1}. {model}")
        
        print(f"\n{len(local_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´ (åŒ…å«BCé è¨“ç·´)")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(local_models) + 1:
                    return None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(local_models):
                    return local_models[choice_num - 1]
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„SB3æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None

# åˆå§‹åŒ–Google Drive
try:
    gdrive_sync = SimpleGDriveSync()
    gdrive_available = gdrive_sync.gdrive_path is not None
    print(f"ğŸ”— Google Drive: {'âœ… å·²é€£æ¥' if gdrive_available else 'âŒ æœªé€£æ¥'}")
except Exception as e:
    print(f"âš ï¸ Google Driveåˆå§‹åŒ–å¤±æ•—: {e}")
    gdrive_sync = None
    gdrive_available = False

# é¸æ“‡æ¨¡å‹è¼‰å…¥æ–¹å¼
model_path = choose_model_loading()

# =================================================================
# 5. å‰µå»ºå’Œé…ç½®SB3 PPOæ¨¡å‹
# =================================================================
if model_path:
    print(f"ğŸ“‚ è¼‰å…¥SB3æ¨¡å‹: {model_path}")
    model = PPO.load(model_path, env=env)
    skip_bc = True
    print("âœ… SB3æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œè·³éBCé è¨“ç·´")
else:
    print("ğŸ”§ å‰µå»ºæ–°çš„SB3 PPOæ¨¡å‹")
    model = PPO('MlpPolicy', env, **PPO_CONFIG)
    skip_bc = False
    print("âœ… SB3 PPOæ¨¡å‹å‰µå»ºå®Œæˆ")

# =================================================================
# 6. BCé è¨“ç·´ (å¦‚æœéœ€è¦)
# =================================================================
expert_data_path = "../data/dataset_kick.npz"
if not skip_bc and os.path.exists(expert_data_path):
    bc_adapter = SB3BCAdapter(expert_data_path)
    bc_loss = bc_adapter.pretrain_sb3_model(model, epochs=50)
    print("âœ… BCé è¨“ç·´å®Œæˆï¼Œé–‹å§‹SB3 PPOè¨“ç·´...")
elif skip_bc:
    print("ğŸ”„ è¼‰å…¥å·²è¨“ç·´æ¨¡å‹ï¼Œè·³éBCé è¨“ç·´")
else:
    print("âš ï¸ æœªæ‰¾åˆ°å°ˆå®¶æ•¸æ“šï¼Œåƒ…ä½¿ç”¨SB3 PPOè¨“ç·´")

# =================================================================
# 7. è¨­ç½®å›èª¿å‡½æ•¸
# =================================================================
# Checkpointå›èª¿ - å®šæœŸä¿å­˜
checkpoint_callback = CheckpointCallback(
    save_freq=SAVE_FREQ,
    save_path='./sb3_checkpoints/',
    name_prefix=MODEL_NAME,
    verbose=1
)

# è‡ªå®šç¾©Google Driveä¸Šå‚³å›èª¿
class GDriveUploadCallback:
    def __init__(self, gdrive_sync, save_freq):
        self.gdrive_sync = gdrive_sync
        self.save_freq = save_freq
        self.best_reward = -np.inf
        
    def __call__(self, locals_, globals_):
        # æ¯save_freqæ­¥ä¸Šå‚³ä¸€æ¬¡
        if locals_['self'].num_timesteps % self.save_freq == 0:
            if self.gdrive_sync and gdrive_available:
                try:
                    model_path = f"./sb3_checkpoints/{MODEL_NAME}_{locals_['self'].num_timesteps}_steps.zip"
                    locals_['self'].save(model_path)
                    
                    # ä¸Šå‚³åˆ°Google Drive (é€™è£¡éœ€è¦é©é…gdrive_utils)
                    print(f"ğŸ“¤ ä¸Šå‚³checkpointåˆ°Google Drive")
                except Exception as e:
                    print(f"âš ï¸ Google Driveä¸Šå‚³å¤±æ•—: {e}")
        
        return True

# =================================================================
# 8. é–‹å§‹SB3 PPOè¨“ç·´
# =================================================================
print(f"ğŸš€ é–‹å§‹Stable Baselines3 PPOè¨“ç·´ ({TOTAL_TIMESTEPS:,} æ­¥)")
print("="*60)

try:
    # é–‹å§‹è¨“ç·´
    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=checkpoint_callback,
        progress_bar=True
    )
    
    print(f"ğŸ‰ è¨“ç·´å®Œæˆ!")
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    final_model_path = f"final_{MODEL_NAME}.zip"
    model.save(final_model_path)
    print(f"âœ… æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # Google Driveä¸Šå‚³æœ€çµ‚æ¨¡å‹
    if gdrive_sync and gdrive_available:
        try:
            # é€™è£¡éœ€è¦é©é…gdrive_utils
            print(f"ğŸ“¤ ä¸Šå‚³æœ€çµ‚æ¨¡å‹åˆ°Google Drive")
        except Exception as e:
            print(f"âš ï¸ æœ€çµ‚æ¨¡å‹Google Driveä¸Šå‚³å¤±æ•—: {e}")
    
except KeyboardInterrupt:
    print(f"\nâ¹ï¸ è¨“ç·´è¢«ä¸­æ–·")
    # ä¿å­˜ä¸­æ–·æ™‚çš„æ¨¡å‹
    interrupted_model_path = f"interrupted_{MODEL_NAME}.zip"
    model.save(interrupted_model_path)
    print(f"ğŸ’¾ ä¸­æ–·æ¨¡å‹å·²ä¿å­˜: {interrupted_model_path}")

print("ğŸ BCé è¨“ç·´ + Stable Baselines3 PPOè¨“ç·´å®Œæˆ!")

# =================================================================
# 9. ç°¡å–®æ¸¬è©¦
# =================================================================
print("\nğŸ§ª é€²è¡Œç°¡å–®æ¸¬è©¦...")
# ä¿®æ­£Gymnasium APIå…¼å®¹æ€§å•é¡Œ
obs, info = env.reset()  # æ–°ç‰ˆGym APIè¿”å›tuple (obs, info)
for i in range(100):
    action, _states = model.predict(obs, deterministic=True)
    obs, rewards, dones, truncated, info = env.step(action)  # æ–°ç‰ˆAPIè¿”å›5å€‹å€¼
    if dones or truncated:
        break

print(f"âœ… æ¸¬è©¦å®Œæˆï¼ŒåŸ·è¡Œäº† {i+1} æ­¥")
env.close()