# -*- coding: utf-8 -*-
# main_td3_curiosity.py
# ä½¿ç”¨TD3æ”¹é€²çš„ç´”å¥½å¥‡å¿ƒé©…å‹•è¨“ç·´è…³æœ¬

import numpy as np
import torch
from sai_rl import SAIClient 
from td3_model import TD3_FF, ReplayBuffer  # ä½¿ç”¨TD3æ›¿ä»£DDPG
from utils import Preprocessor
from logger import TensorBoardLogger
from curiosity_module import CuriosityDrivenExploration
from gdrive_utils import SimpleGDriveSync

# =================================================================
# 1. åˆå§‹åŒ– SAIClient å’Œç’°å¢ƒ
# =================================================================
sai = SAIClient(
    comp_id="booster-soccer-showdown", 
    api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",
)

env = sai.make_env()
print(f"ç’°å¢ƒå·²å‰µå»ºã€‚è§€å¯Ÿç©ºé–“: {env.observation_space} | å‹•ä½œç©ºé–“: {env.action_space}")

N_FEATURES = 45 
N_ACTIONS = env.action_space.shape[0]

# =================================================================
# 2. è¼”åŠ©å‡½æ•¸ï¼šå‹•ä½œè½‰æ› (ä¿æŒä¸è®Š)
# =================================================================
def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (
        expected_bounds[1] - expected_bounds[0]
    )
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return (
        env.action_space.low
        + (env.action_space.high - env.action_space.low) * bounded_percent
    )

# =================================================================
# 3. ğŸš€ A100æœ€ä½³åŒ–è¶…åƒæ•¸è¨­ç½® (TD3 + ç´”å¥½å¥‡å¿ƒç‰ˆ)
# =================================================================
TOTAL_TIMESTEPS = 2000000          # å¢åŠ ç¸½è¨“ç·´æ­¥æ•¸ï¼Œå……åˆ†åˆ©ç”¨A100
MODEL_NAME = "Booster-TD3-A100-Optimized-v1"
BUFFER_CAPACITY = 2000000          # 2M bufferï¼Œåˆ©ç”¨A100å¤§VRAM
BATCH_SIZE = 1024                  # 4å€batch sizeï¼Œå¤§å¹…åŠ é€Ÿè¨“ç·´
LEARNING_RATE = 1e-3               # æé«˜å­¸ç¿’ç‡é…åˆå¤§batch
NEURONS = [512, 512, 256]          # æ›´å¤§æ›´æ·±çš„ç¶²çµ¡æ¶æ§‹
UPDATE_FREQ = 1
SAVE_FREQ = 25                     # æ›´é »ç¹ä¿å­˜

# TD3 ç‰¹æœ‰åƒæ•¸
POLICY_DELAY = 2      # ç­–ç•¥å»¶é²æ›´æ–°é »ç‡
POLICY_NOISE = 0.1    # é™ä½å™ªéŸ³æé«˜ç©©å®šæ€§
NOISE_CLIP = 0.3      # èª¿æ•´å™ªéŸ³ç¯„åœ

# å¥½å¥‡å¿ƒæ¨¡çµ„åƒæ•¸ (A100å„ªåŒ–è¨­ç½®)
INTRINSIC_REWARD_SCALE = 0.8      # ç¨å¾®é™ä½ä»¥å¹³è¡¡å¤§batchæ•ˆæ‡‰
CURIOSITY_UPDATE_FREQ = 1

# åˆå§‹åŒ–TD3æ¨¡å‹
td3_agent = TD3_FF(
    N_FEATURES, 
    env.action_space, 
    NEURONS, 
    torch.nn.functional.relu,
    LEARNING_RATE,
    policy_delay=POLICY_DELAY,
    policy_noise=POLICY_NOISE,
    noise_clip=NOISE_CLIP
)
replay_buffer = ReplayBuffer(BUFFER_CAPACITY, (N_FEATURES,), N_ACTIONS)

# åˆå§‹åŒ–ç´”å¥½å¥‡å¿ƒæ¨¡çµ„
curiosity_explorer = CuriosityDrivenExploration(
    state_dim=N_FEATURES,
    action_dim=N_ACTIONS, 
    intrinsic_reward_scale=INTRINSIC_REWARD_SCALE
)

# =================================================================
# ğŸ”„ æ¨¡å‹è¼‰å…¥é¸æ“‡å’ŒGoogle Driveè¨­ç½®
# =================================================================

# åˆå§‹åŒ–Google DriveåŒæ­¥
gdrive_sync = SimpleGDriveSync()

# è©¢å•æ˜¯å¦è¼‰å…¥èˆŠæ¨¡å‹
def choose_model_loading():
    print("\n" + "="*50)
    print("ğŸ¤” TD3è¨“ç·´æ¨¡å¼é¸æ“‡")
    print("="*50)
    
    # æª¢æŸ¥æœ¬åœ°å·²æœ‰æ¨¡å‹
    import glob
    local_models = glob.glob(f"*{MODEL_NAME}*.pth") + glob.glob(f"best_*.pth") + glob.glob(f"final_*.pth")
    
    # æª¢æŸ¥Google Driveæ¨¡å‹
    gdrive_models = gdrive_sync.list_saved_models(MODEL_NAME.replace("-", "_"))
    
    if local_models or gdrive_models:
        print("ğŸ“‚ ç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹:")
        
        all_models = []
        if local_models:
            print("\næœ¬åœ°æ¨¡å‹:")
            for i, model in enumerate(local_models):
                print(f"  {i+1}. {model}")
                all_models.append(('local', model))
        
        if gdrive_models:
            print(f"\nGoogle Driveæ¨¡å‹ (å‰5å€‹):")
            for i, model in enumerate(gdrive_models[:5]):
                print(f"  {len(local_models)+i+1}. {model['name']} ({model['modified'].strftime('%Y-%m-%d %H:%M')})")
                all_models.append(('gdrive', model['path']))
        
        print(f"\n{len(all_models)+1}. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´")
        
        while True:
            try:
                choice = input("\né¸æ“‡è¦è¼‰å…¥çš„æ¨¡å‹ (è¼¸å…¥æ•¸å­—): ").strip()
                choice_num = int(choice)
                
                if choice_num == len(all_models) + 1:
                    return None, None  # å¾é ­é–‹å§‹
                elif 1 <= choice_num <= len(all_models):
                    model_type, model_path = all_models[choice_num - 1]
                    return model_type, model_path
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        print("ğŸ“‚ æœªç™¼ç¾å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        return None, None

model_type, model_path = choose_model_loading()

# ğŸš€ A100 GPUè¨­ç½®èˆ‡æ··åˆç²¾åº¦
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
td3_agent.to(device)
curiosity_explorer.to(device)

# A100æ··åˆç²¾åº¦åŠ é€Ÿ
scaler = torch.cuda.amp.GradScaler()
print(f"âœ… A100æ··åˆç²¾åº¦è¨“ç·´å·²å•Ÿç”¨ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ”¥ GPUè¨˜æ†¶é«”å„ªåŒ–ï¼šæ··åˆç²¾åº¦å¯ç¯€çœç´„40% VRAM")

# è¼‰å…¥æ¨¡å‹ (å¦‚æœé¸æ“‡äº†)
start_episode = 0
if model_path:
    try:
        if model_type == 'gdrive':
            # å¾Google Driveè¤‡è£½åˆ°æœ¬åœ°
            import shutil
            local_path = f"loaded_{MODEL_NAME}.pth"
            shutil.copy2(model_path, local_path)
            model_path = local_path
        
        print(f"ğŸ“¥ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            td3_agent.load_state_dict(checkpoint['model_state_dict'])
            start_episode = checkpoint.get('episode', 0)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (å¾Episode {start_episode}ç¹¼çºŒ)")
        else:
            td3_agent.load_state_dict(checkpoint)
            print(f"âœ… å·²è¼‰å…¥æ¨¡å‹ (ç‹€æ…‹dictæ ¼å¼)")
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ å°‡å¾é ­é–‹å§‹è¨“ç·´")
        start_episode = 0

print(f"ğŸš€ é–‹å§‹è¨“ç·´ (èµ·å§‹Episode: {start_episode})")

# åˆå§‹åŒ–è¨˜éŒ„å™¨
logger = TensorBoardLogger(model_name=MODEL_NAME) 

# è¿½è¹¤è®Šé‡
episode_reward_sum = 0
episode_intrinsic_reward_sum = 0
episode_extrinsic_reward_sum = 0  # åˆ†åˆ¥è¿½è¹¤åŸå§‹çå‹µ
episode_count = 0
episode_steps = 0
best_reward = -np.inf
best_model_path = f"best_{MODEL_NAME}.pth"

print(f"ğŸš€ A100æœ€ä½³åŒ– TD3 + ç´”å¥½å¥‡å¿ƒè¨“ç·´é–‹å§‹ï¼Œè¨­å‚™ï¼š{device}")
print(f"ğŸ¯ TD3æ”¹é€²ç‰¹æ€§ï¼š")
print(f"   â€¢ Double Q-Learning: âœ…")
print(f"   â€¢ Delayed Policy Updates: âœ… (æ¯{POLICY_DELAY}æ¬¡)")
print(f"   â€¢ Target Policy Smoothing: âœ… (å™ªéŸ³Ïƒ={POLICY_NOISE})")
print(f"ğŸ”¥ A100å„ªåŒ–é…ç½®ï¼š")
print(f"   â€¢ Batch Size: {BATCH_SIZE} (4å€æå‡)")
print(f"   â€¢ Buffer Capacity: {BUFFER_CAPACITY//1000}K (2å€æå‡)")
print(f"   â€¢ Network Size: {NEURONS} (æ›´å¤§æ›´æ·±)")
print(f"   â€¢ Learning Rate: {LEARNING_RATE} (é…åˆå¤§batch)")
print(f"   â€¢ å…§åœ¨çå‹µç¸®æ”¾: {INTRINSIC_REWARD_SCALE}")
print(f"   â€¢ æ··åˆç²¾åº¦: âœ… (A100å°ˆç”¨)")
print(f"âŒ OUå™ªéŸ³ï¼šå·²ç¦ç”¨")
print(f"âŒ PBRSçå‹µï¼šå·²ç¦ç”¨") 
print(f"âœ… ç´”å¥½å¥‡å¿ƒæ¢ç´¢ï¼šå·²å•Ÿç”¨")

# =================================================================
# 4. TD3 + ç´”å¥½å¥‡å¿ƒ è¨“ç·´å¾ªç’°
# =================================================================
current_obs, info = env.reset()
state = Preprocessor().modify_state(current_obs, info)[0] 
state = torch.tensor(state).float().to(device)

for t in range(1, TOTAL_TIMESTEPS + 1):
    # 1. æ¡é›†å‹•ä½œ (ä¸æ·»åŠ OUå™ªéŸ³ï¼Œç´”ä¾è³´å¥½å¥‡å¿ƒæ¢ç´¢)
    with torch.no_grad():
        raw_action_tensor = td3_agent(state.unsqueeze(0))
    raw_action = raw_action_tensor.cpu().numpy().flatten()
    
    # ğŸš« ä¸æ·»åŠ OUå™ªéŸ³ - ç´”ä¾è³´å¥½å¥‡å¿ƒé©…å‹•çš„æ¢ç´¢
    
    # åŸ·è¡Œå‹•ä½œ
    action = action_function(raw_action)
    next_obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    
    # ç‹€æ…‹è½‰æ›
    next_state_np = Preprocessor().modify_state(next_obs, info)[0]
    next_state = torch.tensor(next_state_np).float().to(device)

    # =================================================================
    # ğŸ§  ç´”å¥½å¥‡å¿ƒçå‹µè¨ˆç®—
    # =================================================================
    
    # ğŸ¯ å¢å¼·çå‹µè¨­è¨ˆ - èª¿æ•´é—œéµäº‹ä»¶æ¬Šé‡
    enhanced_extrinsic_reward = reward
    
    # æ ¹æ“šinfoèª¿æ•´ç‰¹å®šäº‹ä»¶çš„çå‹µæ¬Šé‡
    if 'robot_fallen' in str(info).lower() or (done and episode_steps < 50):  # æ¨æ¸¬è·Œå€’
        enhanced_extrinsic_reward += -3.0  # é¡å¤–-3.0æ‡²ç½° (ç¸½å…±-4.5)
    elif 'goal' in str(info).lower():  # æ¨æ¸¬é€²çƒ
        enhanced_extrinsic_reward += 10.0  # å¤§å¹…çå‹µé€²çƒ
    elif 'success' in str(info).lower():  # æ¨æ¸¬ä»»å‹™æˆåŠŸ
        enhanced_extrinsic_reward += 5.0   # çå‹µä»»å‹™æˆåŠŸ
    
    final_reward, intrinsic_reward = curiosity_explorer.get_enhanced_reward(
        state.cpu().numpy(),
        raw_action,
        next_state_np,
        enhanced_extrinsic_reward  # ä½¿ç”¨å¢å¼·çš„å¤–åœ¨çå‹µ
    )
    
    # ç´¯ç©çµ±è¨ˆ
    episode_extrinsic_reward_sum += reward
    episode_intrinsic_reward_sum += intrinsic_reward
    episode_reward_sum += final_reward
    episode_steps += 1

    # =================================================================
    # ğŸ“š ç¶“é©—å„²å­˜å’Œæ¨¡å‹æ›´æ–°
    # =================================================================
    
    # å„²å­˜ç¶“é©— (ä½¿ç”¨å¥½å¥‡å¿ƒå¢å¼·çå‹µ)
    replay_buffer.add(
        state.cpu().numpy(), 
        raw_action, 
        final_reward,
        next_state_np, 
        done
    )

    # ğŸš€ A100å„ªåŒ– TD3 æ¨¡å‹æ›´æ–°ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
    if replay_buffer.size > BATCH_SIZE and t % UPDATE_FREQ == 0:
        states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)
        
        states = torch.tensor(states).float().to(device)
        actions = torch.tensor(actions).float().to(device)
        rewards = torch.tensor(rewards).float().to(device)
        next_states = torch.tensor(next_states).float().to(device)
        dones = torch.tensor(dones).float().to(device)
        
        # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿè¨“ç·´
        with torch.cuda.amp.autocast():
            critic_loss, actor_loss = td3_agent.model_update(states, actions, rewards, next_states, dones)
        
        # æ›´æ–°å¥½å¥‡å¿ƒæ¨¡çµ„
        if t % CURIOSITY_UPDATE_FREQ == 0:
            curiosity_stats = curiosity_explorer.update_curiosity(states, actions, next_states)
            
            # è¨˜éŒ„å¥½å¥‡å¿ƒæŒ‡æ¨™
            logger.set_step(t)
            logger.log_scalar("Curiosity/Forward_Loss", curiosity_stats['forward_loss'])
            logger.log_scalar("Curiosity/Inverse_Loss", curiosity_stats['inverse_loss'])
            logger.log_scalar("Curiosity/Avg_Intrinsic_Reward", curiosity_stats['avg_intrinsic_reward'])
        
        # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        logger.set_step(t) 
        logger.log_scalar("Loss/Critic_Loss", critic_loss) 
        if actor_loss is not None and actor_loss != 0.0:  # TD3çš„å»¶é²æ›´æ–°
            logger.log_scalar("Loss/Actor_Loss", actor_loss)
        
        # è¨˜éŒ„TD3ç‰¹å®šæŒ‡æ¨™
        td3_stats = td3_agent.get_statistics()
        logger.log_scalar("TD3/Update_Counter", td3_stats['update_counter'])
        logger.log_scalar("TD3/Next_Actor_Update", td3_stats['next_actor_update'])

    # =================================================================
    # ğŸ”„ å›åˆçµæŸè™•ç†
    # =================================================================
    if done:
        episode_count += 1

        # è©³ç´°è¨˜éŒ„åˆ†è§£çå‹µ
        logger.log_scalar("Train/Episode_Total_Reward", episode_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Extrinsic_Reward", episode_extrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Intrinsic_Reward", episode_intrinsic_reward_sum, step=t)
        logger.log_scalar("Train/Episode_Steps", episode_steps, step=t)
        
        # è¨ˆç®—å¥½å¥‡å¿ƒè²¢ç»æ¯”ä¾‹
        if episode_reward_sum != 0:
            curiosity_ratio = episode_intrinsic_reward_sum / abs(episode_reward_sum)
            logger.log_scalar("Train/Curiosity_Contribution_Ratio", curiosity_ratio, step=t)
        
        # æª¢æŸ¥æœ€ä½³æ¨¡å‹ä¸¦è‡ªå‹•ä¿å­˜åˆ°Google Drive
        if episode_reward_sum > best_reward:
            best_reward = episode_reward_sum
            
            # ä¿å­˜æ¨¡å‹ç‹€æ…‹ (åŒ…å«å…ƒæ•¸æ“š)
            checkpoint = {
                'model_state_dict': td3_agent.state_dict(),
                'episode': episode_count + start_episode,
                'timestep': t,
                'best_reward': best_reward,
                'total_reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'episode_steps': episode_steps,
                'td3_update_counter': td3_agent.update_counter
            }
            
            # æœ¬åœ°ä¿å­˜
            torch.save(checkpoint, best_model_path)
            
            # è‡ªå‹•ä¿å­˜åˆ°Google Drive
            metadata = {
                'episode': episode_count + start_episode,
                'timestep': t,
                'reward': episode_reward_sum,
                'intrinsic_reward': episode_intrinsic_reward_sum,
                'steps': episode_steps,
                'algorithm': 'TD3'
            }
            gdrive_sync.save_model(checkpoint, f"best_{MODEL_NAME}", metadata)
            
            print(f"ğŸ† æ–°æœ€ä½³æ¨¡å‹!")
            print(f"   ç¸½çå‹µ: {episode_reward_sum:.2f}")
            print(f"   åŸå§‹çå‹µ: {episode_extrinsic_reward_sum:.2f}")
            print(f"   å¥½å¥‡å¿ƒçå‹µ: {episode_intrinsic_reward_sum:.2f}")
            print(f"   å›åˆæ­¥æ•¸: {episode_steps}")
            print(f"   è¨“ç·´æ­¥æ•¸: {t}")
            print(f"   ğŸ“¤ å·²è‡ªå‹•å‚™ä»½åˆ°Google Drive")
        
        # å®šæœŸé€²åº¦å ±å‘Š
        if episode_count % 5 == 0:
            ratio = episode_intrinsic_reward_sum / max(abs(episode_extrinsic_reward_sum), 0.001)
            td3_stats = td3_agent.get_statistics()
            print(f"ğŸ¯ Episode {episode_count:3d} | "
                  f"ç¸½çå‹µ: {episode_reward_sum:6.2f} | "
                  f"åŸå§‹: {episode_extrinsic_reward_sum:6.2f} | "
                  f"å¥½å¥‡å¿ƒ: {episode_intrinsic_reward_sum:5.2f} | "
                  f"æ­¥æ•¸: {episode_steps:3d} | "
                  f"æ¯”ä¾‹: {ratio:.2f} | "
                  f"TD3æ›´æ–°: {td3_stats['update_counter']}")
        
        # é‡ç½®ç’°å¢ƒ
        current_obs, info = env.reset()
        state = Preprocessor().modify_state(current_obs, info)[0]
        state = torch.tensor(state).float().to(device)
        
        # é‡è¨­è®Šé‡
        episode_reward_sum = 0
        episode_intrinsic_reward_sum = 0
        episode_extrinsic_reward_sum = 0
        episode_steps = 0
    else:
        state = next_state
    
    # å¤§é€²åº¦å ±å‘Šå’Œå®šæœŸå‚™ä»½ (ğŸš€ A100å„ªåŒ–: æ›´é »ç¹å‚™ä»½)
    if t % 10000 == 0:
        curiosity_stats = curiosity_explorer.get_statistics()
        td3_stats = td3_agent.get_statistics()
        print(f"\nğŸš€ === TD3è¨“ç·´é€²åº¦å ±å‘Š (æ­¥æ•¸: {t}) ===")
        print(f"ğŸ“Š å›åˆç¸½æ•¸: {episode_count}")
        print(f"ğŸ’¾ Bufferå¤§å°: {replay_buffer.size}")
        print(f"ğŸ† æœ€ä½³ç¸½çå‹µ: {best_reward:.2f}")
        print(f"ğŸ§  ç´¯è¨ˆå¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['total_intrinsic_reward']:.2f}")
        print(f"ğŸ“ˆ å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_stats['average_intrinsic_reward']:.4f}")
        print(f"ğŸ”„ å¥½å¥‡å¿ƒæ›´æ–°æ¬¡æ•¸: {curiosity_stats['update_count']}")
        print(f"ğŸ¯ TD3æ›´æ–°æ¬¡æ•¸: {td3_stats['update_counter']}")
        print(f"â° ä¸‹æ¬¡Actoræ›´æ–°: {td3_stats['next_actor_update']}æ­¥å¾Œ")
        
        # å®šæœŸè‡ªå‹•å‚™ä»½åˆ°Google Drive
        checkpoint_name = f"checkpoint_{t//1000}k"
        checkpoint_data = {
            'model_state_dict': td3_agent.state_dict(),
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'td3_update_counter': td3_agent.update_counter
        }
        checkpoint_meta = {
            'episode': episode_count + start_episode,
            'timestep': t,
            'best_reward': best_reward,
            'checkpoint': True,
            'algorithm': 'TD3'
        }
        
        if gdrive_sync.save_model(checkpoint_data, checkpoint_name, checkpoint_meta):
            print(f"ğŸ“¤ å®šæœŸå‚™ä»½å·²ä¿å­˜åˆ° Google Drive")
        
        print("=" * 50)

# =================================================================
# 5. è¨“ç·´å®Œæˆå’Œç¸½çµ
# =================================================================
final_model_path = f"final_{MODEL_NAME}.pth"

# ä¿å­˜æœ€çµ‚æ¨¡å‹ (åŒ…å«å®Œæ•´ç‹€æ…‹)
final_checkpoint = {
    'model_state_dict': td3_agent.state_dict(),
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS,
    'best_reward': best_reward,
    'final_training': True,
    'td3_update_counter': td3_agent.update_counter
}
torch.save(final_checkpoint, final_model_path)

# è‡ªå‹•ä¿å­˜æœ€çµ‚æ¨¡å‹åˆ°Google Drive
final_metadata = {
    'episode': episode_count + start_episode,
    'timestep': TOTAL_TIMESTEPS, 
    'best_reward': best_reward,
    'training_completed': True,
    'algorithm': 'TD3'
}
gdrive_sync.save_model(final_checkpoint, f"final_{MODEL_NAME}", final_metadata)

curiosity_final_stats = curiosity_explorer.get_statistics()
td3_final_stats = td3_agent.get_statistics()

print(f"\nğŸ‰ TD3 + ç´”å¥½å¥‡å¿ƒè¨“ç·´å®Œæˆï¼")
print(f"ğŸ† æœ€ä½³å›åˆçå‹µ: {best_reward:.2f}")
print(f"ğŸ§  ç¸½å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['total_intrinsic_reward']:.2f}")
print(f"ğŸ“Š å¹³å‡å¥½å¥‡å¿ƒçå‹µ: {curiosity_final_stats['average_intrinsic_reward']:.4f}")
print(f"ğŸ”„ ç¸½å›åˆæ•¸: {episode_count}")
print(f"ğŸ¯ TD3ç¸½æ›´æ–°æ¬¡æ•¸: {td3_final_stats['update_counter']}")
print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {best_model_path}, {final_model_path}")

# æ¸…ç†
env.close()
logger.close()
print("ğŸ TD3ç´”å¥½å¥‡å¿ƒå¯¦é©—å®Œæˆï¼")