# -*- coding: utf-8 -*-
# td3_model.py
# Twin Delayed Deep Deterministic Policy Gradient (TD3) implementation

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class NeuralNetwork(nn.Module):
    def __init__(
        self,
        n_features,
        n_actions,
        neurons,
        activation_function,
        output_activation=None,
    ):
        super().__init__()
        self.n_features = n_features
        self.neurons = neurons
        self.activation_function = activation_function
        self.output_activation = output_activation
        self.n_actions = n_actions

        self.n_layers = len(self.neurons) + 1
        self.layers = torch.nn.ModuleList()
        
        # å»ºç«‹ç¶²è·¯å±¤
        for index in range(self.n_layers):
            if index == 0:
                in_dim = n_features
                out_dim = neurons[index]
            elif index == self.n_layers - 1:
                in_dim = neurons[index - 1]
                out_dim = self.n_actions
            else:
                in_dim = neurons[index - 1]
                out_dim = neurons[index]
            self.layers.append(nn.Linear(in_dim, out_dim))

    def forward(self, current_layer):
        model_device = next(self.parameters()).device
        if current_layer.device != model_device:
            current_layer = current_layer.to(model_device)

        if current_layer.dtype != torch.float32:
            current_layer = current_layer.float()
            
        for index, layer in enumerate(self.layers):
            if index < self.n_layers - 1:
                current_layer = self.activation_function(layer(current_layer))
            else:
                # è¼¸å‡ºå±¤
                current_layer = layer(current_layer)
                if self.output_activation is not None:
                    current_layer = self.output_activation(current_layer)
        return current_layer


class TD3_FF(torch.nn.Module):
    """
    Twin Delayed Deep Deterministic Policy Gradient (TD3) ç®—æ³•
    ç›¸æ¯”DDPGçš„ä¸‰å€‹ä¸»è¦æ”¹é€²:
    1. Double Q-Learning (é›™Criticç¶²è·¯)
    2. Delayed Policy Updates (å»¶é²ç­–ç•¥æ›´æ–°)
    3. Target Policy Smoothing (ç›®æ¨™ç­–ç•¥å¹³æ»‘åŒ–)
    """
    def __init__(
        self, n_features, action_space, neurons, activation_function, learning_rate,
        policy_delay=2, policy_noise=0.2, noise_clip=0.5
    ):
        super().__init__()
        self.action_space = action_space
        self.learning_rate = learning_rate
        self.gamma = 0.99
        self.tau = 0.005  # TD3ä½¿ç”¨è¼ƒå¤§çš„tauå€¼
        
        # TD3 ç‰¹æœ‰åƒæ•¸
        self.policy_delay = policy_delay  # ç­–ç•¥æ›´æ–°å»¶é²
        self.policy_noise = policy_noise  # ç›®æ¨™ç­–ç•¥å™ªéŸ³æ¨™æº–å·®
        self.noise_clip = noise_clip     # å™ªéŸ³è£å‰ªç¯„åœ
        self.update_counter = 0          # æ›´æ–°è¨ˆæ•¸å™¨
        
        action_dim = action_space.shape[0]
        shared_inputs = [neurons, activation_function]
        
        # Actor ç¶²è·¯
        self.actor = NeuralNetwork(
            n_features,
            action_dim,
            *shared_inputs,
            F.tanh,  # è¼¸å‡ºå±¤ä½¿ç”¨ tanhï¼Œå°‡å‹•ä½œç¯„åœç´„æŸåœ¨ [-1, 1]
        )
        
        # ðŸŽ¯ TD3æ”¹é€²1: é›™Criticç¶²è·¯ (Double Q-Learning)
        self.critic1 = NeuralNetwork(
            n_features + action_dim, 1, *shared_inputs
        )
        self.critic2 = NeuralNetwork(
            n_features + action_dim, 1, *shared_inputs
        )

        # Target ç¶²è·¯
        self.target_actor = NeuralNetwork(
            n_features,
            action_dim,
            *shared_inputs,
            F.tanh,
        )
        self.target_critic1 = NeuralNetwork(
            n_features + action_dim, 1, *shared_inputs
        )
        self.target_critic2 = NeuralNetwork(
            n_features + action_dim, 1, *shared_inputs
        )

        # åˆå§‹åŒ– Target ç¶²è·¯èˆ‡ä¸»ç¶²è·¯æ¬Šé‡ç›¸åŒ
        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic1.load_state_dict(self.critic1.state_dict())
        self.target_critic2.load_state_dict(self.critic2.state_dict())

        # å„ªåŒ–å™¨
        self.actor_optimizer = torch.optim.Adam(
            self.actor.parameters(), lr=self.learning_rate
        )
        self.critic_optimizer = torch.optim.Adam(
            list(self.critic1.parameters()) + list(self.critic2.parameters()), 
            lr=self.learning_rate
        )

    def soft_update_targets(self):
        """è»Ÿæ›´æ–° Target ç¶²è·¯æ¬Šé‡ (Polyak Averaging)"""
        # Actor
        for target_param, param in zip(
            self.target_actor.parameters(), self.actor.parameters()
        ):
            target_param.data.copy_(
                self.tau * param.data + (1.0 - self.tau) * target_param.data
            )
        
        # Critic1
        for target_param, param in zip(
            self.target_critic1.parameters(), self.critic1.parameters()
        ):
            target_param.data.copy_(
                self.tau * param.data + (1.0 - self.tau) * target_param.data
            )
            
        # Critic2
        for target_param, param in zip(
            self.target_critic2.parameters(), self.critic2.parameters()
        ):
            target_param.data.copy_(
                self.tau * param.data + (1.0 - self.tau) * target_param.data
            )

    @staticmethod
    def backprop(optimizer, loss, max_grad_norm=1.0):
        """åŸ·è¡Œåå‘å‚³æ’­å’Œæ¢¯åº¦è£å‰ª"""
        optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        for param_group in optimizer.param_groups:
            torch.nn.utils.clip_grad_norm_(param_group["params"], max_grad_norm)
        optimizer.step()

    @staticmethod
    def get_critic_state(state, action):
        """å°‡ç‹€æ…‹å’Œå‹•ä½œåˆä½µç‚º Critic çš„è¼¸å…¥"""
        return torch.cat([state, action], dim=1)

    @staticmethod
    def tensor_to_array(torch_tensor):
        """å°‡ PyTorch Tensor è½‰æ›ç‚º numpy array"""
        return torch_tensor.detach().cpu().numpy()

    def forward(self, state):
        """åƒ…è¿”å›ž Actor çš„å‹•ä½œè¼¸å‡º"""
        return self.actor(state).cpu()

    def select_action(self, state_np):
        """åœ¨ç’°å¢ƒäº¤äº’æ™‚é¸æ“‡å‹•ä½œ"""
        state = torch.tensor(state_np).float().to(next(self.parameters()).device)
        return self.tensor_to_array(self.actor(state))

    def model_update(self, states, actions, rewards, next_states, dones):
        """
        TD3 æ¨¡åž‹çš„å–®æ¬¡æ›´æ–°
        æ³¨æ„ï¼šè¼¸å…¥ states, actions, rewards, next_states, dones å·²ç¶“æ˜¯ tensor ä¸”åœ¨æ­£ç¢ºçš„ device ä¸Š
        """
        self.update_counter += 1
        
        # --- Critic æ›´æ–° (æ¯æ¬¡éƒ½æ›´æ–°) ---
        with torch.no_grad():
            # ðŸŽ¯ TD3æ”¹é€²3: Target Policy Smoothing (ç›®æ¨™ç­–ç•¥å¹³æ»‘åŒ–)
            next_actions = self.target_actor(next_states)
            
            # æ·»åŠ è£å‰ªå™ªéŸ³åˆ°ç›®æ¨™å‹•ä½œ
            noise = torch.clamp(
                torch.randn_like(next_actions) * self.policy_noise,
                -self.noise_clip, self.noise_clip
            )
            next_actions = torch.clamp(next_actions + noise, -1.0, 1.0)
            
            # ðŸŽ¯ TD3æ”¹é€²1: Double Q-Learning (å–å…©å€‹Qå€¼çš„æœ€å°å€¼)
            target_q1 = self.target_critic1(
                TD3_FF.get_critic_state(next_states, next_actions)
            )
            target_q2 = self.target_critic2(
                TD3_FF.get_critic_state(next_states, next_actions)
            )
            target_q = torch.min(target_q1, target_q2)
            
            # Bellman Target
            y = rewards + self.gamma * target_q * (1 - dones)

        # è¨ˆç®—ç•¶å‰ Q å€¼
        current_q1 = self.critic1(TD3_FF.get_critic_state(states, actions))
        current_q2 = self.critic2(TD3_FF.get_critic_state(states, actions))
        
        # Critic æå¤± (å…©å€‹Criticçš„MSEæå¤±ä¹‹å’Œ)
        critic_loss = F.mse_loss(current_q1, y) + F.mse_loss(current_q2, y)
        TD3_FF.backprop(self.critic_optimizer, critic_loss)
        
        actor_loss = None
        
        # ðŸŽ¯ TD3æ”¹é€²2: Delayed Policy Updates (å»¶é²ç­–ç•¥æ›´æ–°)
        if self.update_counter % self.policy_delay == 0:
            # --- Actor æ›´æ–° (æ¯policy_delayæ¬¡æ›´æ–°ä¸€æ¬¡) ---
            
            # è¨ˆç®—ç•¶å‰ç‹€æ…‹çš„æœ€ä½³å‹•ä½œ (ç”± Actor é æ¸¬)
            actor_actions = self.actor(states)
            
            # è¨ˆç®— Actor æå¤± (-Q å€¼ï¼Œåªä½¿ç”¨ç¬¬ä¸€å€‹Critic)
            actor_loss = -self.critic1(
                TD3_FF.get_critic_state(states, actor_actions)
            ).mean()
            TD3_FF.backprop(self.actor_optimizer, actor_loss)
            
            # --- Target ç¶²è·¯è»Ÿæ›´æ–° ---
            self.soft_update_targets()
            
            actor_loss = actor_loss.item()
        else:
            # å¦‚æžœä¸æ›´æ–°Actorï¼Œè¿”å›žNoneæˆ–ä¸Šä¸€æ¬¡çš„å€¼
            actor_loss = 0.0

        return critic_loss.item(), actor_loss

    def get_statistics(self):
        """ç²å–æ¨¡åž‹çµ±è¨ˆä¿¡æ¯"""
        return {
            'update_counter': self.update_counter,
            'policy_delay': self.policy_delay,
            'next_actor_update': self.policy_delay - (self.update_counter % self.policy_delay)
        }


class ReplayBuffer:
    """æ¨™æº–ç¶“é©—é‡æ”¾ç·©è¡å€ (èˆ‡DDPGç›¸åŒ)"""
    def __init__(self, capacity, observation_shape, action_dim):
        self.capacity = capacity
        self.ptr = 0
        self.size = 0
        
        # ä½¿ç”¨ numpy é™£åˆ—å„²å­˜ç¶“é©—
        self.states = np.zeros((capacity, *observation_shape), dtype=np.float32)
        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)
        # èª¿æ•´ rewards å’Œ dones çš„ shape
        self.rewards = np.zeros((capacity, 1), dtype=np.float32)
        self.next_states = np.zeros((capacity, *observation_shape), dtype=np.float32)
        self.dones = np.zeros((capacity, 1), dtype=np.float32)

    def add(self, state, action, reward, next_state, done):
        """å„²å­˜å–®æ¬¡è½‰è®Š (s, a, r, s', d)"""
        self.states[self.ptr] = state
        self.actions[self.ptr] = action
        self.rewards[self.ptr] = reward
        self.next_states[self.ptr] = next_state
        self.dones[self.ptr] = float(done)  # è½‰æ›ç‚º float (0.0 æˆ– 1.0)
        self.ptr = (self.ptr + 1) % self.capacity
        self.size = min(self.size + 1, self.capacity)

    def sample(self, batch_size):
        """éš¨æ©ŸæŽ¡æ¨£æ‰¹æ¬¡ç¶“é©—"""
        ind = np.random.randint(0, self.size, size=batch_size)
        
        return (
            self.states[ind],
            self.actions[ind],
            self.rewards[ind],
            self.next_states[ind],
            self.dones[ind],
        )