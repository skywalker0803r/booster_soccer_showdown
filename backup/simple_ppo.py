from sai_rl import SAIClient
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import os
import torch
import gymnasium as gym
from gymnasium.spaces import Box
from datetime import datetime

## Initialize the SAI client
sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")

## Make the environment
base_env = sai.make_env()

class Preprocessor():

    def get_task_onehot(self, info):
        if 'task_index' in info:
            return info['task_index']
        else:
            return np.array([])

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray):
        q_w = q[:,[-1]]
        q_vec = q[:,:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * (q_w * 2.0)
        c = q_vec * (np.dot(q_vec, v).reshape(-1,1) * 2.0)    
        return a - b + c 

    def modify_state(self, obs, info):
        
        if len(obs.shape) == 1:
            obs = np.expand_dims(obs, axis=0)

        task_onehot = self.get_task_onehot(info)
        if len(task_onehot.shape) == 1:
            task_onehot = np.expand_dims(task_onehot, axis=0)
        
        if len(info["robot_quat"].shape) == 1:
            info["robot_quat"] = np.expand_dims(info["robot_quat"], axis = 0)
            info["robot_gyro"] = np.expand_dims(info["robot_gyro"], axis = 0)
            info["robot_accelerometer"] = np.expand_dims(info["robot_accelerometer"], axis = 0)
            info["robot_velocimeter"] = np.expand_dims(info["robot_velocimeter"], axis = 0)
            info["goal_team_0_rel_robot"] = np.expand_dims(info["goal_team_0_rel_robot"], axis = 0)
            info["goal_team_1_rel_robot"] = np.expand_dims(info["goal_team_1_rel_robot"], axis = 0)
            info["goal_team_0_rel_ball"] = np.expand_dims(info["goal_team_0_rel_ball"], axis = 0)
            info["goal_team_1_rel_ball"] = np.expand_dims(info["goal_team_1_rel_ball"], axis = 0)
            info["ball_xpos_rel_robot"] = np.expand_dims(info["ball_xpos_rel_robot"], axis = 0) 
            info["ball_velp_rel_robot"] = np.expand_dims(info["ball_velp_rel_robot"], axis = 0) 
            info["ball_velr_rel_robot"] = np.expand_dims(info["ball_velr_rel_robot"], axis = 0) 
            info["player_team"] = np.expand_dims(info["player_team"], axis = 0)
            info["goalkeeper_team_0_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_0_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_velp_rel_robot"], axis = 0)
            info["goalkeeper_team_1_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_1_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_velp_rel_robot"], axis = 0)
            info["target_xpos_rel_robot"] = np.expand_dims(info["target_xpos_rel_robot"], axis = 0)
            info["target_velp_rel_robot"] = np.expand_dims(info["target_velp_rel_robot"], axis = 0)
            info["defender_xpos"] = np.expand_dims(info["defender_xpos"], axis = 0)
        
        robot_qpos = obs[:,:12]
        robot_qvel = obs[:,12:24]
        quat = info["robot_quat"]
        base_ang_vel = info["robot_gyro"]
        project_gravity = self.quat_rotate_inverse(quat, np.array([0.0, 0.0, -1.0]))
        
        obs = np.hstack((robot_qpos, 
                         robot_qvel,
                         project_gravity,
                         base_ang_vel,
                         info["robot_accelerometer"],
                         info["robot_velocimeter"],
                         info["goal_team_0_rel_robot"], 
                         info["goal_team_1_rel_robot"], 
                         info["goal_team_0_rel_ball"], 
                         info["goal_team_1_rel_ball"], 
                         info["ball_xpos_rel_robot"], 
                         info["ball_velp_rel_robot"], 
                         info["ball_velr_rel_robot"], 
                         info["player_team"], 
                         info["goalkeeper_team_0_xpos_rel_robot"], 
                         info["goalkeeper_team_0_velp_rel_robot"], 
                         info["goalkeeper_team_1_xpos_rel_robot"], 
                         info["goalkeeper_team_1_velp_rel_robot"], 
                         info["target_xpos_rel_robot"], 
                         info["target_velp_rel_robot"], 
                         info["defender_xpos"],
                         task_onehot))

        return obs

# å‰µå»ºç’°å¢ƒåŒ…è£å™¨ä¾†æ­£ç¢ºè™•ç†é è™•ç†
import gymnasium as gym
from gymnasium.spaces import Box

class SAIPreprocessorWrapper(gym.Wrapper):
    """åŒ…è£å™¨ï¼Œå°‡ SAI ç’°å¢ƒèˆ‡é è™•ç†å™¨æ•´åˆ"""
    
    def __init__(self, sai_env, preprocessor_class):
        super().__init__(sai_env)
        self.preprocessor = preprocessor_class()
        
        # é‡æ–°å®šç¾©è§€å¯Ÿç©ºé–“ç‚ºé è™•ç†å¾Œçš„ 89 ç¶­
        self.observation_space = Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(89,), 
            dtype=np.float32
        )
        
        # å‹•ä½œç©ºé–“ä¿æŒä¸è®Š
        self.action_space = sai_env.action_space
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        # ä¸ä½¿ç”¨çå‹µå½¢å¡‘ï¼Œç„¡éœ€é‡ç½®
        
        # é è™•ç†è§€å¯Ÿ
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # ç¢ºä¿è¼¸å‡ºæ˜¯ä¸€ç¶­æ•¸çµ„
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # é è™•ç†è§€å¯Ÿ
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # ä¸ä½¿ç”¨çå‹µå½¢å¡‘ï¼Œä¿æŒåŸå§‹çå‹µ
        # reward = reward  # ä¿æŒåŸå§‹çå‹µä¸è®Š
        
        # ç¢ºä¿è¼¸å‡ºæ˜¯ä¸€ç¶­æ•¸çµ„
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), reward, terminated, truncated, info

# åŒ…è£ç’°å¢ƒï¼ˆä¸ä½¿ç”¨çå‹µå½¢å¡‘ï¼Œåªç”¨åŸºæœ¬é è™•ç†å™¨ï¼‰
env = SAIPreprocessorWrapper(base_env, Preprocessor)

print(f"âœ… ç’°å¢ƒå·²åŒ…è£")
print(f"   åŸå§‹è§€å¯Ÿç©ºé–“: {base_env.observation_space}")
print(f"   è™•ç†å¾Œè§€å¯Ÿç©ºé–“: {env.observation_space}")
print(f"   å‹•ä½œç©ºé–“: {env.action_space}")

# TensorBoard callback for logging rewards
class TensorBoardRewardCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_count = 0

    def _on_step(self) -> bool:
        # Log rewards when episodes are done
        if len(self.locals.get('infos', [])) > 0:
            for info in self.locals['infos']:
                if 'episode' in info:
                    episode_reward = info['episode']['r']
                    episode_length = info['episode']['l']
                    self.episode_count += 1
                    
                    # Log to tensorboard
                    self.logger.record('reward/episode_reward', episode_reward)
                    self.logger.record('reward/episode_length', episode_length)
                    self.logger.record('reward/episode_count', self.episode_count)
                    
                    print(f"Episode {self.episode_count}: Reward = {episode_reward:.4f}, Length = {episode_length}")
                    
                    # Keep track for moving average
                    self.episode_rewards.append(episode_reward)
                    if len(self.episode_rewards) > 100:
                        self.episode_rewards.pop(0)
                    
                    # Log moving averages
                    if len(self.episode_rewards) >= 10:
                        avg_10 = np.mean(self.episode_rewards[-10:])
                        self.logger.record('reward/avg_reward_10ep', avg_10)
                    
                    if len(self.episode_rewards) >= 50:
                        avg_50 = np.mean(self.episode_rewards[-50:])
                        self.logger.record('reward/avg_reward_50ep', avg_50)
                        
                    if len(self.episode_rewards) == 100:
                        avg_100 = np.mean(self.episode_rewards)
                        self.logger.record('reward/avg_reward_100ep', avg_100)

        return True

def choose_training_mode():
    """é¸æ“‡è¨“ç·´æ¨¡å¼ï¼šå¾é ­é–‹å§‹æˆ–ç¹¼çºŒè¨“ç·´"""
    print("\n" + "="*50)
    print("ğŸ¤” è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
    print("   1 - å¾é ­é–‹å§‹æ–°è¨“ç·´")
    print("   2 - è¼‰å…¥ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´")
    print("="*50)
    
    while True:
        choice = input("è«‹é¸æ“‡ (1 æˆ– 2): ").strip()
        
        if choice == "1":
            return "new", None
            
        elif choice == "2":
            # é¡¯ç¤ºå¯ç”¨çš„æ¨¡å‹
            if os.path.exists("./saved_models"):
                print("\nğŸ“ æ‰¾åˆ°çš„æ¨¡å‹æª”æ¡ˆ:")
                model_files = [f for f in os.listdir("./saved_models") if f.endswith(".zip")]
                if model_files:
                    for i, file in enumerate(model_files, 1):
                        print(f"   {i}. {file}")
                    print("   0. æ‰‹å‹•è¼¸å…¥è·¯å¾‘")
                else:
                    print("   (æ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ)")
            
            while True:
                model_path = input("\nè«‹è¼¸å…¥æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (æˆ–è¼¸å…¥æ•¸å­—é¸æ“‡): ").strip()
                
                # å¦‚æœè¼¸å…¥æ•¸å­—ï¼Œé¸æ“‡å°æ‡‰çš„æ¨¡å‹
                if model_path.isdigit():
                    idx = int(model_path)
                    if idx == 0:
                        model_path = input("è«‹è¼¸å…¥å®Œæ•´è·¯å¾‘: ").strip()
                    elif 1 <= idx <= len(model_files):
                        model_path = f"./saved_models/{model_files[idx-1]}"
                    else:
                        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")
                        continue
                
                # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
                if os.path.exists(model_path):
                    return "continue", model_path
                else:
                    print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {model_path}")
                    retry = input("é‡æ–°è¼¸å…¥? (y/n): ").lower()
                    if retry != 'y':
                        return "new", None
        else:
            print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")

# é¸æ“‡è¨“ç·´æ¨¡å¼
training_mode, model_path = choose_training_mode()

# è¨­å®š TensorBoard æ—¥èªŒç›®éŒ„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
if training_mode == "new":
    tensorboard_log = f"./runs/SimplePPO_{timestamp}"
    print(f"\nğŸ†• å¾é ­é–‹å§‹æ–°è¨“ç·´")
else:
    tensorboard_log = f"./runs/SimplePPO_Continue_{timestamp}"
    print(f"\nğŸ”„ ç¹¼çºŒè¨“ç·´æ¨¡å‹: {model_path}")

os.makedirs("./runs", exist_ok=True)

print(f"ğŸ“Š TensorBoard æ—¥èªŒå°‡ä¿å­˜åˆ°: {tensorboard_log}")
print(f"ğŸ–¥ï¸  å•Ÿå‹• TensorBoard æŒ‡ä»¤: tensorboard --logdir=./runs")

## Create or load the model
# é…ç½® PPO ç­–ç•¥ï¼ŒæŒ‡å®šæ­£ç¢ºçš„è§€å¯Ÿç©ºé–“ç¶­åº¦
policy_kwargs = dict(
    net_arch=[256, 128, 64],  # èˆ‡ DDPG ç‰ˆæœ¬ç›¸åŒçš„ç¶²è·¯æ¶æ§‹
)

if training_mode == "new":
    print("\nğŸ†• å‰µå»ºæ–°çš„ PPO æ¨¡å‹...")
    model = PPO(
        "MlpPolicy", 
        env, 
        verbose=1, 
        tensorboard_log=tensorboard_log,
        policy_kwargs=policy_kwargs,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
else:
    print("\nğŸ“¥ è¼‰å…¥ç¾æœ‰æ¨¡å‹...")
    try:
        model = PPO.load(model_path, env=env)
        # æ›´æ–° tensorboard æ—¥èªŒè·¯å¾‘
        model.tensorboard_log = tensorboard_log
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ æ”¹ç‚ºå‰µå»ºæ–°æ¨¡å‹...")
        model = PPO(
            "MlpPolicy", 
            env, 
            verbose=1, 
            tensorboard_log=tensorboard_log,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )

## Define an action function
def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (
        expected_bounds[1] - expected_bounds[0]
    )
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return (
        env.action_space.low
        + (env.action_space.high - env.action_space.low) * bounded_percent
    )

## Train the model
# å‰µå»ºå›èª¿å‡½æ•¸
callback = TensorBoardRewardCallback()

# è©¢å•è¨“ç·´æ­¥æ•¸
default_steps = 100000
while True:
    try:
        steps_input = input(f"\nè«‹è¼¸å…¥è¨“ç·´æ­¥æ•¸ (é è¨­ {default_steps}): ").strip()
        if not steps_input:
            total_steps = default_steps
            break
        total_steps = int(steps_input)
        if total_steps > 0:
            break
        else:
            print("âŒ è«‹è¼¸å…¥æ­£æ•´æ•¸")
    except ValueError:
        print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")

print(f"\nğŸš€ é–‹å§‹è¨“ç·´ PPO æ¨¡å‹...")
print(f"   æ¨¡å¼: {'æ–°è¨“ç·´' if training_mode == 'new' else 'ç¹¼çºŒè¨“ç·´'}")
print(f"   æ­¥æ•¸: {total_steps:,}")

model.learn(total_timesteps=total_steps, callback=callback)

# ä¿å­˜æ¨¡å‹
os.makedirs("./saved_models", exist_ok=True)
if training_mode == "new":
    save_model_path = f"./saved_models/simple_ppo_{timestamp}"
else:
    save_model_path = f"./saved_models/simple_ppo_continued_{timestamp}"

model.save(save_model_path)
print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_model_path}")

## Watch (è¨»è§£æ‰ï¼Œå› ç‚ºåœ¨ Colab ä¸Šç„¡æ³•ä½¿ç”¨)
#sai.watch(model, action_function, Preprocessor)
print("â„¹ï¸  sai.watch åŠŸèƒ½å·²è¨»è§£æ‰ (Colab ç’°å¢ƒä¸æ”¯æ´)")

## Benchmark the model locally
print("ğŸ“ˆ é€²è¡Œæœ¬åœ°è©•ä¼°...")
sai.benchmark(model, action_function, Preprocessor)

env.close()

print(f"""
ğŸ‰ Colab è¨“ç·´å®Œæˆï¼

ğŸ“¦ ä¸‹è¼‰ä»¥ä¸‹æª”æ¡ˆåˆ°æœ¬åœ°:
   1. saved_models/ è³‡æ–™å¤¾ - åŒ…å«è¨“ç·´å¥½çš„æ¨¡å‹
   2. runs/ è³‡æ–™å¤¾ - åŒ…å« TensorBoard æ—¥èªŒ

ğŸ–¥ï¸  æœ¬åœ°æ“ä½œ:
   1. åŸ·è¡Œ local_watch.py è§€çœ‹æ¨¡å‹ä¸¦æ±ºå®šæ˜¯å¦æäº¤
   2. åŸ·è¡Œ tensorboard --logdir=./runs æŸ¥çœ‹è¨“ç·´æ›²ç·š

ğŸ’¾ æ¨¡å‹æª”æ¡ˆ: {save_model_path}.zip
""")