"""
Behavioral Cloning (BC) - è¡Œç‚ºå…‹éš†
å¾å°ˆå®¶è»Œè·¡å­¸ç¿’ç­–ç•¥ï¼Œæ¯”å¼·åŒ–å­¸ç¿’æ›´ç©©å®šå¯é 
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import pickle
import os
from datetime import datetime
from pathlib import Path

from sai_rl import SAIClient
from sai_compatible_dreamerv3 import SAICompatibleDreamerV3

class ExpertDataset(Dataset):
    """å°ˆå®¶è»Œè·¡æ•¸æ“šé›†"""
    
    def __init__(self, trajectories, sequence_length=10):
        self.trajectories = trajectories
        self.sequence_length = sequence_length
        self.samples = []
        
        print(f"ğŸ”„ è™•ç† {len(trajectories)} æ¢è»Œè·¡...")
        
        for traj in trajectories:
            observations = np.array(traj['observations'])
            actions = np.array(traj['actions'])
            
            # å‰µå»ºåºåˆ—æ¨£æœ¬
            for i in range(len(observations) - sequence_length):
                obs_seq = observations[i:i+sequence_length]
                action_seq = actions[i:i+sequence_length]
                
                self.samples.append({
                    'observations': obs_seq,
                    'actions': action_seq,
                    'next_action': actions[i+sequence_length-1]  # é æ¸¬ç›®æ¨™
                })
        
        print(f"âœ… ç”Ÿæˆ {len(self.samples)} å€‹è¨“ç·´æ¨£æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.FloatTensor(sample['observations']),
            torch.FloatTensor(sample['actions']),
            torch.FloatTensor(sample['next_action'])
        )


class BehavioralCloningAgent(nn.Module):
    """è¡Œç‚ºå…‹éš†æ™ºèƒ½é«”"""
    
    def __init__(self, obs_dim, action_dim, hidden_dim=256, sequence_length=10):
        super().__init__()
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.sequence_length = sequence_length
        
        # è§€å¯Ÿç·¨ç¢¼å™¨
        self.obs_encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU()
        )
        
        # åºåˆ—è™•ç†å™¨ (LSTM)
        self.lstm = nn.LSTM(
            input_size=hidden_dim + action_dim,  # obs + previous_action
            hidden_size=hidden_dim,
            num_layers=2,
            dropout=0.1,
            batch_first=True
        )
        
        # å‹•ä½œé æ¸¬å™¨
        self.action_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim),
            nn.Tanh()  # è¼¸å‡º [-1, 1]
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.orthogonal_(module.weight, 0.01)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
    
    def forward(self, observations, actions):
        """
        observations: (batch, sequence_length, obs_dim)
        actions: (batch, sequence_length, action_dim)
        """
        batch_size, seq_len = observations.shape[:2]
        
        # ç·¨ç¢¼è§€å¯Ÿ
        obs_flat = observations.reshape(-1, self.obs_dim)
        obs_encoded = self.obs_encoder(obs_flat)
        obs_encoded = obs_encoded.reshape(batch_size, seq_len, -1)
        
        # æº–å‚™LSTMè¼¸å…¥ (obs + previous_action)
        # ç¬¬ä¸€å€‹æ™‚é–“æ­¥çš„previous_actionè¨­ç‚º0
        prev_actions = torch.cat([
            torch.zeros(batch_size, 1, self.action_dim, device=actions.device),
            actions[:, :-1]
        ], dim=1)
        
        lstm_input = torch.cat([obs_encoded, prev_actions], dim=-1)
        
        # LSTMè™•ç†
        lstm_out, _ = self.lstm(lstm_input)
        
        # é æ¸¬å‹•ä½œ
        actions_pred = self.action_predictor(lstm_out)
        
        return actions_pred
    
    def select_action(self, obs, hidden_state=None, deterministic=True):
        """å–®æ­¥å‹•ä½œé¸æ“‡"""
        self.eval()
        with torch.no_grad():
            if isinstance(obs, np.ndarray):
                obs = torch.FloatTensor(obs).unsqueeze(0).unsqueeze(0)  # (1, 1, obs_dim)
            
            # ç·¨ç¢¼è§€å¯Ÿ
            obs_encoded = self.obs_encoder(obs.reshape(-1, self.obs_dim))
            obs_encoded = obs_encoded.reshape(1, 1, -1)
            
            # å¦‚æœæ²’æœ‰hidden_stateï¼Œåˆå§‹åŒ–
            if hidden_state is None:
                prev_action = torch.zeros(1, 1, self.action_dim)
                lstm_input = torch.cat([obs_encoded, prev_action], dim=-1)
                lstm_out, hidden_state = self.lstm(lstm_input)
            else:
                # ä½¿ç”¨ä¹‹å‰çš„hidden_state
                prev_action = hidden_state.get('prev_action', torch.zeros(1, 1, self.action_dim))
                lstm_input = torch.cat([obs_encoded, prev_action], dim=-1)
                lstm_out, (h, c) = self.lstm(lstm_input, (hidden_state['h'], hidden_state['c']))
                hidden_state = {'h': h, 'c': c}
            
            # é æ¸¬å‹•ä½œ
            action = self.action_predictor(lstm_out).squeeze()
            
            # æ›´æ–°hidden_state
            if hidden_state is None:
                hidden_state = {'h': lstm_out, 'c': lstm_out}
            hidden_state['prev_action'] = action.unsqueeze(0).unsqueeze(0)
            
            return action.cpu().numpy(), hidden_state


def train_behavioral_cloning():
    """è¨“ç·´è¡Œç‚ºå…‹éš†æ¨¡å‹"""
    
    print("ğŸ¯ è¡Œç‚ºå…‹éš†è¨“ç·´é–‹å§‹")
    print("="*50)
    
    # æª¢æŸ¥å°ˆå®¶è»Œè·¡
    expert_data_path = "expert_data/expert_trajectories.pkl"
    if not os.path.exists(expert_data_path):
        print("âŒ æ²’æœ‰æ‰¾åˆ°å°ˆå®¶è»Œè·¡æ•¸æ“šï¼")
        print("è«‹å…ˆé‹è¡Œ extract_expert_trajectories.py")
        return None
    
    # è¼‰å…¥å°ˆå®¶è»Œè·¡
    with open(expert_data_path, 'rb') as f:
        trajectories = pickle.load(f)
    
    print(f"âœ… è¼‰å…¥ {len(trajectories)} æ¢å°ˆå®¶è»Œè·¡")
    
    # éæ¿¾é«˜è³ªé‡è»Œè·¡
    good_trajectories = [t for t in trajectories if t['episode_reward'] > 0]
    if not good_trajectories:
        good_trajectories = [t for t in trajectories if t['episode_reward'] > -10]
    
    print(f"ğŸ¯ ä½¿ç”¨ {len(good_trajectories)} æ¢é«˜è³ªé‡è»Œè·¡")
    
    # å‰µå»ºæ•¸æ“šé›†
    sequence_length = 10
    dataset = ExpertDataset(good_trajectories, sequence_length)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)
    
    # å‰µå»ºæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = BehavioralCloningAgent(
        obs_dim=89,
        action_dim=12,
        hidden_dim=256,
        sequence_length=sequence_length
    ).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)
    
    # TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    writer = SummaryWriter(f"runs/BehavioralCloning_{timestamp}")
    
    # è¨“ç·´å¾ªç’°
    num_epochs = 200
    best_loss = float('inf')
    
    print(f"ğŸš€ é–‹å§‹è¨“ç·´ {num_epochs} epochs...")
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        num_batches = 0
        
        for batch_idx, (obs_seq, action_seq, target_action) in enumerate(dataloader):
            obs_seq = obs_seq.to(device)
            action_seq = action_seq.to(device)
            target_action = target_action.to(device)
            
            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            predicted_actions = model(obs_seq, action_seq)
            
            # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„é æ¸¬
            predicted_final = predicted_actions[:, -1]
            
            # æå¤±å‡½æ•¸
            loss = F.mse_loss(predicted_final, target_action)
            
            # åå‘å‚³æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        scheduler.step()
        avg_loss = epoch_loss / num_batches
        
        # è¨˜éŒ„
        writer.add_scalar('Loss/Train', avg_loss, epoch)
        writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], epoch)
        
        print(f"Epoch {epoch:3d}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.2e}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
            }
            
            os.makedirs('saved_models/behavioral_cloning', exist_ok=True)
            torch.save(checkpoint, 'saved_models/behavioral_cloning/best_bc_model.pth')
            print(f"  ğŸ’¾ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (loss: {best_loss:.6f})")
        
        # é©—è­‰
        if epoch % 20 == 0:
            print(f"  ğŸ” Epoch {epoch} é©—è­‰...")
            evaluate_bc_model(model, device)
    
    writer.close()
    print(f"ğŸ‰ è¨“ç·´å®Œæˆï¼æœ€ä½³æå¤±: {best_loss:.6f}")
    
    return model

def evaluate_bc_model(model, device):
    """è©•ä¼°BCæ¨¡å‹"""
    
    # åˆå§‹åŒ–ç’°å¢ƒ
    sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
    env = sai.make_env()
    
    from main_improved_dreamerv3 import Preprocessor
    preprocessor = Preprocessor()
    
    model.eval()
    total_rewards = []
    
    # æ¸¬è©¦5å€‹episode
    for episode in range(5):
        obs, info = env.reset()
        obs = preprocessor.modify_state(obs, info).squeeze()
        
        episode_reward = 0
        hidden_state = None
        
        for step in range(400):
            action, hidden_state = model.select_action(obs, hidden_state)
            
            # è½‰æ›å‹•ä½œ
            env_action = env.action_space.low + (env.action_space.high - env.action_space.low) * (action + 1) / 2
            
            next_obs, reward, terminated, truncated, next_info = env.step(env_action)
            episode_reward += reward
            
            obs = preprocessor.modify_state(next_obs, next_info).squeeze()
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        print(f"    Episode {episode}: {episode_reward:.3f}")
    
    avg_reward = np.mean(total_rewards)
    print(f"  ğŸ“Š å¹³å‡çå‹µ: {avg_reward:.3f}")
    
    return avg_reward

def action_function(policy):
    """BCæ¨¡å‹çš„å‹•ä½œå‡½æ•¸"""
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (expected_bounds[1] - expected_bounds[0])
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    
    sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
    env = sai.make_env()
    return env.action_space.low + (env.action_space.high - env.action_space.low) * bounded_percent

if __name__ == "__main__":
    # è¨“ç·´BCæ¨¡å‹
    bc_model = train_behavioral_cloning()
    
    if bc_model is not None:
        print("\nğŸ¯ å‰µå»ºSAIæäº¤æ¨¡å‹...")
        
        # å‰µå»ºSAIå…¼å®¹wrapper
        sai_model = SAICompatibleDreamerV3(bc_model)
        
        # æ¸¬è©¦
        sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
        from main_improved_dreamerv3 import Preprocessor
        
        print("ğŸ” æœ¬åœ°åŸºæº–æ¸¬è©¦...")
        sai.benchmark(sai_model, action_function, Preprocessor)
        
        print("ğŸš€ æäº¤åˆ°æ’è¡Œæ¦œ...")
        sai.submit("Vedanta_BehavioralCloning", sai_model, action_function, Preprocessor)
        
        print("ğŸ‰ å®Œæˆï¼BCæ¨¡å‹æ‡‰è©²æ¯”RLæ›´ç©©å®šå¯é ï¼")