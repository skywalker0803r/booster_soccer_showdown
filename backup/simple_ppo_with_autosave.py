from sai_rl import SAIClient
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import os
import torch
import gymnasium as gym
from gymnasium.spaces import Box
from datetime import datetime
from aligned_reward_shaping_fixed import aligned_enhanced_preprocessor

class Preprocessor():
    def get_task_onehot(self, info):
        if 'task_index' in info:
            return info['task_index']
        else:
            return np.array([])

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray):
        q_w = q[:,[-1]]
        q_vec = q[:,:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * (q_w * 2.0)
        c = q_vec * (np.dot(q_vec, v).reshape(-1,1) * 2.0)    
        return a - b + c 

    def modify_state(self, obs, info):
        if len(obs.shape) == 1:
            obs = np.expand_dims(obs, axis=0)

        task_onehot = self.get_task_onehot(info)
        if len(task_onehot.shape) == 1:
            task_onehot = np.expand_dims(task_onehot, axis=0)
        
        if len(info["robot_quat"].shape) == 1:
            info["robot_quat"] = np.expand_dims(info["robot_quat"], axis = 0)
            info["robot_gyro"] = np.expand_dims(info["robot_gyro"], axis = 0)
            info["robot_accelerometer"] = np.expand_dims(info["robot_accelerometer"], axis = 0)
            info["robot_velocimeter"] = np.expand_dims(info["robot_velocimeter"], axis = 0)
            info["goal_team_0_rel_robot"] = np.expand_dims(info["goal_team_0_rel_robot"], axis = 0)
            info["goal_team_1_rel_robot"] = np.expand_dims(info["goal_team_1_rel_robot"], axis = 0)
            info["goal_team_0_rel_ball"] = np.expand_dims(info["goal_team_0_rel_ball"], axis = 0)
            info["goal_team_1_rel_ball"] = np.expand_dims(info["goal_team_1_rel_ball"], axis = 0)
            info["ball_xpos_rel_robot"] = np.expand_dims(info["ball_xpos_rel_robot"], axis = 0) 
            info["ball_velp_rel_robot"] = np.expand_dims(info["ball_velp_rel_robot"], axis = 0) 
            info["ball_velr_rel_robot"] = np.expand_dims(info["ball_velr_rel_robot"], axis = 0) 
            info["player_team"] = np.expand_dims(info["player_team"], axis = 0)
            info["goalkeeper_team_0_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_0_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_velp_rel_robot"], axis = 0)
            info["goalkeeper_team_1_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_1_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_velp_rel_robot"], axis = 0)
            info["target_xpos_rel_robot"] = np.expand_dims(info["target_xpos_rel_robot"], axis = 0)
            info["target_velp_rel_robot"] = np.expand_dims(info["target_velp_rel_robot"], axis = 0)
            info["defender_xpos"] = np.expand_dims(info["defender_xpos"], axis = 0)
        
        robot_qpos = obs[:,:12]
        robot_qvel = obs[:,12:24]
        quat = info["robot_quat"]
        base_ang_vel = info["robot_gyro"]
        project_gravity = self.quat_rotate_inverse(quat, np.array([0.0, 0.0, -1.0]))
        
        obs = np.hstack((robot_qpos, 
                         robot_qvel,
                         project_gravity,
                         base_ang_vel,
                         info["robot_accelerometer"],
                         info["robot_velocimeter"],
                         info["goal_team_0_rel_robot"], 
                         info["goal_team_1_rel_robot"], 
                         info["goal_team_0_rel_ball"], 
                         info["goal_team_1_rel_ball"], 
                         info["ball_xpos_rel_robot"], 
                         info["ball_velp_rel_robot"], 
                         info["ball_velr_rel_robot"], 
                         info["player_team"], 
                         info["goalkeeper_team_0_xpos_rel_robot"], 
                         info["goalkeeper_team_0_velp_rel_robot"], 
                         info["goalkeeper_team_1_xpos_rel_robot"], 
                         info["goalkeeper_team_1_velp_rel_robot"], 
                         info["target_xpos_rel_robot"], 
                         info["target_velp_rel_robot"], 
                         info["defender_xpos"],
                         task_onehot))

        return obs

class TensorBoardRewardCallback(BaseCallback):
    def __init__(self, save_path="./saved_models", save_prefix="best_model", verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_count = 0
        self.save_path = save_path
        self.save_prefix = save_prefix
        self.best_mean_reward = float('-inf')
        self.best_single_reward = float('-inf')
        os.makedirs(save_path, exist_ok=True)

    def _on_step(self) -> bool:
        if len(self.locals.get('infos', [])) > 0:
            for info in self.locals['infos']:
                if 'episode' in info:
                    episode_reward = info['episode']['r']
                    episode_length = info['episode']['l']
                    self.episode_count += 1
                    
                    self.logger.record('reward/episode_reward', episode_reward)
                    self.logger.record('reward/episode_length', episode_length)
                    self.logger.record('reward/episode_count', self.episode_count)
                    
                    print(f"Episode {self.episode_count}: Reward = {episode_reward:.4f}, Length = {episode_length}")
                    
                    if episode_reward > self.best_single_reward:
                        self.best_single_reward = episode_reward
                        single_best_path = os.path.join(self.save_path, f"{self.save_prefix}_single_best.zip")
                        self.model.save(single_best_path)
                        print(f"ğŸ† NEW SINGLE BEST! Reward: {episode_reward:.4f}")
                    
                    self.episode_rewards.append(episode_reward)
                    if len(self.episode_rewards) > 100:
                        self.episode_rewards.pop(0)
                    
                    if len(self.episode_rewards) >= 10:
                        avg_10 = np.mean(self.episode_rewards[-10:])
                        self.logger.record('reward/avg_reward_10ep', avg_10)
                    
                    if len(self.episode_rewards) >= 50:
                        avg_50 = np.mean(self.episode_rewards[-50:])
                        self.logger.record('reward/avg_reward_50ep', avg_50)
                        
                    if len(self.episode_rewards) >= 100:
                        avg_100 = np.mean(self.episode_rewards)
                        self.logger.record('reward/avg_reward_100ep', avg_100)
                        
                        if avg_100 > self.best_mean_reward:
                            self.best_mean_reward = avg_100
                            mean_best_path = os.path.join(self.save_path, f"{self.save_prefix}_mean_best.zip")
                            self.model.save(mean_best_path)
                            print(f"ğŸ“ˆ NEW MEAN BEST! Avg reward (100 ep): {avg_100:.4f}")
        return True

class SAIPreprocessorWrapper(gym.Wrapper):
    def __init__(self, sai_env, preprocessor_class):
        super().__init__(sai_env)
        self.preprocessor = preprocessor_class()
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(89,), dtype=np.float32)
        self.action_space = sai_env.action_space
        self.episode_count = 0
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        # é‡ç½®çå‹µå½¢å¡‘å™¨
        if hasattr(self.preprocessor, 'reset_episode'):
            self.preprocessor.reset_episode()
        self.episode_count += 1
        
        processed_obs = self.preprocessor.modify_state(obs, info)
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        return processed_obs.astype(np.float32), info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        original_reward = reward
        
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # æ‡‰ç”¨çå‹µå½¢å¡‘
        if hasattr(self.preprocessor, 'shape_reward') and not (terminated or truncated):
            reward = self.preprocessor.shape_reward(processed_obs.squeeze(), info, reward, terminated or truncated)
        
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        # å¶çˆ¾é¡¯ç¤ºçå‹µæ¯”è¼ƒ
        if self.episode_count % 100 == 0 and not (terminated or truncated):
            print(f"Step - Original: {original_reward:.4f}, Shaped: {reward:.4f}")
        
        return processed_obs.astype(np.float32), reward, terminated, truncated, info

def choose_training_mode():
    print("\nè«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
    print("1 - å¾é ­é–‹å§‹æ–°è¨“ç·´")
    print("2 - è¼‰å…¥ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´")
    
    while True:
        choice = input("è«‹é¸æ“‡ (1 æˆ– 2): ").strip()
        if choice == "1":
            return "new", None
        elif choice == "2":
            if os.path.exists("./saved_models"):
                model_files = [f for f in os.listdir("./saved_models") if f.endswith(".zip")]
                if model_files:
                    print("\næ‰¾åˆ°çš„æ¨¡å‹æª”æ¡ˆ:")
                    for i, file in enumerate(model_files, 1):
                        print(f"{i}. {file}")
                    while True:
                        try:
                            idx = int(input("è«‹é¸æ“‡æª”æ¡ˆç·¨è™Ÿ: "))
                            if 1 <= idx <= len(model_files):
                                return "continue", f"./saved_models/{model_files[idx-1]}"
                        except ValueError:
                            pass
                        print("ç„¡æ•ˆé¸æ“‡")
            return "new", None
        else:
            print("è«‹è¼¸å…¥ 1 æˆ– 2")

def action_function(policy):
    expected_bounds = [-1, 1]
    action_percent = (policy - expected_bounds[0]) / (expected_bounds[1] - expected_bounds[0])
    bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
    return base_env.action_space.low + (base_env.action_space.high - base_env.action_space.low) * bounded_percent

## Initialize the SAI client
sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")

## Make the environment
base_env = sai.make_env()

# å‰µå»ºå¢å¼·é è™•ç†å™¨ï¼ˆå«çå‹µå½¢å¡‘ï¼‰
EnhancedPreprocessor = aligned_enhanced_preprocessor(Preprocessor)
env = SAIPreprocessorWrapper(base_env, EnhancedPreprocessor)

print(f"ç’°å¢ƒå·²åŒ…è£ (å«çå‹µå½¢å¡‘)")
print(f"åŸå§‹è§€å¯Ÿç©ºé–“: {base_env.observation_space}")
print(f"è™•ç†å¾Œè§€å¯Ÿç©ºé–“: {env.observation_space}")
print(f"çå‹µå½¢å¡‘: âœ… å•Ÿç”¨")

# é¸æ“‡è¨“ç·´æ¨¡å¼
training_mode, model_path = choose_training_mode()

# è¨­å®š TensorBoard æ—¥èªŒç›®éŒ„
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
if training_mode == "new":
    tensorboard_log = f"./runs/SimplePPO_{timestamp}"
    print(f"å¾é ­é–‹å§‹æ–°è¨“ç·´")
else:
    tensorboard_log = f"./runs/SimplePPO_Continue_{timestamp}"
    print(f"ç¹¼çºŒè¨“ç·´æ¨¡å‹: {model_path}")

os.makedirs("./runs", exist_ok=True)
print(f"TensorBoard æ—¥èªŒ: {tensorboard_log}")

## Create or load the model
policy_kwargs = dict(net_arch=[256, 128, 64])

if training_mode == "new":
    print("å‰µå»ºæ–°çš„ PPO æ¨¡å‹...")
    model = PPO("MlpPolicy", env, verbose=1, tensorboard_log=tensorboard_log, policy_kwargs=policy_kwargs, learning_rate=3e-4, n_steps=2048, batch_size=64, device='cuda' if torch.cuda.is_available() else 'cpu')
else:
    print("è¼‰å…¥ç¾æœ‰æ¨¡å‹...")
    try:
        model = PPO.load(model_path, env=env)
        model.tensorboard_log = tensorboard_log
        print("æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        model = PPO("MlpPolicy", env, verbose=1, tensorboard_log=tensorboard_log, policy_kwargs=policy_kwargs, learning_rate=3e-4, n_steps=2048, batch_size=64, device='cuda' if torch.cuda.is_available() else 'cpu')

# è¨“ç·´æ­¥æ•¸
while True:
    try:
        steps_input = input(f"è«‹è¼¸å…¥è¨“ç·´æ­¥æ•¸ (é è¨­ 100000): ").strip()
        if not steps_input:
            total_steps = 100000
            break
        total_steps = int(steps_input)
        if total_steps > 0:
            break
        else:
            print("è«‹è¼¸å…¥æ­£æ•´æ•¸")
    except ValueError:
        print("è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")

print(f"é–‹å§‹è¨“ç·´...")
print(f"æ¨¡å¼: {'æ–°è¨“ç·´' if training_mode == 'new' else 'ç¹¼çºŒè¨“ç·´'}")
print(f"æ­¥æ•¸: {total_steps:,}")

## Train the model
callback = TensorBoardRewardCallback(save_path="./saved_models", save_prefix=f"simple_ppo_{timestamp}")

print("æ¨¡å‹æœƒè‡ªå‹•ä¿å­˜:")
print("ğŸ† å–®æ¬¡æœ€ä½³: xxx_single_best.zip")
print("ğŸ“ˆ å¹³å‡æœ€ä½³: xxx_mean_best.zip")

model.learn(total_timesteps=total_steps, callback=callback)

# ä¿å­˜æ¨¡å‹
os.makedirs("./saved_models", exist_ok=True)
if training_mode == "new":
    save_model_path = f"./saved_models/simple_ppo_{timestamp}"
else:
    save_model_path = f"./saved_models/simple_ppo_continued_{timestamp}"

model.save(save_model_path)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_model_path}")

## Benchmark the model locally
print("é€²è¡Œæœ¬åœ°è©•ä¼°...")
sai.benchmark(model, action_function, EnhancedPreprocessor)

env.close()

print(f"""
è¨“ç·´å®Œæˆï¼

ğŸ“¦ ä¸‹è¼‰ä»¥ä¸‹æª”æ¡ˆåˆ°æœ¬åœ°:
   1. saved_models/ è³‡æ–™å¤¾ - åŒ…å«è¨“ç·´å¥½çš„æ¨¡å‹
   2. runs/ è³‡æ–™å¤¾ - åŒ…å« TensorBoard æ—¥èªŒ

ğŸ–¥ï¸  æœ¬åœ°æ“ä½œ:
   1. åŸ·è¡Œ local_watch.py è§€çœ‹æ¨¡å‹ä¸¦æ±ºå®šæ˜¯å¦æäº¤
   2. åŸ·è¡Œ tensorboard --logdir=./runs æŸ¥çœ‹è¨“ç·´æ›²ç·š

ğŸ’¾ æ¨¡å‹æª”æ¡ˆ: {save_model_path}.zip
""")