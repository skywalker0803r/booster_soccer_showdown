"""
è½‰æ›å°ˆå®¶æ•¸æ“šæ ¼å¼
å°‡æ‰‹å‹•æ”¶é›†çš„è»Œè·¡è½‰æ›ç‚ºBC/DAggerå¯ç”¨çš„æ ¼å¼
"""

import pickle
import numpy as np
import os
from pathlib import Path
from sai_rl import SAIClient


class ExpertDataConverter:
    """å°ˆå®¶æ•¸æ“šæ ¼å¼è½‰æ›å™¨"""
    
    def __init__(self):
        self.sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
        self.env = self.sai.make_env()
        
        # å°å…¥preprocessor
        import sys
        sys.path.append('training_scripts')
        from main_improved_dreamerv3 import Preprocessor
        self.preprocessor = Preprocessor()
        
        print("ğŸ”„ å°ˆå®¶æ•¸æ“šè½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_expert_trajectories(self, filepath):
        """è¼‰å…¥å°ˆå®¶è»Œè·¡"""
        with open(filepath, 'rb') as f:
            trajectories = pickle.load(f)
        
        print(f"âœ… è¼‰å…¥ {len(trajectories)} æ¢è»Œè·¡ from {filepath}")
        return trajectories
    
    def convert_trajectory(self, traj):
        """è½‰æ›å–®æ¢è»Œè·¡æ ¼å¼"""
        
        # åŸå§‹æ•¸æ“š
        observations = traj['observations']  # åŸå§‹ç’°å¢ƒobs
        actions = traj['actions']           # é—œç¯€æ§åˆ¶æŒ‡ä»¤
        rewards = traj['rewards']
        episode_reward = traj['episode_reward']
        
        # è½‰æ›è§€å¯Ÿå€¼æ ¼å¼
        converted_observations = []
        
        print(f"ğŸ”„ è½‰æ›è»Œè·¡ (åŸå§‹é•·åº¦: {len(observations)})...")
        
        for i, raw_obs in enumerate(observations):
            try:
                # æ¨¡æ“¬ç’°å¢ƒé‡ç½®ä»¥ç²å¾—æ­£ç¢ºçš„infoæ ¼å¼
                if i == 0:
                    _, info = self.env.reset()
                    # é€™è£¡éœ€è¦å¾raw_obsä¸­æå–infoï¼Œé€™æ˜¯å€‹æŒ‘æˆ°
                    # ç°¡åŒ–è™•ç†ï¼šä½¿ç”¨resetçš„infoä½œç‚ºæ¨¡æ¿
                    mock_info = info
                else:
                    mock_info = info  # é‡ç”¨
                
                # ä½¿ç”¨preprocessorè½‰æ›
                processed_obs = self.preprocessor.modify_state(raw_obs, mock_info)
                converted_observations.append(processed_obs.squeeze())
                
            except Exception as e:
                print(f"âš ï¸ è§€å¯Ÿå€¼è½‰æ›å¤±æ•— at step {i}: {e}")
                continue
        
        # è½‰æ›å‹•ä½œæ ¼å¼ (å¾é—œç¯€æ§åˆ¶åˆ°æ­¸ä¸€åŒ–å‹•ä½œ)
        converted_actions = []
        
        for action in actions:
            try:
                # å‡è¨­actionå·²ç¶“æ˜¯é—œç¯€ç©ºé–“çš„æ§åˆ¶æŒ‡ä»¤
                # éœ€è¦åå‘è½‰æ›åˆ° [-1, 1] çš„æ­¸ä¸€åŒ–ç©ºé–“
                normalized_action = self._convert_joint_to_normalized(action)
                converted_actions.append(normalized_action)
                
            except Exception as e:
                print(f"âš ï¸ å‹•ä½œè½‰æ›å¤±æ•—: {e}")
                continue
        
        # ç¢ºä¿é•·åº¦ä¸€è‡´
        min_length = min(len(converted_observations), len(converted_actions))
        if min_length == 0:
            return None
        
        converted_traj = {
            'observations': converted_observations[:min_length],
            'actions': converted_actions[:min_length],
            'rewards': rewards[:min_length],
            'episode_reward': episode_reward,
            'quality': traj.get('quality', 'unknown'),
            'original_length': len(observations),
            'converted_length': min_length
        }
        
        print(f"âœ… è½‰æ›å®Œæˆ: {len(observations)} â†’ {min_length} steps")
        return converted_traj
    
    def _convert_joint_to_normalized(self, joint_action):
        """å°‡é—œç¯€æ§åˆ¶è½‰æ›ç‚ºæ­¸ä¸€åŒ–å‹•ä½œ"""
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯èƒ½éœ€è¦æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´
        
        # å‡è¨­joint_actionçš„ç¯„åœå’Œenv.action_spaceçš„ç¯„åœç›¸åŒ
        action_low = self.env.action_space.low
        action_high = self.env.action_space.high
        
        # æ­¸ä¸€åŒ–åˆ° [-1, 1]
        normalized = 2 * (joint_action - action_low) / (action_high - action_low) - 1
        normalized = np.clip(normalized, -1, 1)
        
        return normalized
    
    def convert_all_trajectories(self, input_filepath, output_filepath=None):
        """è½‰æ›æ‰€æœ‰è»Œè·¡"""
        
        # è¼‰å…¥åŸå§‹æ•¸æ“š
        original_trajectories = self.load_expert_trajectories(input_filepath)
        
        # è½‰æ›
        converted_trajectories = []
        
        for i, traj in enumerate(original_trajectories):
            print(f"\nğŸ“ è½‰æ›è»Œè·¡ {i+1}/{len(original_trajectories)}...")
            converted = self.convert_trajectory(traj)
            
            if converted is not None:
                converted_trajectories.append(converted)
                print(f"âœ… è»Œè·¡ {i+1} è½‰æ›æˆåŠŸ")
            else:
                print(f"âŒ è»Œè·¡ {i+1} è½‰æ›å¤±æ•—")
        
        # ä¿å­˜è½‰æ›å¾Œçš„æ•¸æ“š
        if output_filepath is None:
            output_filepath = input_filepath.replace('.pkl', '_converted.pkl')
        
        os.makedirs(os.path.dirname(output_filepath), exist_ok=True)
        
        with open(output_filepath, 'wb') as f:
            pickle.dump(converted_trajectories, f)
        
        print(f"\nğŸ’¾ å·²ä¿å­˜ {len(converted_trajectories)} æ¢è½‰æ›å¾Œçš„è»Œè·¡")
        print(f"ğŸ“ è¼¸å‡ºæ–‡ä»¶: {output_filepath}")
        
        # çµ±è¨ˆä¿¡æ¯
        self.print_conversion_stats(original_trajectories, converted_trajectories)
        
        return converted_trajectories
    
    def print_conversion_stats(self, original, converted):
        """æ‰“å°è½‰æ›çµ±è¨ˆ"""
        
        print(f"\nğŸ“Š è½‰æ›çµ±è¨ˆ:")
        print(f"   åŸå§‹è»Œè·¡æ•¸: {len(original)}")
        print(f"   è½‰æ›è»Œè·¡æ•¸: {len(converted)}")
        print(f"   æˆåŠŸç‡: {len(converted)/len(original)*100:.1f}%")
        
        if converted:
            rewards = [t['episode_reward'] for t in converted]
            lengths = [t['converted_length'] for t in converted]
            qualities = [t['quality'] for t in converted]
            
            print(f"   å¹³å‡çå‹µ: {np.mean(rewards):.3f}")
            print(f"   å¹³å‡é•·åº¦: {np.mean(lengths):.1f}")
            print(f"   è³ªé‡åˆ†å¸ƒ: {dict(zip(*np.unique(qualities, return_counts=True)))}")


def find_latest_expert_data():
    """æ‰¾åˆ°æœ€æ–°çš„å°ˆå®¶æ•¸æ“šæ–‡ä»¶"""
    
    expert_dir = Path("expert_data")
    if not expert_dir.exists():
        return None
    
    pkl_files = list(expert_dir.glob("expert_trajectories_*.pkl"))
    if not pkl_files:
        return None
    
    # æ‰¾åˆ°æœ€æ–°çš„æ–‡ä»¶
    latest_file = max(pkl_files, key=lambda p: p.stat().st_mtime)
    return str(latest_file)


if __name__ == "__main__":
    print("ğŸ”„ å°ˆå®¶æ•¸æ“šæ ¼å¼è½‰æ›å™¨")
    print("="*50)
    
    # æŸ¥æ‰¾æœ€æ–°çš„å°ˆå®¶æ•¸æ“š
    latest_file = find_latest_expert_data()
    
    if latest_file is None:
        print("âŒ æ²’æœ‰æ‰¾åˆ°å°ˆå®¶è»Œè·¡æ•¸æ“šï¼")
        print("è«‹å…ˆé‹è¡Œ expert_data_collector.py æ”¶é›†å°ˆå®¶è»Œè·¡")
        exit(1)
    
    print(f"ğŸ“ æ‰¾åˆ°å°ˆå®¶æ•¸æ“š: {latest_file}")
    
    # å‰µå»ºè½‰æ›å™¨
    converter = ExpertDataConverter()
    
    # è½‰æ›æ•¸æ“š
    output_file = "expert_data/expert_trajectories.pkl"  # BCæœŸæœ›çš„æ ¼å¼
    converted_trajectories = converter.convert_all_trajectories(latest_file, output_file)
    
    print(f"\nğŸ‰ è½‰æ›å®Œæˆï¼")
    print(f"ğŸ’¡ ç¾åœ¨å¯ä»¥é‹è¡Œ behavioral_cloning.py é–‹å§‹è¨“ç·´BCæ¨¡å‹")