"""
Extract Expert Trajectories from Training Data
å¾ä½ çš„æ­·å²è¨“ç·´ä¸­æå–æˆåŠŸçš„è»Œè·¡ä½œç‚ºå°ˆå®¶æ¼”ç¤º
"""

import numpy as np
import pickle
import os
from pathlib import Path
import torch

def extract_successful_episodes():
    """å¾è¨“ç·´æ­·å²ä¸­æå–æˆåŠŸè»Œè·¡"""
    
    print("ğŸ” å°‹æ‰¾å°ˆå®¶è»Œè·¡æ•¸æ“š...")
    
    # æª¢æŸ¥å¯èƒ½çš„æ•¸æ“šä¾†æº
    possible_sources = [
        "saved_models/training_logs/",
        "runs/",
        "training_data/",
        "episodes_data/",
        "./"  # ç•¶å‰ç›®éŒ„
    ]
    
    expert_trajectories = []
    
    # æ–¹æ³•1: å¾TensorBoard logsæå–
    print("\nğŸ“Š æ–¹æ³•1: æª¢æŸ¥TensorBoardæ—¥èªŒ...")
    runs_dir = Path("runs")
    if runs_dir.exists():
        for run_dir in runs_dir.iterdir():
            if run_dir.is_dir():
                print(f"   Found run: {run_dir.name}")
                # TODO: è§£æTensorBoard eventsæ–‡ä»¶
    
    # æ–¹æ³•2: å¾checkpointé‡ç¾æˆåŠŸepisode
    print("\nğŸ¯ æ–¹æ³•2: å¾æœ€ä½³checkpointé‡ç¾è»Œè·¡...")
    checkpoints = list(Path("saved_models/checkpoints/").glob("*.pth")) if Path("saved_models/checkpoints/").exists() else []
    best_models = list(Path("saved_models/best_models/").glob("*.pth")) if Path("saved_models/best_models/").exists() else []
    
    if checkpoints or best_models:
        print(f"   Found {len(checkpoints)} checkpoints, {len(best_models)} best models")
    
    # æ–¹æ³•3: æ‰‹å‹•æ”¶é›†å°ˆå®¶è»Œè·¡
    print("\nğŸ® æ–¹æ³•3: æ”¶é›†æ–°çš„å°ˆå®¶è»Œè·¡...")
    print("   å¯ä»¥å¾ä»¥ä¸‹ä¾†æºæ”¶é›†:")
    print("   - äººé¡æ“æ§æ©Ÿå™¨äºº")
    print("   - ä½¿ç”¨æˆåŠŸçš„checkpoint")
    print("   - å¾å…¶ä»–æˆåŠŸçš„è¨“ç·´é‹è¡Œ")
    
    return expert_trajectories

def collect_expert_trajectory_from_model(model_path, num_episodes=10):
    """å¾è¨“ç·´å¥½çš„æ¨¡å‹æ”¶é›†å°ˆå®¶è»Œè·¡"""
    
    print(f"ğŸ¤– å¾æ¨¡å‹æ”¶é›†å°ˆå®¶è»Œè·¡: {model_path}")
    
    from sai_rl import SAIClient
    from improved_dreamerv3 import ImprovedDreamerV3
    from sai_compatible_dreamerv3 import SAICompatibleDreamerV3
    
    # åŠ è¼‰æ¨¡å‹
    try:
        model = ImprovedDreamerV3(obs_dim=89, action_dim=12)
        model.load_state_dict(torch.load(model_path, map_location='cpu', weights_only=False))
        model.eval()
        print("âœ… æ¨¡å‹åŠ è¼‰æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        return []
    
    # åˆå§‹åŒ–ç’°å¢ƒ
    sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
    env = sai.make_env()
    
    from main_improved_dreamerv3 import Preprocessor
    preprocessor = Preprocessor()
    
    expert_trajectories = []
    successful_count = 0
    
    for episode in range(num_episodes):
        print(f"   æ”¶é›†è»Œè·¡ {episode+1}/{num_episodes}...")
        
        obs, info = env.reset()
        obs = preprocessor.modify_state(obs, info).squeeze()
        
        trajectory = {
            'observations': [],
            'actions': [],
            'rewards': [],
            'episode_reward': 0
        }
        
        agent_state = None
        
        for step in range(800):  # æœ€å¤§æ­¥æ•¸
            # è¨˜éŒ„ç‹€æ…‹
            trajectory['observations'].append(obs.copy())
            
            # ç²å–å‹•ä½œ
            action, agent_state = model.select_action(obs, agent_state, deterministic=True)
            trajectory['actions'].append(action.copy())
            
            # åŸ·è¡Œå‹•ä½œ
            env_action = env.action_space.low + (env.action_space.high - env.action_space.low) * (action + 1) / 2
            next_obs, reward, terminated, truncated, next_info = env.step(env_action)
            
            trajectory['rewards'].append(reward)
            trajectory['episode_reward'] += reward
            
            obs = preprocessor.modify_state(next_obs, next_info).squeeze()
            
            if terminated or truncated:
                break
        
        print(f"     Episodeçå‹µ: {trajectory['episode_reward']:.3f}")
        
        # åªä¿ç•™æˆåŠŸçš„è»Œè·¡
        if trajectory['episode_reward'] > -1.0:  # æ¯”ç´”step penaltyå¥½
            expert_trajectories.append(trajectory)
            successful_count += 1
            print(f"     âœ… æˆåŠŸè»Œè·¡ #{successful_count}")
        
    print(f"ğŸ¯ æ”¶é›†åˆ° {successful_count} æ¢æˆåŠŸè»Œè·¡")
    return expert_trajectories

def save_expert_trajectories(trajectories, filename="expert_trajectories.pkl"):
    """ä¿å­˜å°ˆå®¶è»Œè·¡"""
    
    if not trajectories:
        print("âŒ æ²’æœ‰è»Œè·¡å¯ä¿å­˜")
        return
    
    os.makedirs("expert_data", exist_ok=True)
    filepath = f"expert_data/{filename}"
    
    with open(filepath, 'wb') as f:
        pickle.dump(trajectories, f)
    
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(trajectories)} æ¢è»Œè·¡åˆ° {filepath}")
    
    # çµ±è¨ˆä¿¡æ¯
    rewards = [traj['episode_reward'] for traj in trajectories]
    lengths = [len(traj['observations']) for traj in trajectories]
    
    print(f"ğŸ“Š è»Œè·¡çµ±è¨ˆ:")
    print(f"   å¹³å‡çå‹µ: {np.mean(rewards):.3f}")
    print(f"   æœ€ä½³çå‹µ: {max(rewards):.3f}")
    print(f"   å¹³å‡é•·åº¦: {np.mean(lengths):.1f} æ­¥")
    print(f"   æœ€é•·è»Œè·¡: {max(lengths)} æ­¥")

if __name__ == "__main__":
    print("ğŸ¯ å°ˆå®¶è»Œè·¡æå–å·¥å…·")
    print("="*50)
    
    # 1. æª¢æŸ¥ç¾æœ‰æ•¸æ“š
    extract_successful_episodes()
    
    # 2. å¾æœ€ä½³æ¨¡å‹æ”¶é›†è»Œè·¡
    best_models = list(Path("saved_models/best_models/").glob("*.pth")) if Path("saved_models/best_models/").exists() else []
    
    if best_models:
        print(f"\nğŸš€ ç™¼ç¾ {len(best_models)} å€‹æœ€ä½³æ¨¡å‹:")
        for model_path in best_models:
            print(f"   {model_path}")
        
        # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
        latest_model = max(best_models, key=lambda p: p.stat().st_mtime)
        print(f"\nğŸ¯ ä½¿ç”¨æœ€æ–°æ¨¡å‹: {latest_model}")
        
        trajectories = collect_expert_trajectory_from_model(latest_model, num_episodes=20)
        
        if trajectories:
            save_expert_trajectories(trajectories)
        else:
            print("âŒ æ²’æœ‰æ”¶é›†åˆ°æˆåŠŸè»Œè·¡")
    else:
        print("\nâš ï¸ æ²’æœ‰æ‰¾åˆ°è¨“ç·´å¥½çš„æ¨¡å‹")
        print("å»ºè­°:")
        print("1. å…ˆè¨“ç·´ä¸€å€‹åŸºæœ¬å¯ç”¨çš„æ¨¡å‹")
        print("2. æˆ–æ‰‹å‹•æ”¶é›†å°ˆå®¶è»Œè·¡")
        print("3. æˆ–ä½¿ç”¨online imitation learning")