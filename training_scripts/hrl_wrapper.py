# hrl_wrapper.py

import gymnasium as gym
from gymnasium.spaces import Discrete, Box
import numpy as np
import os
from stable_baselines3 import PPO 
from typing import Union, Tuple, Dict, Any, Optional

# --- å…¨åŸŸå¸¸æ•¸/è·¯å¾‘ (ç¢ºä¿èˆ‡ ppo_with_pbrs.py ä¸­çš„å®šç¾©ä¸€è‡´) ---
MODEL_DIR = "low_level_models"
MOVE_POLICY_PATH = os.path.join(MODEL_DIR, "move_policy_final.zip")
KICK_POLICY_PATH = os.path.join(MODEL_DIR, "kick_policy_final.zip")

# --- 1. ä½éšç­–ç•¥/æŠ€èƒ½æ§åˆ¶å™¨ (è¼‰å…¥ä¸¦ä½¿ç”¨å·²è¨“ç·´å¥½çš„æ¨¡å‹) ---
class SkillPolicy:
    """
    ç”¨æ–¼ç®¡ç†å’ŒåŸ·è¡Œä½éš Move (0) å’Œ Kick (1) ç­–ç•¥çš„é¡åˆ¥ã€‚
    """
    def __init__(self):
        self.move_model: PPO = self._load_policy(MOVE_POLICY_PATH, "Move")
        self.kick_model: PPO = self._load_policy(KICK_POLICY_PATH, "Kick")
        print("âœ… SkillPolicy è¼‰å…¥æˆåŠŸã€‚")

    def _load_policy(self, path: str, name: str) -> PPO:
        """è¼‰å…¥å–®ä¸€ PPO æ¨¡å‹ã€‚"""
        try:
            # PPO.load æœƒè‡ªå‹•å°‡æ¨¡å‹è¨­å®šç‚ºæ¨è«–æ¨¡å¼
            model = PPO.load(path)
            return model
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥ {name} æ¨¡å‹: {path}. è«‹ç¢ºä¿ä½éšæ¨¡å‹å·²è¨“ç·´ä¸¦å­˜åœ¨ã€‚éŒ¯èª¤: {e}")
            raise

    def predict(self, obs: np.ndarray, skill_id: int) -> np.ndarray:
        """æ ¹æ“š skill_id é¸æ“‡ä¸¦åŸ·è¡Œä½éšå‹•ä½œã€‚"""
        model = self.move_model if skill_id == 0 else self.kick_model
        # âš ï¸ æ³¨æ„ï¼šé€™è£¡çš„ obs å¿…é ˆæ˜¯æœªç¶“é HRL æ“´å±•çš„åŸå§‹è§€å¯Ÿï¼Œå› ç‚ºä½éšæ¨¡å‹æ˜¯ç¨ç«‹è¨“ç·´çš„ã€‚
        action, _ = model.predict(obs, deterministic=True)
        return action
    
    
# --- 2. HRL ç’°å¢ƒåŒ…è£ (Wrapper) ---
class HierarchicalWrapper(gym.Wrapper):
    
    def __init__(self, env: gym.Env, ll_steps: int):
        super().__init__(env)
        self.ll_steps = ll_steps
        self.skill_policy = SkillPolicy() # è¼‰å…¥ä½éšç­–ç•¥
        self.current_obs: Optional[np.ndarray] = None # å„²å­˜æœªæ“´å±•çš„åŸå§‹è§€å¯Ÿ
        self.current_skill = 0  # ç•¶å‰æŠ€èƒ½ ID
        self.last_skill = 0
        
        # å‹•ä½œç©ºé–“ï¼šé›¢æ•£çš„æŠ€èƒ½ ID (0: Move, 1: Kick)
        # ç”±æ–¼é€™æ˜¯ HRL çš„é ‚å±¤ï¼Œé€™å€‹ action_space ä»£è¡¨é«˜å±¤å‹•ä½œç©ºé–“
        self.action_space = Discrete(2) 
        
        # ğŸ’¡ ä¿®å¾©: ç‚ºäº†æ»¿è¶³ _augment_obs ä¸­çš„å±¬æ€§å­˜å–ï¼Œæ˜ç¢ºå®šç¾©å®ƒã€‚
        # é›–ç„¶ self.action_space å·²ç¶“æ˜¯ Discrete(2)ï¼Œä½†ç‚ºäº†ç›¸å®¹éŒ¯èª¤è¿½æº¯ä¸­çš„å‘½åï¼Œæˆ‘å€‘æ–°å¢æ­¤å±¬æ€§ã€‚
        self.action_space_high_level = self.action_space  # <--- é—œéµä¿®å¾©é»

        # ğŸ’¡ æ“´å±•è§€å¯Ÿç©ºé–“ï¼šåŸå§‹è§€å¯Ÿ + [ç•¶å‰æŠ€èƒ½ ID (2ç¶­ 1-hot), æŠ€èƒ½åŸ·è¡Œé€²åº¦ (1ç¶­ float)]
        original_obs_space = self.env.observation_space.shape[0]
        # æŠ€èƒ½ ID (2ç¶­ 1-hot) + æŠ€èƒ½é€²åº¦ (1ç¶­ float) = 3 ç¶­
        new_obs_dim = original_obs_space + self.action_space_high_level.n + 1 
        
        # ç”±æ–¼ VecEnv æœƒå°‡å¤šå€‹ç’°å¢ƒçš„è¼¸å‡ºå †ç–Šï¼Œæ‰€ä»¥é€™è£¡çš„ shape åªéœ€è¦ (new_obs_dim,)
        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(new_obs_dim,), dtype=np.float32)

    @property
    def num_envs(self) -> int:
        """HRLWrapper æ‡‰è©²è¢«åŒ…è£¹åœ¨ DummyVecEnv ä¸­ï¼Œæ‰€ä»¥é€™è£¡ num_envs æ‡‰ç‚º 1"""
        return 1 

    def _augment_obs(self, obs: np.ndarray, skill_id: int, progress: float) -> np.ndarray:
        """
        å°‡æŠ€èƒ½ ID (1-hot) å’Œé€²åº¦æ·»åŠ åˆ°è§€å¯Ÿç‹€æ…‹ä¸­ã€‚
        :param obs: åŸå§‹è§€å¯Ÿç‹€æ…‹ (1D)ã€‚
        ...
        """
        # å‰µå»º 1-hot æŠ€èƒ½æ•¸çµ„
        # ğŸ’¡ ä¿®å¾©: ç¾åœ¨ self.action_space_high_level å·²ç¶“å­˜åœ¨
        num_high_level_actions = self.action_space_high_level.n
        skill_one_hot = np.zeros(num_high_level_actions, dtype=np.float32)
        skill_one_hot[skill_id] = 1.0
        
        # å°‡æ‰€æœ‰æ•¸çµ„ä¿æŒç‚º 1D é€²è¡Œæ‹¼æ¥
        progress_scalar = np.array([progress], dtype=np.float32)
        
        # åœ¨ 1D ä¸Šæ‹¼æ¥ (è»¸ 0)
        # æ‹¼æ¥å¾Œ shape: (original_obs_dim + num_skills + 1,)
        return np.concatenate([obs, skill_one_hot, progress_scalar], axis=0).astype(np.float32)
    
    def _check_skill_termination(self, skill_id: int, info: Dict[str, Any]) -> bool:
        """
        å¯¦ä½œå–®ä¸€ç’°å¢ƒçš„å…§éƒ¨æŠ€èƒ½çµ‚æ­¢æ¢ä»¶ã€‚
        è¿”å›ä¸€å€‹å¸ƒæ—å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦é”åˆ°å…§éƒ¨çµ‚æ­¢æ¢ä»¶ã€‚
        """
        
        # info ä¸­çš„ key (å¦‚ 'ball_xpos_rel_robot') æ˜¯ (1, dim) çš„ NumPy æ•¸çµ„
        
        # è·é›¢ (L2 norm)
        agent_to_ball_dist = np.linalg.norm(info['ball_xpos_rel_robot'][0, :2])
        
        if skill_id == 0: # Move æŠ€èƒ½ï¼šåˆ°é”çƒé™„è¿‘å³æˆåŠŸçµ‚æ­¢
            MOVE_SUCCESS_THRESHOLD = 0.3 
            return agent_to_ball_dist < MOVE_SUCCESS_THRESHOLD
            
        elif skill_id == 1: # Kick æŠ€èƒ½ï¼šçƒè¢«è¸¢å‡ºå³æˆåŠŸçµ‚æ­¢ (æª¢æŸ¥çƒçš„é€Ÿåº¦)
            
            # å‡è¨­ info ä¸­æœ‰ 'ball_xvel' (çƒçš„çµ•å°é€Ÿåº¦)
            if 'ball_xvel' in info:
                 ball_speed = np.linalg.norm(info['ball_xvel'][0])
            else:
                 # å¦‚æœç’°å¢ƒæ²’æœ‰æä¾›é€Ÿåº¦ä¿¡æ¯ï¼Œå‰‡ä¸è§¸ç™¼å…§éƒ¨çµ‚æ­¢
                 return False
            
            KICK_SUCCESS_SPEED = 1.0 # ä¾‹å¦‚ï¼Œçƒé€Ÿè¶…é 1.0 m/s
            return ball_speed > KICK_SUCCESS_SPEED
            
        return False

    def reset(self, **kwargs) -> Tuple[np.ndarray, Dict[str, Any]]:
        obs, info = self.env.reset(**kwargs)
        self.current_obs = obs # å„²å­˜æœªæ“´å±•çš„åŸå§‹è§€å¯Ÿ (ç”¨æ–¼ LL Policy)
        self.last_skill = 0 
        self.current_skill = 0 
        
        # è¿”å›æ“´å±•å¾Œçš„è§€å¯Ÿç‹€æ…‹ (åˆå§‹æŠ€èƒ½ 0, é€²åº¦ 0.0)
        return self._augment_obs(obs, 0, 0.0), info

    def step(self, action: Union[int, np.ndarray]) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:
        
        # ç¢ºä¿å‹•ä½œæ˜¯æ•´æ•¸ (Skill ID)
        skill_id = int(action.item()) if isinstance(action, np.ndarray) else int(action)
        
        # ğŸ’¡ ç´¯ç©çå‹µå¿…é ˆæ˜¯ NumPy é™£åˆ— (1,)ï¼Œä»¥ç¬¦åˆ DummyVecEnv æ¥å£
        accumulated_reward = np.zeros((self.num_envs,), dtype=np.float32) 
        
        # ğŸ’¡ æ‡²ç½°æŠ€èƒ½åˆ‡æ›ï¼šå¦‚æœæŠ€èƒ½ç™¼ç”Ÿè®ŠåŒ–ï¼Œæ–½åŠ å°çš„è² çå‹µ (é¿å… chattering)
        SWITCH_PENALTY = -0.05 
        
        if skill_id != self.current_skill: 
            accumulated_reward += SWITCH_PENALTY
        
        self.last_skill = self.current_skill
        self.current_skill = skill_id

        final_obs = self.current_obs
        final_info = None
        
        # çµ‚æ­¢å’Œæˆªæ–·å¿…é ˆæ˜¯ NumPy é™£åˆ— (1,)
        terminated = np.zeros((self.num_envs,), dtype=bool)
        truncated = np.zeros((self.num_envs,), dtype=bool)
        
        # --- åŸ·è¡Œ N å€‹ä½éšæ™‚é–“æ­¥ (LL Steps) ---
        for i in range(self.ll_steps):
            
            # 1. ä½éšç­–ç•¥æ¨è«–ï¼šä½¿ç”¨æœªæ“´å±•çš„åŸå§‹è§€å¯Ÿ (current_obs)
            ll_action = self.skill_policy.predict(self.current_obs, self.current_skill)
            
            # 2. åŸ·è¡Œç’°å¢ƒæ­¥é©Ÿ (obs, reward, terminated, truncated, info éƒ½æ˜¯ (1,) é™£åˆ—)
            obs, reward, terminated_ll, truncated_ll, info = self.env.step(ll_action)

            # 3. æ›´æ–°ç´¯ç©çå‹µå’Œç•¶å‰ç‹€æ…‹
            # ğŸ’¡ ç´¯ç©çå‹µæ˜¯é™£åˆ—åŠ æ³•
            accumulated_reward += reward 
            self.current_obs = obs # å°‡æ–°çš„åŸå§‹è§€å¯Ÿç‹€æ…‹å„²å­˜

            # 4. æª¢æŸ¥å…§éƒ¨æŠ€èƒ½çµ‚æ­¢æ¢ä»¶ (é‡å°ç•¶å‰å–®ä¸€ç’°å¢ƒ)
            internal_terminate = self._check_skill_termination(self.current_skill, info)
            
            # 5. å¦‚æœé”åˆ°å¤–éƒ¨çµ‚æ­¢æˆ–å…§éƒ¨çµ‚æ­¢ï¼Œå‰‡çµæŸ LL Steps
            if terminated_ll[0] or truncated_ll[0] or internal_terminate:
                terminated = terminated_ll # ä¿æŒ NumPy é™£åˆ— (1,) æ ¼å¼
                truncated = truncated_ll   # ä¿æŒ NumPy é™£åˆ— (1,) æ ¼å¼
                break # çµ‚æ­¢ LL å¾ªç’°

        # 6. è™•ç†æœ€çµ‚ç‹€æ…‹å’Œè§€å¯Ÿ 
        final_obs = self.current_obs
        final_info = info
        
        # è¨ˆç®—æœ€çµ‚çš„é€²åº¦ 
        progress = (i + 1) / self.ll_steps
        
        # è¿”å›æ“´å±•å¾Œçš„è§€å¯Ÿç‹€æ…‹ (obs, reward, terminated, truncated éƒ½æ˜¯ (1,) é™£åˆ—)
        return self._augment_obs(final_obs, self.current_skill, progress), accumulated_reward, terminated, truncated, final_info