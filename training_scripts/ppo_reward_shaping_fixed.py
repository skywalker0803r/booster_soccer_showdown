from sai_rl import SAIClient
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import os
import torch
import gymnasium as gym
from gymnasium.spaces import Box
from datetime import datetime

class RewardShapingPreprocessor():
    """åŒ…å«çå‹µå¡‘å½¢çš„é è™•ç†å™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–å‰ä¸€å€‹æ™‚é–“æ­¥çš„æ½›åŠ›å€¼
        self._prev_potential = None
        
    def get_task_onehot(self, info):
        if 'task_index' in info:
            return info['task_index']
        else:
            return np.array([1, 0, 0])  # é è¨­ç‚º Task 1

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray):
        q_w = q[:,[-1]]
        q_vec = q[:,:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * (q_w * 2.0)
        c = q_vec * (np.dot(q_vec, v).reshape(-1,1) * 2.0)    
        return a - b + c 

    def modify_state(self, obs, info):
        if len(obs.shape) == 1:
            obs = np.expand_dims(obs, axis=0)

        task_onehot = self.get_task_onehot(info)
        if len(task_onehot.shape) == 1:
            task_onehot = np.expand_dims(task_onehot, axis=0)
        
        if len(info["robot_quat"].shape) == 1:
            info["robot_quat"] = np.expand_dims(info["robot_quat"], axis = 0)
            info["robot_gyro"] = np.expand_dims(info["robot_gyro"], axis = 0)
            info["robot_accelerometer"] = np.expand_dims(info["robot_accelerometer"], axis = 0)
            info["robot_velocimeter"] = np.expand_dims(info["robot_velocimeter"], axis = 0)
            info["goal_team_0_rel_robot"] = np.expand_dims(info["goal_team_0_rel_robot"], axis = 0)
            info["goal_team_1_rel_robot"] = np.expand_dims(info["goal_team_1_rel_robot"], axis = 0)
            info["goal_team_0_rel_ball"] = np.expand_dims(info["goal_team_0_rel_ball"], axis = 0)
            info["goal_team_1_rel_ball"] = np.expand_dims(info["goal_team_1_rel_ball"], axis = 0)
            info["ball_xpos_rel_robot"] = np.expand_dims(info["ball_xpos_rel_robot"], axis = 0) 
            info["ball_velp_rel_robot"] = np.expand_dims(info["ball_velp_rel_robot"], axis = 0) 
            info["ball_velr_rel_robot"] = np.expand_dims(info["ball_velr_rel_robot"], axis = 0) 
            info["player_team"] = np.expand_dims(info["player_team"], axis = 0)
            info["goalkeeper_team_0_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_0_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_velp_rel_robot"], axis = 0)
            info["goalkeeper_team_1_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_1_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_velp_rel_robot"], axis = 0)
            info["target_xpos_rel_robot"] = np.expand_dims(info["target_xpos_rel_robot"], axis = 0)
            info["target_velp_rel_robot"] = np.expand_dims(info["target_velp_rel_robot"], axis = 0)
            info["defender_xpos"] = np.expand_dims(info["defender_xpos"], axis = 0)
        
        robot_qpos = obs[:,:12]
        robot_qvel = obs[:,12:24]
        quat = info["robot_quat"]
        base_ang_vel = info["robot_gyro"]
        project_gravity = self.quat_rotate_inverse(quat, np.array([0.0, 0.0, -1.0]))
        
        obs = np.hstack((robot_qpos, 
                         robot_qvel,
                         project_gravity,
                         base_ang_vel,
                         info["robot_accelerometer"],
                         info["robot_velocimeter"],
                         info["goal_team_0_rel_robot"], 
                         info["goal_team_1_rel_robot"], 
                         info["goal_team_0_rel_ball"], 
                         info["goal_team_1_rel_ball"], 
                         info["ball_xpos_rel_robot"], 
                         info["ball_velp_rel_robot"], 
                         info["ball_velr_rel_robot"], 
                         info["player_team"], 
                         info["goalkeeper_team_0_xpos_rel_robot"], 
                         info["goalkeeper_team_0_velp_rel_robot"], 
                         info["goalkeeper_team_1_xpos_rel_robot"], 
                         info["goalkeeper_team_1_velp_rel_robot"], 
                         info["target_xpos_rel_robot"], 
                         info["target_velp_rel_robot"], 
                         info["defender_xpos"],
                         task_onehot))

        return obs

    def reward_shaping(self, reward, info, gamma=0.99):
        """åŸºæ–¼æ½›åŠ›å‡½æ•¸çš„çå‹µå¡‘å½¢ (PBRS)"""
        
        # ç²å–ä»»å‹™é¡å‹
        task_onehot = self.get_task_onehot(info)
        if len(task_onehot.shape) > 1:
            task_onehot = task_onehot.squeeze()
        
        # æå–é—œéµä½ç½®è³‡è¨Š
        try:
            ball_pos_rel_robot = info["ball_xpos_rel_robot"].squeeze()
            goal_pos_rel_robot = info["goal_team_0_rel_robot"].squeeze()
            target_pos_rel_robot = info["target_xpos_rel_robot"].squeeze()
            
            # è¨ˆç®—æ½›åŠ›å‡½æ•¸
            current_potential = 0.0
            
            # é€šç”¨çå‹µï¼šé¼“å‹µæ¥è¿‘çƒ
            dist_robot_ball = np.linalg.norm(ball_pos_rel_robot)
            robot_to_ball_potential = -0.1 * dist_robot_ball  # è¶Šè¿‘è¶Šå¥½
            current_potential += robot_to_ball_potential
            
            if task_onehot[0] == 1 or task_onehot[1] == 1:  # Task 1 & 2: è¸¢çƒå…¥é–€
                # é¼“å‹µçƒæ¥è¿‘çƒé–€
                dist_ball_goal = np.linalg.norm(goal_pos_rel_robot - ball_pos_rel_robot)
                ball_to_goal_potential = -0.05 * dist_ball_goal
                current_potential += ball_to_goal_potential
                
            elif task_onehot[2] == 1:  # Task 3: ç²¾æº–å‚³çƒ
                # é¼“å‹µçƒæ¥è¿‘ç›®æ¨™é»
                dist_ball_target = np.linalg.norm(target_pos_rel_robot - ball_pos_rel_robot)
                ball_to_target_potential = -0.05 * dist_ball_target
                current_potential += ball_to_target_potential
            
            # é¡å¤–çå‹µï¼šé¼“å‹µçƒçš„é‹å‹•ï¼ˆé€Ÿåº¦çå‹µï¼‰
            if "ball_velp_rel_robot" in info:
                ball_velocity = info["ball_velp_rel_robot"].squeeze()
                ball_speed = np.linalg.norm(ball_velocity)
                speed_potential = 0.01 * ball_speed  # é¼“å‹µçƒé‹å‹•
                current_potential += speed_potential
                
        except Exception as e:
            print(f"çå‹µå¡‘å½¢è¨ˆç®—éŒ¯èª¤: {e}")
            current_potential = 0.0
        
        # è™•ç†ç¬¬ä¸€æ­¥
        if self._prev_potential is None:
            self._prev_potential = current_potential
            shaped_reward = reward  # ç¬¬ä¸€æ­¥ä¸åŠ é¡å¤–çå‹µ
        else:
            # PBRS å…¬å¼
            potential_diff = gamma * current_potential - self._prev_potential
            shaped_reward = reward + potential_diff
            self._prev_potential = current_potential
        
        return float(shaped_reward)

    def reset_episode(self):
        """é‡ç½® episode æ™‚èª¿ç”¨"""
        self._prev_potential = None

# Enhanced TensorBoard callback with best model saving
class TensorBoardRewardCallback(BaseCallback):
    def __init__(self, save_path="./saved_models", save_prefix="best_model", verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_count = 0
        self.save_path = save_path
        self.save_prefix = save_prefix
        
        # æœ€ä½³æ¨¡å‹è¿½è¹¤
        self.best_mean_reward = float('-inf')
        self.best_single_reward = float('-inf')
        self.evaluation_window = 100
        self.check_freq = 10000
        
        os.makedirs(save_path, exist_ok=True)

    def _on_step(self) -> bool:
        if len(self.locals.get('infos', [])) > 0:
            for info in self.locals['infos']:
                if 'episode' in info:
                    episode_reward = info['episode']['r']
                    episode_length = info['episode']['l']
                    self.episode_count += 1
                    
                    self.logger.record('reward/episode_reward', episode_reward)
                    self.logger.record('reward/episode_length', episode_length)
                    self.logger.record('reward/episode_count', self.episode_count)
                    
                    print(f"Episode {self.episode_count}: Reward = {episode_reward:.4f}, Length = {episode_length}")
                    
                    # è¿½è¹¤æœ€ä½³å–®æ¬¡çå‹µ
                    if episode_reward > self.best_single_reward:
                        self.best_single_reward = episode_reward
                        single_best_path = os.path.join(self.save_path, f"{self.save_prefix}_single_best.zip")
                        self.model.save(single_best_path)
                        print(f"ğŸ† NEW SINGLE BEST! Reward: {episode_reward:.4f} - Saved to {single_best_path}")
                    
                    self.episode_rewards.append(episode_reward)
                    if len(self.episode_rewards) > 200:
                        self.episode_rewards.pop(0)
                    
                    # è¨ˆç®—ç§»å‹•å¹³å‡
                    if len(self.episode_rewards) >= 10:
                        avg_10 = np.mean(self.episode_rewards[-10:])
                        self.logger.record('reward/avg_reward_10ep', avg_10)
                    
                    if len(self.episode_rewards) >= 50:
                        avg_50 = np.mean(self.episode_rewards[-50:])
                        self.logger.record('reward/avg_reward_50ep', avg_50)
                        
                    if len(self.episode_rewards) >= 100:
                        avg_100 = np.mean(self.episode_rewards[-100:])
                        self.logger.record('reward/avg_reward_100ep', avg_100)
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³å¹³å‡çå‹µ
                        if avg_100 > self.best_mean_reward:
                            self.best_mean_reward = avg_100
                            mean_best_path = os.path.join(self.save_path, f"{self.save_prefix}_mean_best.zip")
                            self.model.save(mean_best_path)
                            print(f"ğŸ“ˆ NEW MEAN BEST! Avg reward (100 ep): {avg_100:.4f} - Saved to {mean_best_path}")

        # å®šæœŸä¿å­˜æª¢æŸ¥é»
        if self.n_calls % self.check_freq == 0:
            checkpoint_path = os.path.join(self.save_path, f"{self.save_prefix}_checkpoint_{self.n_calls}.zip")
            self.model.save(checkpoint_path)
            print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")

        return True
    
    def get_best_stats(self):
        """ç²å–æœ€ä½³çµ±è¨ˆè³‡è¨Š"""
        return {
            'best_single_reward': self.best_single_reward,
            'best_mean_reward': self.best_mean_reward,
            'total_episodes': self.episode_count,
            'final_avg_reward': np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
        }

# å‰µå»ºç’°å¢ƒåŒ…è£å™¨
class SAIRewardShapingWrapper(gym.Wrapper):
    """åŒ…å«çå‹µå¡‘å½¢çš„ç’°å¢ƒåŒ…è£å™¨"""
    
    def __init__(self, sai_env, preprocessor_class):
        super().__init__(sai_env)
        self.preprocessor = preprocessor_class()
        
        self.observation_space = Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(89,), 
            dtype=np.float32
        )
        
        self.action_space = sai_env.action_space
        self.episode_count = 0
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        # é‡ç½®çå‹µå¡‘å½¢å™¨
        self.preprocessor.reset_episode()
        self.episode_count += 1
        
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        original_reward = reward
        
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # æ‡‰ç”¨çå‹µå¡‘å½¢
        if not (terminated or truncated):
            reward = self.preprocessor.reward_shaping(reward, info, gamma=0.99)
        
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), reward, terminated, truncated, info

# é¸æ“‡è¨“ç·´æ¨¡å¼çš„å‡½æ•¸
def choose_training_mode():
    print("\n" + "="*50)
    print("ğŸ¤” è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
    print("   1 - å¾é ­é–‹å§‹æ–°è¨“ç·´")
    print("   2 - è¼‰å…¥ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´")
    print("="*50)
    
    while True:
        choice = input("è«‹é¸æ“‡ (1 æˆ– 2): ").strip()
        
        if choice == "1":
            return "new", None
            
        elif choice == "2":
            if os.path.exists("./saved_models"):
                print("\nğŸ“ æ‰¾åˆ°çš„æ¨¡å‹æª”æ¡ˆ:")
                model_files = [f for f in os.listdir("./saved_models") if f.endswith(".zip")]
                if model_files:
                    for i, file in enumerate(model_files, 1):
                        print(f"   {i}. {file}")
                    print("   0. æ‰‹å‹•è¼¸å…¥è·¯å¾‘")
                else:
                    print("   (æ²’æœ‰æ‰¾åˆ°æ¨¡å‹æª”æ¡ˆ)")
            
            while True:
                model_path = input("\nè«‹è¼¸å…¥æ¨¡å‹æª”æ¡ˆè·¯å¾‘ (æˆ–è¼¸å…¥æ•¸å­—é¸æ“‡): ").strip()
                
                if model_path.isdigit():
                    idx = int(model_path)
                    if idx == 0:
                        model_path = input("è«‹è¼¸å…¥å®Œæ•´è·¯å¾‘: ").strip()
                    elif 1 <= idx <= len(model_files):
                        model_path = f"./saved_models/{model_files[idx-1]}"
                    else:
                        print("âŒ ç„¡æ•ˆçš„é¸æ“‡")
                        continue
                
                if os.path.exists(model_path):
                    return "continue", model_path
                else:
                    print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {model_path}")
                    retry = input("é‡æ–°è¼¸å…¥? (y/n): ").lower()
                    if retry != 'y':
                        return "new", None
        else:
            print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")

def main():
    print("ğŸ¯ PPO + çå‹µå¡‘å½¢è¨“ç·´ (ä¿®å¾©ç‰ˆ)")
    print("=" * 50)
    
    # Initialize SAI
    sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
    base_env = sai.make_env()
    
    # åŒ…è£ç’°å¢ƒ
    env = SAIRewardShapingWrapper(base_env, RewardShapingPreprocessor)
    
    print(f"âœ… ç’°å¢ƒå·²åŒ…è£ (å«çå‹µå¡‘å½¢)")
    print(f"   åŸå§‹è§€å¯Ÿç©ºé–“: {base_env.observation_space}")
    print(f"   è™•ç†å¾Œè§€å¯Ÿç©ºé–“: {env.observation_space}")
    
    # é¸æ“‡è¨“ç·´æ¨¡å¼
    training_mode, model_path = choose_training_mode()
    
    # è¨­å®š TensorBoard
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if training_mode == "new":
        tensorboard_log = f"./runs/PPO_RewardShaping_{timestamp}"
        print(f"\nğŸ†• å¾é ­é–‹å§‹æ–°è¨“ç·´ (å«çå‹µå¡‘å½¢)")
    else:
        tensorboard_log = f"./runs/PPO_RewardShaping_Continue_{timestamp}"
        print(f"\nğŸ”„ ç¹¼çºŒè¨“ç·´æ¨¡å‹: {model_path}")
    
    os.makedirs("./runs", exist_ok=True)
    print(f"ğŸ“Š TensorBoard: {tensorboard_log}")
    
    # å‰µå»ºæ¨¡å‹
    policy_kwargs = dict(net_arch=[256, 128, 64])
    
    if training_mode == "new":
        model = PPO(
            "MlpPolicy", 
            env, 
            verbose=1, 
            tensorboard_log=tensorboard_log,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
    else:
        try:
            model = PPO.load(model_path, env=env)
            model.tensorboard_log = tensorboard_log
            print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            model = PPO("MlpPolicy", env, verbose=1, tensorboard_log=tensorboard_log)
    
    # è¨“ç·´æ­¥æ•¸
    while True:
        try:
            steps_input = input(f"\nè«‹è¼¸å…¥è¨“ç·´æ­¥æ•¸ (å»ºè­° 200000): ").strip()
            if not steps_input:
                total_steps = 200000
                break
            total_steps = int(steps_input)
            if total_steps > 0:
                break
            else:
                print("âŒ è«‹è¼¸å…¥æ­£æ•´æ•¸")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´...")
    print(f"   æ¨¡å¼: {'æ–°è¨“ç·´' if training_mode == 'new' else 'ç¹¼çºŒè¨“ç·´'}")
    print(f"   æ­¥æ•¸: {total_steps:,}")
    print(f"   çå‹µå¡‘å½¢: âœ… å•Ÿç”¨")
    
    # å‰µå»ºå¢å¼·å›èª¿
    callback = TensorBoardRewardCallback(
        save_path="./saved_models",
        save_prefix=f"ppo_reward_shaping_{timestamp}"
    )
    
    print(f"\nğŸ¤– æ¨¡å‹æœƒè‡ªå‹•ä¿å­˜ï¼š")
    print(f"   ğŸ† å–®æ¬¡æœ€ä½³: xxx_single_best.zip")
    print(f"   ğŸ“ˆ å¹³å‡æœ€ä½³: xxx_mean_best.zip") 
    print(f"   ğŸ’¾ å®šæœŸæª¢æŸ¥é»: xxx_checkpoint_xxxxx.zip")
    
    # è¨“ç·´
    model.learn(total_timesteps=total_steps, callback=callback)
    
    # ç²å–è¨“ç·´çµ±è¨ˆ
    stats = callback.get_best_stats()
    print(f"\nğŸ“Š è¨“ç·´çµ±è¨ˆæ‘˜è¦:")
    print(f"   ğŸ† æœ€ä½³å–®æ¬¡çå‹µ: {stats['best_single_reward']:.4f}")
    print(f"   ğŸ“ˆ æœ€ä½³å¹³å‡çå‹µ: {stats['best_mean_reward']:.4f}")
    print(f"   ğŸ® ç¸½å›åˆæ•¸: {stats['total_episodes']}")
    print(f"   ğŸ¯ æœ€çµ‚å¹³å‡çå‹µ: {stats['final_avg_reward']:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    os.makedirs("./saved_models", exist_ok=True)
    save_model_path = f"./saved_models/ppo_reward_shaping_{timestamp}"
    model.save(save_model_path)
    
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_model_path}")
    
    # è©•ä¼°
    print("ğŸ“ˆ é€²è¡Œæœ¬åœ°è©•ä¼°...")
    
    def action_function(policy):
        expected_bounds = [-1, 1]
        action_percent = (policy - expected_bounds[0]) / (expected_bounds[1] - expected_bounds[0])
        bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
        return base_env.action_space.low + (base_env.action_space.high - base_env.action_space.low) * bounded_percent

    sai.benchmark(model, action_function, RewardShapingPreprocessor)
    
    env.close()
    
    print(f"""
ğŸ‰ è¨“ç·´å®Œæˆï¼

ğŸ“¦ ä¸‹è¼‰ä»¥ä¸‹æª”æ¡ˆåˆ°æœ¬åœ°:
   1. saved_models/ è³‡æ–™å¤¾ - åŒ…å«è¨“ç·´å¥½çš„æ¨¡å‹
   2. runs/ è³‡æ–™å¤¾ - åŒ…å« TensorBoard æ—¥èªŒ

ğŸ–¥ï¸  æœ¬åœ°æ“ä½œ:
   1. åŸ·è¡Œ local_watch.py è§€çœ‹æ¨¡å‹ä¸¦æ±ºå®šæ˜¯å¦æäº¤
   2. åŸ·è¡Œ tensorboard --logdir=./runs æŸ¥çœ‹è¨“ç·´æ›²ç·š

ğŸ’¾ æ¨¡å‹æª”æ¡ˆ: {save_model_path}.zip
""")

if __name__ == "__main__":
    main()