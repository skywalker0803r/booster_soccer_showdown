# ppo_with_pbrs.py
import shutil # ç”¨æ–¼æª”æ¡ˆè¤‡è£½
import argparse
import os
import sys
from datetime import datetime
import numpy as np
import gymnasium as gym
from gymnasium.spaces import Box, Discrete
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.logger import configure
import torch
from typing import Dict, Any, Union, Tuple
# ğŸ’¡ æ–°å¢ VecNormalize å°å…¥
from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv 

from sai_rl import SAIClient

# --- å¤–éƒ¨æ¨¡çµ„åŒ¯å…¥ ---
try:
    from log_callback import DetailedLogCallback
    from hrl_wrapper import HierarchicalWrapper
    # ç¢ºä¿ make_pbrs_env å­˜åœ¨æ–¼ pbrs_wrapper.py
    from pbrs_wrapper import make_pbrs_env 
except ImportError as e:
    print(f"âŒ éŒ¯èª¤: ç„¡æ³•åŒ¯å…¥æ‰€éœ€æ¨¡çµ„ã€‚è«‹ç¢ºä¿ 'log_callback.py', 'hrl_wrapper.py', 'pbrs_wrapper.py' å­˜åœ¨ã€‚éŒ¯èª¤: {e}")
    sys.exit(1)


# --- å…¨åŸŸå¸¸æ•¸ ---
_FLOAT_EPS = np.finfo(np.float64).eps
MODEL_DIR = "low_level_models" # LL Policy çš„å„²å­˜ç›®éŒ„
HRL_MODEL_DIR = "hrl_models"   # HL Policy çš„å„²å­˜ç›®éŒ„


# --- é è¨­è¶…åƒæ•¸é…ç½® (èª¿æ•´ä»¥æé«˜ç©©å®šæ€§å’Œæ€§èƒ½) ---
default_config: Dict[str, Any] = {
    # PPO Core
    'policy': 'MlpPolicy',
    'n_steps': 2048,           # Rollout buffer size
    'batch_size': 256,         # Minibatch size for gradient updates
    'gamma': 0.99,             # Discount factor
    'learning_rate': 3e-4,     # Initial learning rate
    'n_epochs': 10,            # Number of epochs for PPO
    'gae_lambda': 0.95,        # GAE åƒæ•¸
    'clip_range': 0.2,         # Clipping parameter

    # ğŸ’¡ èª¿æ•´ç¶²çµ¡çµæ§‹ (å¢åŠ å®¹é‡)
    'policy_kwargs': dict(net_arch=dict(pi=[512, 512, 256], vf=[512, 512, 256])), 

    # ğŸ’¡ èª¿æ•´ç†µä¿‚æ•¸ (é™ä½éš¨æ©Ÿæ€§ï¼Œé¼“å‹µæ”¶æ–‚)
    'ent_coef': 0.005,         

    # Training and Logging
    'total_timesteps': 10_000_000,
    'log_interval': 10000, 

    # PBRS Parameters (Low-Level Only)
    'k1': 10.0,  # æ¥è¿‘çƒçš„æ½›åŠ›ä¿‚æ•¸ (agent-ball)
    'k2': 5.0,   # è¸¢å‘ç›®æ¨™çš„æ½›åŠ›ä¿‚æ•¸ (ball-goal)
    'k3': 2.0,   # ğŸ’¡ æ–°å¢è§’åº¦å¼•å°ä¿‚æ•¸ (åœ¨ kick éšæ®µç”Ÿæ•ˆï¼Œé¼“å‹µæ©Ÿå™¨äººç«™åœ¨å¥½çš„ä½ç½®è¸¢çƒ)
    
    # HRL Parameters (High-Level Only)
    'll_steps': 10, # æ¯å€‹é«˜éšæ™‚é–“æ­¥åŸ·è¡Œçš„ä½éšæ­¥æ•¸
}


# --- ç’°å¢ƒå»ºç«‹å‡½æ•¸ (å¢åŠ  VecNormalize) ---
def make_env(
    sai: SAIClient,
    comp_id: str,
    stage: str,
    num_envs: int,
    config: Dict[str, Any],
) -> gym.Env:
    """
    å»ºç«‹å‘é‡åŒ–ç’°å¢ƒï¼Œä¸¦æ ¹æ“š stage æ‡‰ç”¨ Wrapperï¼Œæœ€å¾Œæ‡‰ç”¨ VecNormalizeã€‚
    """
    if stage in ('move', 'kick'):
        env = make_pbrs_env(
            sai=sai, 
            comp_id=comp_id, 
            stage=stage, 
            num_envs=num_envs, 
            config=config # config åŒ…å« k1, k2, k3
        )
    elif stage == 'hrl':
        # å»ºç«‹åŸºç¤ç’°å¢ƒä¸¦åŒ…è£¹ HierarchicalWrapper
        def env_fn():
            # å‡è¨­ HRL ä½¿ç”¨çš„åŸºç¤ç’°å¢ƒèˆ‡ kick è¨“ç·´ç›¸ä¼¼
            base_env = sai.make_env()
            # HierarchicalWrapper æœƒåœ¨å…§éƒ¨è™•ç† LL Policy çš„è¼‰å…¥
            return HierarchicalWrapper(base_env, ll_steps=config['ll_steps'])

        env = DummyVecEnv([env_fn] * num_envs)

    else:
        raise ValueError(f"ä¸æ”¯æ´çš„éšæ®µ: {stage}")

    # ğŸ’¡ æ‡‰ç”¨è§€å¯Ÿç©ºé–“å’Œçå‹µæ­£è¦åŒ– (å¤§å¹…æé«˜ç©©å®šæ€§)
    # æˆ‘å€‘åœ¨é€™è£¡ä½¿ç”¨ True/True é€²è¡Œæ­£è¦åŒ–
    env = VecNormalize(
        env, 
        norm_obs=True, 
        norm_reward=True, 
        clip_obs=10.,
        gamma=config['gamma']
    )

    return env


# --- ä¸»é‚è¼¯ (Main Logic) ---
def main(stage: str, mode: str, num_envs: int = 1):
    # åˆå§‹åŒ– SAI Client
    sai = SAIClient(
        comp_id="booster-soccer-showdown", # âš ï¸ è«‹æ›¿æ›ç‚ºæ‚¨çš„æ¯”è³½ ID
        api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv",        # âš ï¸ è«‹æ›¿æ›ç‚ºæ‚¨çš„ API Key
    )

    config = default_config

    # --- è¨“ç·´ç’°å¢ƒèˆ‡æ¨¡å‹æº–å‚™ ---
    print("ğŸ› ï¸ æ­£åœ¨åˆå§‹åŒ–ç’°å¢ƒå’Œæ¨¡å‹...")
    env = make_env(sai, comp_id="booster-soccer-showdown", stage=stage, num_envs=num_envs, config=config)

    # è¨­ç½®æ—¥èªŒå’Œæ¨¡å‹å„²å­˜è·¯å¾‘
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_prefix = f"ppo_{stage}_{current_time}"
    
    if stage == 'hrl':
        base_dir = HRL_MODEL_DIR
    else:
        base_dir = MODEL_DIR
        
    save_path = os.path.join(base_dir, save_prefix)
    log_dir = os.path.join("runs", save_prefix)
    
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    # è¨­å®š logger ä»¥ç¢ºä¿æ—¥èªŒè¼¸å‡ºåˆ°æŒ‡å®šç›®éŒ„
    new_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])

    # åˆå§‹åŒ–æˆ–è¼‰å…¥æ¨¡å‹
    if mode == 'new':
        print(f"ğŸ”„ å‰µå»ºæ–°çš„ {stage.upper()} PPO æ¨¡å‹...")
        model = PPO(
            config['policy'],
            env,
            verbose=1,
            tensorboard_log=log_dir,
            learning_rate=config['learning_rate'],
            n_steps=config['n_steps'] // num_envs, 
            batch_size=config['batch_size'],
            gamma=config['gamma'],
            n_epochs=config['n_epochs'],
            clip_range=config['clip_range'],
            gae_lambda=config['gae_lambda'],
            ent_coef=config['ent_coef'],
            policy_kwargs=config['policy_kwargs'],
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        model.set_logger(new_logger)
    elif mode == 'continue':
        # è¼‰å…¥é‚è¼¯éœ€è¦é¡å¤–çš„æª”æ¡ˆè·¯å¾‘è™•ç†ï¼Œé€™è£¡éœ€è¦ä½¿ç”¨è€…è™•ç†è·¯å¾‘å’Œ VecNormalize çµ±è¨ˆæ•¸æ“š
        raise NotImplementedError("Continue mode requires specifying a model path and handling VecNormalize stats loading.")
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å¼: {mode}")

    # --- è¨“ç·´åƒæ•¸æ‘˜è¦ (çœç•¥äº†åŸæ–‡ä»¶ä¸­çš„éƒ¨åˆ†è¼¸å‡ºï¼Œä½†ç¢ºä¿æ ¸å¿ƒåƒæ•¸å¯è¦‹) ---
    print("\n-----------------------------")
    print(f"STAGE: {stage.upper()} | ENVS: {num_envs}")
    print(f"Learning Rate: {config['learning_rate']} | Gamma: {config['gamma']}")
    print(f"Ent Coef: {config['ent_coef']} | Total Timesteps: {config['total_timesteps']}")
    if stage != 'hrl':
        print(f"PBRS: k1={config['k1']}, k2={config['k2']}, k3={config['k3']}")
    else:
        print(f"HRL LL Steps: {config['ll_steps']}")
    print("-----------------------------\n")

    # --- Model Training ---
    callback = DetailedLogCallback(
        save_path=save_path, 
        save_prefix=save_prefix, 
        log_interval=config['log_interval'],
        verbose=1
    )

    try:
        model.learn(total_timesteps=config['total_timesteps'], callback=callback, reset_num_timesteps=(mode=='new'))
    except KeyboardInterrupt:
        print("\nTraining interrupted by user.")
    finally:
        # ğŸ’¡ ä¿å­˜æœ€çµ‚æ¨¡å‹å’Œ VecNormalize çµ±è¨ˆæ•¸æ“š
        final_model_path = os.path.join(save_path, f"{save_prefix}_final.zip")
        model.save(final_model_path)
        print(f"\nâœ… Final model saved to {final_model_path}")

        # --- æ–°å¢é‚è¼¯: å°‡æ¨¡å‹è¤‡è£½åˆ° HRL é æœŸçš„å›ºå®šè·¯å¾‘ (FIX) ---
        if stage in ['move', 'kick']:
            # HRL é æœŸçš„å›ºå®šè·¯å¾‘ï¼šlow_level_models/move_policy_final.zip æˆ– kick_policy_final.zip
            hrl_target_path = os.path.join(MODEL_DIR, f"{stage}_policy_final.zip") 
            try:
                shutil.copyfile(final_model_path, hrl_target_path)
                print(f"âœ… Copied {stage} model to HRL fixed path: {hrl_target_path}")
            except Exception as e:
                # é›–ç„¶ä¸æ‡‰ç™¼ç”Ÿï¼Œä½†ä¿ç•™éŒ¯èª¤è™•ç†
                print(f"âŒ Warning: Failed to copy model to HRL fixed path: {e}")
        # -----------------------------------------------------------
        
        # ä¿å­˜ VecNormalize çµ±è¨ˆæ•¸æ“š (å°æ¨è«–å¾ˆé‡è¦)
        stats_path = os.path.join(save_path, f"vec_normalize_{stage}.pkl")
        env.save(stats_path)
        print(f"âœ… VecNormalize stats saved to {stats_path}")
        
        env.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="PPO Training Script with PBRS/HRL")
    parser.add_argument('--stage', type=str, required=True, choices=['move', 'kick', 'hrl'],
                        help="è¨“ç·´éšæ®µ: move (ç§»å‹•), kick (è¸¢çƒ), hrl (åˆ†å±¤)")
    parser.add_argument('--mode', type=str, default='new', choices=['new', 'continue'],
                        help="è¨“ç·´æ¨¡å¼: new (æ–°çš„è¨“ç·´), continue (ç¹¼çºŒè¨“ç·´)")
    parser.add_argument('--num_envs', type=int, default=1,
                        help="å‘é‡åŒ–ç’°å¢ƒæ•¸é‡")
    
    args = parser.parse_args()
    main(args.stage, args.mode, args.num_envs)