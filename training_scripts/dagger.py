"""
DAgger (Dataset Aggregation) - æ”¹é€²çš„æ¨¡ä»¿å­¸ç¿’
çµåˆåœ¨ç·šæ”¶é›†å’Œé›¢ç·šè¨“ç·´ï¼Œæ¯”ç´”BCæ›´å¼·å¤§
"""

import torch
import torch.nn as nn
import numpy as np
from collections import deque
import pickle
import os
from datetime import datetime

from behavioral_cloning import BehavioralCloningAgent, ExpertDataset
from sai_rl import SAIClient

class DAggerAgent:
    """DAggeræ™ºèƒ½é«”"""
    
    def __init__(self, obs_dim, action_dim, initial_trajectories=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è¡Œç‚ºå…‹éš†æ¨¡å‹
        self.bc_model = BehavioralCloningAgent(obs_dim, action_dim).to(self.device)
        self.optimizer = torch.optim.AdamW(self.bc_model.parameters(), lr=1e-4)
        
        # æ•¸æ“šèšåˆ
        self.all_trajectories = initial_trajectories or []
        self.iteration = 0
        
        # ç’°å¢ƒ
        self.sai = SAIClient(comp_id="booster-soccer-showdown", api_key="sai_LFcuaCZiqEkUbNVolQ3wbk5yU7H11jfv")
        self.env = self.sai.make_env()
        
        from main_improved_dreamerv3 import Preprocessor
        self.preprocessor = Preprocessor()
        
        print("ğŸ¯ DAgger Agent åˆå§‹åŒ–å®Œæˆ")
    
    def collect_trajectories_with_expert_labels(self, num_episodes=10):
        """ä½¿ç”¨ç•¶å‰ç­–ç•¥æ”¶é›†è»Œè·¡ï¼Œä¸¦ç”¨å°ˆå®¶ç­–ç•¥æ¨™è¨˜"""
        
        print(f"ğŸ® DAgger Iteration {self.iteration}: æ”¶é›† {num_episodes} å€‹episode...")
        
        new_trajectories = []
        
        for episode in range(num_episodes):
            obs, info = self.env.reset()
            obs = self.preprocessor.modify_state(obs, info).squeeze()
            
            trajectory = {
                'observations': [],
                'expert_actions': [],  # å°ˆå®¶æ¨™è¨˜çš„å‹•ä½œ
                'policy_actions': [],  # ç•¶å‰ç­–ç•¥çš„å‹•ä½œ
                'rewards': [],
                'episode_reward': 0
            }
            
            hidden_state = None
            
            for step in range(400):
                trajectory['observations'].append(obs.copy())
                
                # ç•¶å‰ç­–ç•¥å‹•ä½œ
                if self.iteration == 0 or np.random.random() < 0.3:
                    # å‰å¹¾æ¬¡è¿­ä»£æˆ–éš¨æ©Ÿæ™‚ä½¿ç”¨å°ˆå®¶ç­–ç•¥
                    expert_action = self._get_expert_action(obs)
                    policy_action = expert_action
                else:
                    # ä½¿ç”¨ç•¶å‰ç­–ç•¥
                    policy_action, hidden_state = self.bc_model.select_action(obs, hidden_state)
                    expert_action = self._get_expert_action(obs)
                
                trajectory['policy_actions'].append(policy_action.copy())
                trajectory['expert_actions'].append(expert_action.copy())
                
                # åŸ·è¡Œå‹•ä½œï¼ˆä½¿ç”¨ç­–ç•¥å‹•ä½œï¼Œä½†å­¸ç¿’å°ˆå®¶å‹•ä½œï¼‰
                env_action = self.env.action_space.low + (self.env.action_space.high - self.env.action_space.low) * (policy_action + 1) / 2
                next_obs, reward, terminated, truncated, next_info = self.env.step(env_action)
                
                trajectory['rewards'].append(reward)
                trajectory['episode_reward'] += reward
                
                obs = self.preprocessor.modify_state(next_obs, next_info).squeeze()
                
                if terminated or truncated:
                    break
            
            new_trajectories.append(trajectory)
            print(f"   Episode {episode}: {trajectory['episode_reward']:.3f}")
        
        # åŠ å…¥æ•¸æ“šé›†
        self.all_trajectories.extend(new_trajectories)
        print(f"ğŸ“Š ç¸½è»Œè·¡æ•¸: {len(self.all_trajectories)}")
        
        return new_trajectories
    
    def _get_expert_action(self, obs):
        """ç²å–å°ˆå®¶å‹•ä½œï¼ˆé€™è£¡ä½¿ç”¨å•Ÿç™¼å¼å°ˆå®¶ï¼‰"""
        
        # ç°¡å–®çš„å•Ÿç™¼å¼å°ˆå®¶ç­–ç•¥
        # ç›®æ¨™ï¼šä¿æŒç©©å®š + æœçƒç§»å‹•
        
        action = np.zeros(12)
        
        # æ·»åŠ å°çš„éš¨æ©Ÿæ“¾å‹•ä¿æŒç©©å®š
        action += np.random.normal(0, 0.05, 12)
        
        # é™åˆ¶å‹•ä½œç¯„åœ
        action = np.clip(action, -0.3, 0.3)
        
        return action
    
    def train_on_aggregated_data(self, num_epochs=50):
        """åœ¨èšåˆæ•¸æ“šä¸Šè¨“ç·´"""
        
        print(f"ğŸ”„ åœ¨èšåˆæ•¸æ“šä¸Šè¨“ç·´...")
        
        # è½‰æ›è»Œè·¡æ ¼å¼ç‚ºBCæ ¼å¼
        bc_trajectories = []
        for traj in self.all_trajectories:
            bc_traj = {
                'observations': traj['observations'],
                'actions': traj['expert_actions'],  # å­¸ç¿’å°ˆå®¶å‹•ä½œ
                'episode_reward': traj['episode_reward']
            }
            bc_trajectories.append(bc_traj)
        
        # å‰µå»ºæ•¸æ“šé›†
        dataset = ExpertDataset(bc_trajectories, sequence_length=5)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
        
        # è¨“ç·´
        self.bc_model.train()
        for epoch in range(num_epochs):
            epoch_loss = 0
            num_batches = 0
            
            for obs_seq, action_seq, target_action in dataloader:
                obs_seq = obs_seq.to(self.device)
                action_seq = action_seq.to(self.device)
                target_action = target_action.to(self.device)
                
                self.optimizer.zero_grad()
                
                predicted_actions = self.bc_model(obs_seq, action_seq)
                loss = torch.nn.functional.mse_loss(predicted_actions[:, -1], target_action)
                
                loss.backward()
                self.optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch}: Loss = {epoch_loss/num_batches:.6f}")
    
    def run_dagger_iteration(self):
        """é‹è¡Œä¸€æ¬¡DAggerè¿­ä»£"""
        
        print(f"\nğŸ¯ DAgger Iteration {self.iteration}")
        print("="*50)
        
        # 1. æ”¶é›†æ–°è»Œè·¡
        new_trajectories = self.collect_trajectories_with_expert_labels(num_episodes=20)
        
        # 2. åœ¨èšåˆæ•¸æ“šä¸Šè¨“ç·´
        self.train_on_aggregated_data(num_epochs=30)
        
        # 3. è©•ä¼°ç•¶å‰ç­–ç•¥
        avg_reward = self.evaluate_policy()
        
        # 4. ä¿å­˜æ¨¡å‹
        self.save_checkpoint()
        
        self.iteration += 1
        
        return avg_reward
    
    def evaluate_policy(self, num_episodes=5):
        """è©•ä¼°ç•¶å‰ç­–ç•¥"""
        
        print(f"ğŸ“Š è©•ä¼°ç­–ç•¥...")
        
        self.bc_model.eval()
        rewards = []
        
        for episode in range(num_episodes):
            obs, info = self.env.reset()
            obs = self.preprocessor.modify_state(obs, info).squeeze()
            
            episode_reward = 0
            hidden_state = None
            
            for step in range(400):
                action, hidden_state = self.bc_model.select_action(obs, hidden_state)
                
                env_action = self.env.action_space.low + (self.env.action_space.high - self.env.action_space.low) * (action + 1) / 2
                next_obs, reward, terminated, truncated, next_info = self.env.step(env_action)
                
                episode_reward += reward
                obs = self.preprocessor.modify_state(next_obs, next_info).squeeze()
                
                if terminated or truncated:
                    break
            
            rewards.append(episode_reward)
            print(f"   Episode {episode}: {episode_reward:.3f}")
        
        avg_reward = np.mean(rewards)
        print(f"ğŸ“ˆ å¹³å‡çå‹µ: {avg_reward:.3f}")
        
        return avg_reward
    
    def save_checkpoint(self):
        """ä¿å­˜æª¢æŸ¥é»"""
        
        os.makedirs('saved_models/dagger', exist_ok=True)
        
        checkpoint = {
            'iteration': self.iteration,
            'model_state_dict': self.bc_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'trajectories_count': len(self.all_trajectories)
        }
        
        torch.save(checkpoint, f'saved_models/dagger/dagger_iter_{self.iteration}.pth')
        print(f"ğŸ’¾ å·²ä¿å­˜æª¢æŸ¥é»: iteration {self.iteration}")

def run_dagger_training():
    """é‹è¡Œå®Œæ•´çš„DAggerè¨“ç·´"""
    
    print("ğŸš€ DAggerè¨“ç·´é–‹å§‹")
    print("="*60)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰åˆå§‹å°ˆå®¶è»Œè·¡
    initial_trajectories = []
    expert_data_path = "expert_data/expert_trajectories.pkl"
    if os.path.exists(expert_data_path):
        with open(expert_data_path, 'rb') as f:
            initial_trajectories = pickle.load(f)
        print(f"âœ… è¼‰å…¥ {len(initial_trajectories)} æ¢åˆå§‹å°ˆå®¶è»Œè·¡")
    
    # å‰µå»ºDAgger agent
    agent = DAggerAgent(obs_dim=89, action_dim=12, initial_trajectories=initial_trajectories)
    
    # é‹è¡Œå¤šæ¬¡è¿­ä»£
    num_iterations = 10
    best_reward = float('-inf')
    
    for iteration in range(num_iterations):
        avg_reward = agent.run_dagger_iteration()
        
        if avg_reward > best_reward:
            best_reward = avg_reward
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ€§èƒ½: {best_reward:.3f}")
        
        # æ—©åœæ¢ä»¶
        if avg_reward > 0:  # å¦‚æœç²å¾—æ­£çå‹µ
            print(f"ğŸ¯ é”åˆ°æ­£çå‹µï¼Œè¨“ç·´å®Œæˆï¼")
            break
    
    print(f"\nğŸ† DAggerè¨“ç·´å®Œæˆï¼")
    print(f"   æœ€ä½³çå‹µ: {best_reward:.3f}")
    print(f"   ç¸½è¿­ä»£æ•¸: {agent.iteration}")
    
    return agent

if __name__ == "__main__":
    # é‹è¡ŒDAggerè¨“ç·´
    agent = run_dagger_training()
    
    print("\nğŸ¯ å‰µå»ºæœ€çµ‚æäº¤...")
    
    # æäº¤åˆ°SAI
    from sai_compatible_dreamerv3 import SAICompatibleDreamerV3
    from main_improved_dreamerv3 import Preprocessor
    
    sai_model = SAICompatibleDreamerV3(agent.bc_model)
    
    def action_function(policy):
        expected_bounds = [-1, 1]
        action_percent = (policy - expected_bounds[0]) / (expected_bounds[1] - expected_bounds[0])
        bounded_percent = np.minimum(np.maximum(action_percent, 0), 1)
        env = agent.env
        return env.action_space.low + (env.action_space.high - env.action_space.low) * bounded_percent
    
    print("ğŸ” æœ¬åœ°æ¸¬è©¦...")
    agent.sai.benchmark(sai_model, action_function, Preprocessor)
    
    print("ğŸš€ æäº¤åˆ°æ’è¡Œæ¦œ...")
    agent.sai.submit("Vedanta_DAgger", sai_model, action_function, Preprocessor)
    
    print("ğŸ‰ DAggeræ¨¡å‹æäº¤å®Œæˆï¼")