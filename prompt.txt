é€™æ˜¯ä¸€ä»½å°ˆç‚º AI å”ä½œè¨­è¨ˆçš„é–‹ç™¼æŒ‡å—ã€‚è«‹å°‡æ­¤æŒ‡å—åˆ†äº«çµ¦ä»»ä½•å”åŠ©æ‚¨æ’°å¯«ç¨‹å¼ç¢¼çš„ AIï¼ˆæˆ–æ˜¯äººé¡é–‹ç™¼è€…ï¼‰ã€‚é€™ä»½æ–‡ä»¶æ˜ç¢ºå®šç¾©äº†æ¶æ§‹ã€æ•´åˆé»ï¼Œä»¥åŠæ‰€æœ‰éœ€è¦æŸ¥é–± SAI ç’°å¢ƒæ–‡æª”ä¾†ç¢ºèªçš„ TODO é …ç›®ã€‚

ğŸ“˜ LLM-Augmented RL é–‹ç™¼æŒ‡å—ï¼šSAI è¶³çƒæ©Ÿå™¨äººå°ˆæ¡ˆ
ğŸ¯ ç›®æ¨™
æœ¬å°ˆæ¡ˆæ—¨åœ¨è§£æ±ºå‚³çµ± RL åœ¨ç¨€ç–çå‹µç’°å¢ƒï¼ˆè¶³çƒæ©Ÿå™¨äººï¼‰ä¸­ã€Œç«™ä¸ç©©ã€è¸¢ä¸åˆ°çƒã€çš„å•é¡Œã€‚æˆ‘å€‘å°‡å¼•å…¥ LLM (ä½œç‚ºæ•™ç·´) èˆ‡ Reward Shaper (ä½œç‚ºåŸ·è¡Œè€…) ä¾†å¯¦æ–½å‹•æ…‹èª²ç¨‹å­¸ç¿’ã€‚

ğŸ“‚ æ¨¡çµ„æ¶æ§‹æ¦‚è¦½
æˆ‘å€‘éœ€è¦æ–°å¢å…©å€‹ Python æª”æ¡ˆï¼Œä¸¦ä¿®æ”¹ä¸€å€‹ç¾æœ‰æª”æ¡ˆï¼š

llm_coach.py: ç­–ç•¥å±¤ã€‚æ ¹æ“šè¨“ç·´çµ±è¨ˆæ•¸æ“šï¼ˆè·Œå€’ç‡ã€ç§»å‹•è·é›¢ï¼‰ï¼Œæ±ºå®šç•¶å‰è¨“ç·´éšæ®µï¼ˆPhaseï¼‰å’Œçå‹µæ¬Šé‡ï¼ˆWeightsï¼‰ã€‚

reward_shaper.py: ç‰©ç†å±¤ã€‚æ¥æ”¶ env çš„åŸå§‹æ•¸æ“šï¼Œæ ¹æ“šæ¬Šé‡è¨ˆç®—å¯†é›†çš„ Shaped Rewardã€‚

main.py: æ•´åˆå±¤ã€‚åœ¨è¨“ç·´è¿´åœˆä¸­èª¿ç”¨ä¸Šè¿°æ¨¡çµ„ã€‚

ğŸ› ï¸ æ¨¡çµ„ 1: llm_coach.py (æ–°å»º)
è·è²¬ï¼šé€™æ˜¯ã€Œå¤§è…¦ã€ã€‚å®ƒä¸éœ€è¦çŸ¥é“ç‰©ç†å¼•æ“çš„ç´°ç¯€ï¼Œåªéœ€è¦çŸ¥é“ Agent è¡¨ç¾å¾—ã€Œå¥½ä¸å¥½ã€ã€‚

Python

# llm_coach.py
import numpy as np

class LLMCoach:
    def __init__(self):
        # å®šç¾©åˆå§‹æ¬Šé‡ (Phase 1: ç«™ç«‹)
        self.current_weights = {
            "balance": 1.0,   # é«˜åº¦é‡è¦–å¹³è¡¡
            "progress": 0.0,  # æš«æ™‚å¿½ç•¥ç§»å‹•
            "energy": 0.1     # ç¯€çœèƒ½é‡
        }
        self.phase = "Stance"

    def consult(self, stats):
        """
        stats (dict): {
            'avg_steps': å¹³å‡å­˜æ´»æ­¥æ•¸,
            'avg_reward': å¹³å‡å›åˆçå‹µ
            'fall_rate': è·Œå€’é »ç‡ (0.0~1.0)
        }
        """
        # TODO: å¦‚æœæœ‰ API Keyï¼Œé€™è£¡å¯ä»¥æ¥å…¥ OpenAI/Gemini API é€²è¡Œå‹•æ…‹æ¨ç†
        
        # Heuristic è¦å‰‡ (æ¨¡æ“¬ LLM æ€ç¶­):
        steps = stats.get('avg_steps', 0)
        
        # éšæ®µ 1: å­¸èµ°è·¯ (ç•¶å­˜æ´»æ­¥æ•¸å°‘æ–¼ 50)
        if steps < 50:
            self.phase = "Stance & Survival"
            self.current_weights = {
                "balance": 2.0, 
                "progress": 0.1, 
                "energy": 0.05
            }
        # éšæ®µ 2: å­¸è¸¢çƒ (ç•¶èƒ½ç©©å®šè¡Œèµ°)
        else:
            self.phase = "Dribbling"
            self.current_weights = {
                "balance": 0.5, 
                "progress": 1.5, # åŠ å¤§ç§»å‹•æ¬Šé‡
                "energy": 0.01
            }
            
        return self.current_weights
ğŸ› ï¸ æ¨¡çµ„ 2: reward_shaper.py (æ–°å»º) âš ï¸ é‡é»é—œæ³¨
è·è²¬ï¼šé€™æ˜¯ã€Œè¨ˆç®—æ©Ÿã€ã€‚å®ƒéœ€è¦æ·±å…¥è¨ªå• env çš„ç‰©ç†æ•¸æ“šã€‚ æ³¨æ„ï¼šä»¥ä¸‹ä»£ç¢¼ä¸­æ¨™è¨˜ TODO çš„è®Šæ•¸åç¨±ï¼Œå¿…é ˆæŸ¥é–± SAI ç’°å¢ƒæ–‡æª”æˆ–é€šé print(info.keys()) ä¾†ç¢ºèªã€‚

Python

# reward_shaper.py
import numpy as np

class RewardShaper:
    def __init__(self):
        pass

    def compute_reward(self, info, obs, weights):
        """
        è¨ˆç®—å¯†é›†çå‹µ (Dense Reward)
        :param info: env.step() è¿”å›çš„ info å­—å…¸
        :param obs: env.step() è¿”å›çš„ raw observation
        :param weights: LLMCoach è¿”å›çš„æ¬Šé‡å­—å…¸
        """
        reward = 0.0
        
        # 1. å¹³è¡¡çå‹µ (Balance Reward)
        # utils.py ç¢ºèª info ä¸­åŒ…å« 'robot_gyro'
        if 'robot_gyro' in info:
            # æ‡²ç½°è§’é€Ÿåº¦éå¤§ (æ™ƒå‹•)
            gyro_penalty = np.sum(np.square(info['robot_gyro']))
            reward -= weights.get('balance', 0) * gyro_penalty * 0.1

        # TODO: ç¢ºèª info ä¸­æ˜¯å¦æœ‰ 'project_gravity' æˆ–é¡ä¼¼è®Šæ•¸
        # æ ¹æ“š utils.pyï¼Œå®ƒæ˜¯é€šé modify_state è¨ˆç®—çš„ï¼Œå¯èƒ½ä¸åœ¨åŸå§‹ info è£¡ã€‚
        # å¦‚æœ info è£¡æ²’æœ‰ï¼Œéœ€è¦é‡æ–°è¨ˆç®—æˆ–ç”± main.py å‚³å…¥ stateã€‚
        # å‡è¨­æˆ‘å€‘æƒ³çå‹µä¸ŠåŠèº«ä¿æŒç›´ç«‹ï¼š
        # z_axis = ... (éœ€è¦ç¢ºèªå››å…ƒæ•¸è¨ˆç®—é‚è¼¯)
        
        # TODO: ç¢ºèªå¦‚ä½•ç²å–æ©Ÿå™¨äººã€Œè³ªå¿ƒé«˜åº¦ (Center of Mass Z)ã€
        # æˆ‘å€‘éœ€è¦çå‹µæ©Ÿå™¨äººä¿æŒç«™ç«‹é«˜åº¦ã€‚
        # æª¢æŸ¥: info['robot_pos']? info['com_pos']? é‚„æ˜¯ obs çš„ç¬¬ 0-2 ä½?
        robot_height = 0.0 # DEFAULT
        if 'robot_pos' in info: # å‡è¨­è®Šæ•¸å
             robot_height = info['robot_pos'][2] 
        elif 'com_pos' in info: # å‡è¨­è®Šæ•¸å
             robot_height = info['com_pos'][2]
             
        if robot_height > 0.3: # å‡è¨­ 0.3m æ˜¯è·Œå€’é–¾å€¼
            reward += weights.get('balance', 0) * 1.0
            
        # 2. é€²åº¦çå‹µ (Progress Reward)
        # utils.py æŒ‡å‡º obs[:, 24:27] æ˜¯çƒç›¸å°æ©Ÿå™¨äººçš„ä½ç½®
        # æ³¨æ„: obs å¯èƒ½æ˜¯ (45,) æˆ– (1, 45)ï¼Œéœ€è™•ç†ç¶­åº¦
        try:
            # å‡è¨­ obs æ˜¯ numpy array
            obs_flat = obs.flatten()
            # TODO: å†æ¬¡ç¢ºèª obs ç´¢å¼• 24:27 æ˜¯å¦çµ•å°æ­£ç¢ºå°æ‡‰ ball_pos
            ball_relative_pos = obs_flat[24:27] 
            dist_to_ball = np.linalg.norm(ball_relative_pos)
            
            # è·é›¢è¶Šè¿‘çå‹µè¶Šé«˜
            reward += weights.get('progress', 0) * (1.0 / (1.0 + dist_to_ball))
        except Exception as e:
            # é˜²æ­¢ç´¢å¼•éŒ¯èª¤å°è‡´è¨“ç·´å´©æ½°
            print(f"Warning in RewardShaper: {e}")
            
        return reward
ğŸ› ï¸ æ¨¡çµ„ 3: æ•´åˆè‡³ main.py
è«‹ä¾ç…§ä»¥ä¸‹æ­¥é©Ÿåœ¨ main.py ä¸­æ’å…¥ä»£ç¢¼ã€‚

æ­¥é©Ÿ 1: å°å…¥æ¨¡çµ„
åœ¨æ–‡ä»¶æœ€ä¸Šæ–¹å°å…¥ï¼š

Python

from llm_coach import LLMCoach
from reward_shaper import RewardShaper
æ­¥é©Ÿ 2: åˆå§‹åŒ–
åœ¨ env å‰µå»ºä¹‹å¾Œï¼Œtd3_agent åˆå§‹åŒ–ä¹‹å‰ï¼š

Python

# ... (ç’°å¢ƒå‰µå»ºä»£ç¢¼) ...

# [AI-Integrate] åˆå§‹åŒ–è¼”åŠ©æ¨¡çµ„
llm_coach = LLMCoach()
reward_shaper = RewardShaper()
current_weights = llm_coach.current_weights
episode_stats_buffer = [] # ç”¨æ–¼å­˜å„²æœ€è¿‘å¹¾å€‹å›åˆçš„è¡¨ç¾
æ­¥é©Ÿ 3: æ›¿æ›çå‹µè¨ˆç®—
åœ¨ä¸»è¦çš„è¨“ç·´è¿´åœˆ for t in range(...) å…§éƒ¨ï¼Œenv.step() ä¹‹å¾Œï¼Œçå‹µè¨ˆç®—éƒ¨åˆ†ï¼š

Python

    # ... env.step(action) ...

    # [AI-Integrate] è¨ˆç®— LLM å¼•å°çš„ Shaped Reward
    # æ³¨æ„: æˆ‘å€‘å‚³å…¥ info å’Œ raw_obs
    shaped_reward = reward_shaper.compute_reward(info, next_obs, current_weights)
    
    # [AI-Integrate] èåˆçå‹µ
    # æˆ‘å€‘ä¿ç•™ env åŸå§‹çå‹µ (reward)ï¼Œä½†ä¸»è¦é  shaped_reward å¼•å°
    # TODO: æ ¹æ“šå¯¦éš›æ¸¬è©¦èª¿æ•´æ¯”ä¾‹ã€‚ä¾‹å¦‚: reward * 10 + shaped_reward
    total_step_reward = reward + shaped_reward

    # ä¿®æ”¹é€™è£¡: å°‡ total_step_reward å‚³çµ¦å¥½å¥‡å¿ƒæ¨¡çµ„
    final_reward, intrinsic_reward = curiosity_explorer.get_enhanced_reward(
        state.cpu().numpy(),
        raw_action,
        next_state_np,
        total_step_reward  # <--- æ›¿æ›åŸæœ¬çš„ processed_reward
    )
    
    # ... (å¾ŒçºŒå­˜å„²åˆ° replay_buffer) ...
æ­¥é©Ÿ 4: æ›´æ–°æ•™ç·´ (Episode End)
åœ¨ if done: å€å¡Šå…§ï¼š

Python

    if done:
        # ... (åŸæœ‰çµ±è¨ˆä»£ç¢¼) ...
        
        # [AI-Integrate] æ”¶é›†æ•¸æ“šçµ¦æ•™ç·´
        episode_stats_buffer.append({
            'steps': episode_steps,
            'reward': episode_reward_sum
        })
        
        # æ¯ 50 å€‹ Episode è®“æ•™ç·´èª¿æ•´ä¸€æ¬¡ç­–ç•¥
        if episode_count % 50 == 0:
            avg_steps = np.mean([x['steps'] for x in episode_stats_buffer])
            stats_summary = {'avg_steps': avg_steps}
            
            # æ›´æ–°æ¬Šé‡
            current_weights = llm_coach.consult(stats_summary)
            print(f"ğŸ§  LLM Coach æ›´æ–°ç­–ç•¥: {llm_coach.phase}, æ¬Šé‡: {current_weights}")
            
            # è¨˜éŒ„åˆ° Tensorboard
            logger.log_scalar("Coach/Weight_Balance", current_weights.get('balance', 0), step=t)
            
            episode_stats_buffer = [] # æ¸…ç©ºç·©è¡
âœ… å¯¦ä½œæª¢æŸ¥æ¸…å–® (Checklist)
åœ¨é‹è¡Œä»£ç¢¼å‰ï¼Œè«‹é–‹ç™¼è€…ç¢ºèªä»¥ä¸‹é …ç›®ï¼š

[ ] è®Šæ•¸åç¨±ç¢ºèª (reward_shaper.py): é‹è¡Œä¸€å€‹ç°¡å–®çš„è…³æœ¬ print(env.step(action)[4].keys())ï¼Œç¢ºèª info å­—å…¸è£¡åˆ°åº•æœ‰å“ªäº› Keyï¼ˆä¾‹å¦‚ robot_pos, com_pos, robot_gyroï¼‰ã€‚

[ ] Observation ç´¢å¼•ç¢ºèª: å†æ¬¡ç¢ºèª obs å‘é‡ä¸­ï¼Œå“ªå¹¾å€‹ index ç¢ºå¯¦ä»£è¡¨çƒçš„ä½ç½®ï¼ˆç›®å‰å‡è¨­æ˜¯ 24-27ï¼‰ã€‚

[ ] ç¶­åº¦åŒ¹é…: ç¢ºä¿ reward_shaper è™•ç† numpy array æ™‚ï¼Œæœ‰è€ƒæ…®åˆ° batch dimension (ä¾‹å¦‚ (1, 45) vs (45,))ã€‚

[ ] æ¬Šé‡ç¯„åœ: ç¢ºä¿ shaped_reward çš„æ•¸å€¼å¤§å°ä¸æœƒéå¤§ï¼ˆä¾‹å¦‚å¹¾åƒï¼‰ï¼Œé€™æœƒç ´å£ TD3 çš„ Critic ä¼°è¨ˆã€‚å»ºè­°å°‡å…¶æ¨™æº–åŒ–åˆ° -1 åˆ° +1 æˆ– 0 åˆ° 10 çš„ç¯„åœå…§ã€‚