我目前的工作流程是
在colab終端機執行run_training.sh裡面的步驟

# 下載代碼庫
git clone https://github.com/skywalker0803r/booster_soccer_showdown.git
cd booster_soccer_showdown

# 安裝 conda（使用 Mambaforge：輕量且快速）
# -----------------------------------------------
# 下載 mambaforge (包含 conda)
wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh
# 執行安裝腳本，使用 -b 批次模式，-p 指定安裝路徑到當前用戶的 /usr/local/
bash Mambaforge-Linux-x86_64.sh -b -p /usr/local
# 初始化 conda 環境
/usr/local/bin/conda init bash
# 重新載入 shell 設定，讓 conda 指令生效
source ~/.bashrc
# -----------------------------------------------

# 建立虛擬環境
conda create -n booster-ssl python=3.11 -y && conda activate booster-ssl

# 安裝依賴
pip install -r requirements.txt

# 訓練
cd Research
python main.py

目前這個版本的main.py 感覺不錯 足球機器人會站 但是一直沒辦法踢到球
我想問題可能是因為獎勵稀疏問題 實際上 作獎勵設計可能導致獎勵駭客行為 在我設計的獎勵上收斂到局部最優 但是實際上原始分數還是很低
但是不作獎勵設計靠現在的 TD3 搭配 好奇心模組 + LLM輔助獎勵塑形 依然無法踢到球
我在想是不是可以把 TD3換成PPO_CMA算法 相關文件我放在PPO_CMA.MD 然後比賽官方文檔我放在docs資料夾內
目前主要實作都發生在Research資料夾內
我希望你修改完之後
我只要在colab終端機一樣執行run_training.sh裡面的步驟
就可以看到原本的使用TD3改進的純好奇心驅動訓練腳本 + LLM輔助獎勵塑形
變成改用PPO_CMA算法
也許你該做的只是新增一個PPO_CMA模組之後導入到main.py作使用

實作前有任何問題要釐清嗎?