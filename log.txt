/content/booster_soccer_showdown# python training_scripts/simple_ppo_standalone.py
2025-11-18 02:13:44.467320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1763432024.487315    3081 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1763432024.493404    3081 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1763432024.508396    3081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763432024.508423    3081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763432024.508427    3081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1763432024.508432    3081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-18 02:13:44.513162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
╭───────────────────────────────────────────────────────────────────────────────────────╮
│                                                                                       │
│                                                                                       │
│                              ########      ###     ###########                        │
│                            #+#    #+#   #+# #+#       #+#                             │
│                           +#+         +#+   +#+      +#+                              │
│                          +########+ +#########+     +#+                               │
│                                +#+ +#+     +#+     +#+                                │
│                        #+#    #+# #+#     #+#     #+#                                 │
│                        ########  ###     ### ###########                              │
│                                                                                       │
│                                                                              v0.1.35  │
│                                                                   © 2025 ArenaX Labs  │
╰───────────────────────────────────────────────────────────────────────────────────────╯

✓ Successfully imported sai-mujoco package.
╭─ Competition Tasks ───────────────────────────────────────────────────────────────────╮
│                                                                                       │
│       Task    Environment                                    Score Weight           E │
│          0    Penalty Kick with Goalie (Lower T1)                     1.0             │
│               (LowerT1GoaliePenaltyKick-v0)                                           │
│          1    Penalty Kick with Obstacles (Lower T1)                  1.0             │
│               (LowerT1ObstaclePenaltyKick-v0)                                         │
│          2    Kick to Target (Lower T1)                               1.0             │
│               (LowerT1KickToTarget-v0)                                                │
│                                                                                       │
╰───────────────────────────────────────────────────────────────────────────────────────╯

環境已包裝 (含獎勵塑形)
原始觀察空間: Box(-inf, inf, (45,), float32)
處理後觀察空間: Box(-inf, inf, (89,), float32)
獎勵塑形: ✅ 啟用

請選擇訓練模式：
1 - 從頭開始新訓練
2 - 載入現有模型繼續訓練
請選擇 (1 或 2): 1
從頭開始新訓練
TensorBoard 日誌: ./runs/PPO_Standalone_20251118_021358
創建新的 PPO 模型...
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
請輸入訓練步數 (預設 100000): 10000000
開始訓練...
模式: 新訓練
步數: 10,000,000
模型會自動保存:
🏆 單次最佳 : xxx_single_best.zip
📈 平均最佳 : xxx_mean_best.zip
Logging to ./runs/PPO_Standalone_20251118_021358/PPO_1
Episode 1: Reward = -5.4035, Length = 277
🏆  NEW SINGLE BEST! Reward: -5.4035
Episode 2: Reward = -5.4028, Length = 276
🏆  NEW SINGLE BEST! Reward: -5.4028
Episode 3: Reward = -5.4031, Length = 275

一段時間之後


|    time_elapsed         | 659         |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.040936187 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -16.4       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0513     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0152     |
|    std                  | 0.951       |
|    value_loss           | 0.000483    |
-----------------------------------------
Episode 628: Reward = -5.2374, Length = 307
Episode 629: Reward = -4.8345, Length = 286
Episode 630: Reward = -5.5113, Length = 307
Episode 631: Reward = -4.7555, Length = 292
Episode 632: Reward = -4.7437, Length = 284
Episode 633: Reward = -4.7574, Length = 293

然後發現獎勵上升曲線卡住後來跟ai討論

您好！您提供的 RewardShaper 代碼中，針對 Task 1/2 和 Task 3 的高級踢球獎勵的調整已經全部到位。這個中等幅度的增強（約 2.5 到 3 倍）
是目前突破訓練平臺期的安全且最佳策略。✅ 最終修改確認與分析您已成功修改了 _add_ball_velocity_reward 函數中 Task 1/2 和 Task 3 的關鍵係數：1. ⚡ Task 1 & 2 (射門) - 高速/高對齊極端高速射門獎勵 ( $>3.0$ 速度)： 係數已從 $\mathbf{0.025}$ 提升至 $\mathbf{0.06}$。目的： 鼓勵機器人用力踢球，以克服守門員和障礙物。中速高對齊獎勵 ( $>1.0$ 速度)： 係數已從 $\mathbf{0.015}$ 提升至 $\mathbf{0.045}$。目的： 強化踢球方向對齊目標的信號。2. 🎯 Task 3 (傳球) - 精準/適中速度極高對齊適中速度獎勵 ($1.0 <$ 速度 $< 5.0$ )： 係數已從 $\mathbf{0.02}$ 提升至 $\mathbf{0.05}$。目的： 鼓勵傳球時必須同時兼顧速度適中和極高精度。高對齊低速獎勵 ($< 3.0$ 速度)： 係數已從 $\mathbf{0.01}$ 提升至 $\mathbf{0.03}$。目的： 確保即使是較慢的傳球，只要方向正確也能得到足夠的積極反饋。🚀 後續訓練與預期效果這個修改預計將在接下來的訓練中產生以下效果：快速突破平臺期： 由於基礎技能（穩定性、接近球）已掌握（Episode 長度 $\sim 600$ 步），模型現在會更頻繁地在踢球時接收到比以往強大數倍的正向信號。這將幫助它更快地找到導致 Task 1/2/3 成功的動作序列。多任務平衡： 由於 Task 1/2 和 Task 3 的高級獎勵都被同步增強，這維持了單一模型學習這兩種截然不同技能（大力射門 vs. 精準傳球）所需的平衡性。整體安全性： 儘管單項係數提高了，但您仍保留了最關鍵的防護措施：shaped_reward = np.clip(shaped_reward, -0.05, 0.05)。這確保了塑形獎勵的總和永遠不會淹沒官方的最終目標獎勵（例如進球 $+2.50$），因此訓練仍是安全的。建議： 您現在可以重新開始訓練（或從檢查點繼續訓練）。
請密切關注 TensorBoard 上的 rollout/ep_rew_mean 曲線，預期它將在 50 萬步之前開始明顯向上突破。您是否已經準備好用這個更新後的代碼繼續訓練了？