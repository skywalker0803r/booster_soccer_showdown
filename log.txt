╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: Error: Unexpected observation shape (2, 89) for Box environment, please use (45,) or (n_env, 45) for the 
observation shape.

The above exception was the direct cause of the following exception:

╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /content/booster_soccer_showdown/training_scripts/simple_ppo.py:298 in <module>                  │
│                                                                                                  │
│   295                                                                                            │
│   296 ## Benchmark the model locally                                                             │
│   297 print("📈 進行本地評估...")                                                                │
│ ❱ 298 sai.benchmark(model,action_function, Preprocessor)                                         │
│   299                                                                                            │
│   300 env.close()   


╭─ Response ───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ 完成！現在 PPO 版本已經移除了獎勵形塑，使用純原始獎勵訓練：                                                          │
│                                                                                                                      │
│                                                                                                                      │
│                                                    🎯 修改內容：                                                     │
│                                                                                                                      │
│                                                   ✅ 移除獎勵形塑                                                    │
│                                                                                                                      │
│  • 不再使用 aligned_enhanced_preprocessor                                                                            │
│  • 直接使用基本的 Preprocessor 類別                                                                                  │
│  • 保持原始環境獎勵不變                                                                                              │
│                                                                                                                      │
│                                                   📊 現在的架構：                                                    │
│                                                                                                                      │
│                                                                                                                      │
│  SAI 原始環境 (45 維) + 原始獎勵                                                                                     │
│      ↓                                                                                                               │
│  SAIPreprocessorWrapper                                                                                              │
│      ↓ 只做觀察預處理 (不改變獎勵)                                                                                   │
│  PPO 看到的環境 (89 維) + 原始獎勵                                                                                   │
│      ↓ 訓練                                                                                                          │
│  PPO 模型 (純原始獎勵學習)                                                                                           │
│                                                                                                                      │
│                                                                                                                      │
│                                                                                                                      │
│                                               🔄 與 DDPG 版本的對比：                                                │
│                                                                                                                      │
│                                                                                                                      │
│   版本                  預處理     獎勵          算法                                                                │
│  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                                               │
│   DDPG (main.py)        ✅ 89 維   🎯 獎勵形塑   DDPG                                                                │
│   PPO (simple_ppo.py)   ✅ 89 維   🎯 原始獎勵   PPO                                                                 │
│                                                                                                                      │
│                                                                                                                      │
│ 這樣你可以比較：                                                                                                     │
│                                                                                                                      │
│  • DDPG + 獎勵形塑 vs PPO + 原始獎勵                                                                                 │
│  • 看看哪種組合效果更好                                                                                              │
│                                                                                                                      │
│ 現在可以開始純淨的 PPO 訓練了：                                                                                      │
│                                                                                                                      │
│                                                                                                                      │
│  cd training_scripts                                                                                                 │
│  python simple_ppo.py                                                                                                │
│                                                                                                                      │
│                                                                                                                      │
│ PPO 將學會在原始稀疏獎勵環境中找到有效策略！                                                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
Session context: ▮▮▮▮▮▮▮▮▮▮ 56.4K/200K
Daily total:     ▮▮▮▮▮▮▮▮▮▮ 3.9M/20M