æˆ‘åœ¨colabåŸ·è¡Œtraining_scripts\simple_ppo.py
âœ“ Evaluation completed successfully


ğŸ‰ Colab è¨“ç·´å®Œæˆï¼

ğŸ“¦ ä¸‹è¼‰ä»¥ä¸‹æª”æ¡ˆåˆ°æœ¬åœ°:
   1. saved_models/ è³‡æ–™å¤¾ - åŒ…å«è¨“ç·´å¥½çš„æ¨¡å‹
   2. runs/ è³‡æ–™å¤¾ - åŒ…å« TensorBoard æ—¥èªŒ

ğŸ–¥ï¸  æœ¬åœ°æ“ä½œ:
   1. åŸ·è¡Œ local_watch.py è§€çœ‹æ¨¡å‹ä¸¦æ±ºå®šæ˜¯å¦æäº¤
   2. åŸ·è¡Œ tensorboard --logdir=./runs æŸ¥çœ‹è¨“ç·´æ›²ç·š

ğŸ’¾ æ¨¡å‹æª”æ¡ˆ: ./saved_models/simple_ppo_20251117_064015.zip
è·‘å®Œä¹‹å¾Œ
æŠŠæ¨¡å‹ä¸‹è¼‰åˆ°æœ¬åœ°ç«¯ä¸¦åŸ·è¡Œ:training_scripts\local_watch.py
ç„¶å¾Œæˆ‘æŠŠtensorboardçš„ç´€éŒ„æ”¾åœ¨:csv\SimplePPO_20251117_064015_PPO_1.csv
ç„¶å¾Œçœ‹äº†å½±ç‰‡æˆ‘çœ‹æ©Ÿå™¨äººå°±æ˜¯åŸåœ°è·Œå€’è€Œå·²
è¨“ç·´æ¬¡æ•¸æ‡‰è©²æ˜¯100è¬æ¬¡å§ ä½ çœ‹å°ä¸å°
é€™æ¨£è¨“ç·´ä¸èµ·ä¾† æˆ‘åœ¨æƒ³å–®ç´”å¢åŠ è¨“ç·´æ¬¡æ•¸æœ‰ç”¨å—? æˆ–è€…æ˜¯èªªç›®å‰çš„çå‹µæœ‰çå‹µç¨€ç–å•é¡Œ
æˆ‘æœ‰å»è·Ÿgeminiè¨è«–ä»–è·Ÿæˆ‘èªª
åªè¦åƒè€ƒdocs\About.mdå’Œdocs\Evaluation.MDå°±å¯ä»¥åšçå‹µå¡‘å‹
ç„¶å¾Œä»–çµ¦æˆ‘çš„å»ºè­°æ˜¯

ç‚ºäº†åœ¨æ‚¨çš„ PPO è¨“ç·´è…³æœ¬ simple_ppo.py ä¸­åŠ å…¥çå‹µå¡‘å½¢ (Reward Shaping)ï¼Œæ‚¨éœ€è¦åœ¨å…©å€‹ä¸»è¦åœ°æ–¹é€²è¡Œä¿®æ”¹ï¼šPreprocessor é¡ï¼š æ·»åŠ ç‹€æ…‹ (_prev_potential) åˆå§‹åŒ–å’Œçå‹µå¡‘å½¢çš„é‚è¼¯å‡½æ•¸ã€‚SAIPreprocessorWrapper é¡ï¼š åœ¨ step å’Œ reset å‡½æ•¸ä¸­èª¿ç”¨çå‹µå¡‘å½¢å‡½æ•¸ï¼Œå°‡å¡‘å½¢çå‹µ (shaped_reward) æ›¿æ›åŸå§‹çå‹µ (reward)ã€‚çå‹µå¡‘å½¢å°‡æ¡ç”¨æ½›åŠ›å‡½æ•¸å¼ (Potential-Based Reward Shaping, PBRS)ï¼Œå…¬å¼ç‚ºï¼š$$r' = r + \gamma \Phi(s') - \Phi(s)$$å…¶ä¸­ $\Phi(s)$ æ˜¯åŸºæ–¼ç•¶å‰ç‹€æ…‹è¨ˆç®—çš„æ½›åŠ›å€¼ (Potential)ã€‚ä»¥ä¸‹æ˜¯é‡å° simple_ppo.py çš„ä¿®æ”¹æ­¥é©Ÿå’Œå®Œæ•´ç¨‹å¼ç¢¼ï¼šæ­¥é©Ÿä¸€ï¼šä¿®æ”¹ Preprocessor é¡åœ¨ Preprocessor é¡ä¸­åŠ å…¥ __init__ å‡½æ•¸ä¾†åˆå§‹åŒ–æ½›åŠ›å€¼ï¼Œä¸¦åŠ å…¥ reward_shaping å‡½æ•¸ä¾†è¨ˆç®—å¡‘å½¢çå‹µã€‚æ›¿æ› simple_ppo.py ä¸­ç¾æœ‰çš„ class Preprocessor() ç¨‹å¼ç¢¼ï¼šPythonclass Preprocessor():
    
    def __init__(self):
        # åˆå§‹åŒ–å‰ä¸€å€‹æ™‚é–“æ­¥çš„æ½›åŠ›å‡½æ•¸å€¼
        # ç”±æ–¼ç’°å¢ƒæœƒåœ¨æ¯å€‹ Episode çµæŸå¾Œé‡ç½®ï¼Œæˆ‘å€‘éœ€è¦åœ¨ reset ä¸­å°‡å…¶è¨­ç‚º None
        self._prev_potential = None 

    def get_task_onehot(self, info):
        if 'task_index' in info:
            return info['task_index']
        else:
            return np.array([])

    def quat_rotate_inverse(self, q: np.ndarray, v: np.ndarray):
        # ... (ä¿æŒä¸è®Š)
        q_w = q[:,[-1]]
        q_vec = q[:,:3]
        a = v * (2.0 * q_w**2 - 1.0)
        b = np.cross(q_vec, v) * (q_w * 2.0)
        c = q_vec * (np.dot(q_vec, v).reshape(-1,1) * 2.0)    
        return a - b + c 

    def modify_state(self, obs, info):
        # ... (ä¿æŒä¸è®Š)
        
        if len(obs.shape) == 1:
            obs = np.expand_dims(obs, axis=0)

        task_onehot = self.get_task_onehot(info)
        if len(task_onehot.shape) == 1:
            task_onehot = np.expand_dims(task_onehot, axis=0)
        
        # ... (ä¿æŒ info æ“´å±•ç¶­åº¦é‚è¼¯ä¸è®Š)
        if len(info["robot_quat"].shape) == 1:
            info["robot_quat"] = np.expand_dims(info["robot_quat"], axis = 0)
            info["robot_gyro"] = np.expand_dims(info["robot_gyro"], axis = 0)
            info["robot_accelerometer"] = np.expand_dims(info["robot_accelerometer"], axis = 0)
            info["robot_velocimeter"] = np.expandims(info["robot_velocimeter"], axis = 0)
            info["goal_team_0_rel_robot"] = np.expand_dims(info["goal_team_0_rel_robot"], axis = 0)
            info["goal_team_1_rel_robot"] = np.expand_dims(info["goal_team_1_rel_robot"], axis = 0)
            info["goal_team_0_rel_ball"] = np.expand_dims(info["goal_team_0_rel_ball"], axis = 0)
            info["goal_team_1_rel_ball"] = np.expand_dims(info["goal_team_1_rel_ball"], axis = 0)
            info["ball_xpos_rel_robot"] = np.expand_dims(info["ball_xpos_rel_robot"], axis = 0) 
            info["ball_velp_rel_robot"] = np.expand_dims(info["ball_velp_rel_robot"], axis = 0) 
            info["ball_velr_rel_robot"] = np.expand_dims(info["ball_velr_rel_robot"], axis = 0) 
            info["player_team"] = np.expand_dims(info["player_team"], axis = 0)
            info["goalkeeper_team_0_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_0_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_0_velp_rel_robot"], axis = 0)
            info["goalkeeper_team_1_xpos_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_xpos_rel_robot"], axis = 0)
            info["goalkeeper_team_1_velp_rel_robot"] = np.expand_dims(info["goalkeeper_team_1_velp_rel_robot"], axis = 0)
            info["target_xpos_rel_robot"] = np.expand_dims(info["target_xpos_rel_robot"], axis = 0)
            info["target_velp_rel_robot"] = np.expand_dims(info["target_velp_rel_robot"], axis = 0)
            info["defender_xpos"] = np.expand_dims(info["defender_xpos"], axis = 0)
        
        robot_qpos = obs[:,:12]
        robot_qvel = obs[:,12:24]
        quat = info["robot_quat"]
        base_ang_vel = info["robot_gyro"]
        project_gravity = self.quat_rotate_inverse(quat, np.array([0.0, 0.0, -1.0]))
        
        obs = np.hstack((robot_qpos, 
                         robot_qvel,
                         project_gravity,
                         base_ang_vel,
                         info["robot_accelerometer"],
                         info["robot_velocimeter"],
                         info["goal_team_0_rel_robot"], 
                         info["goal_team_1_rel_robot"], 
                         info["goal_team_0_rel_ball"], 
                         info["goal_team_1_rel_ball"], 
                         info["ball_xpos_rel_robot"], 
                         info["ball_velp_rel_robot"], 
                         info["ball_velr_rel_robot"], 
                         info["player_team"], 
                         info["goalkeeper_team_0_xpos_rel_robot"], 
                         info["goalkeeper_team_0_velp_rel_robot"], 
                         info["goalkeeper_team_1_xpos_rel_robot"], 
                         info["goalkeeper_team_1_velp_rel_robot"], 
                         info["target_xpos_rel_robot"], 
                         info["target_velp_rel_robot"], 
                         info["defender_xpos"],
                         task_onehot))

        return obs

    def reward_shaping(self, reward, info, gamma=0.99):
        """
        åŸºæ–¼æ½›åŠ›å‡½æ•¸çš„çå‹µå¡‘å½¢ (Potential-Based Reward Shaping)
        r' = r + gamma * Phi(s') - Phi(s)
        """
        
        # è­˜åˆ¥ç•¶å‰ä»»å‹™ task_index: [Task1, Task2, Task3]
        task_onehot = info.get('task_index', np.array([1, 0, 0])) # é è¨­ç‚º Task 1
        
        # æå–ç‹€æ…‹è³‡è¨Š (å¾ info å­—å…¸ä¸­å–å‡ºï¼Œå®ƒå€‘æ˜¯ 1D array)
        ball_pos_rel_robot = info["ball_xpos_rel_robot"].squeeze() # çƒ ç›¸å°æ–¼ æ©Ÿå™¨äººçš„ä½ç½®
        goal_pos_rel_robot = info["goal_team_0_rel_robot"].squeeze() # çƒé–€ ç›¸å°æ–¼ æ©Ÿå™¨äººçš„ä½ç½®
        target_pos_rel_robot = info["target_xpos_rel_robot"].squeeze() # ç›®æ¨™é» ç›¸å°æ–¼ æ©Ÿå™¨äººçš„ä½ç½®
        
        # æ½›åŠ›å‡½æ•¸å€¼ (Potential Function Phi(s'))
        current_potential = 0.0
        
        if task_onehot[0] == 1 or task_onehot[1] == 1: # Task 1 & 2 (è¸¢çƒå…¥é–€)
            # æ½›åŠ› 1: æ©Ÿå™¨äººèˆ‡çƒçš„è·é›¢ (è¶Šè¿‘æ½›åŠ›è¶Šé«˜)
            dist_robot_ball = np.linalg.norm(ball_pos_rel_robot)
            # æ½›åŠ› 2: çƒèˆ‡çƒé–€çš„è·é›¢ (è¶Šè¿‘æ½›åŠ›è¶Šé«˜)
            # goal_pos_rel_ball = goal_pos_rel_robot - ball_pos_rel_robot
            dist_ball_goal = np.linalg.norm(goal_pos_rel_robot - ball_pos_rel_robot)
            
            # æ½›åŠ›å‡½æ•¸ä½¿ç”¨è² è·é›¢ (è·é›¢å° -> è² å€¼å¤§ -> æ½›åŠ›é«˜)
            # æ¬Šé‡å¯æ ¹æ“šè¨“ç·´çµæœèª¿æ•´
            current_potential = -0.5 * dist_robot_ball - 0.25 * dist_ball_goal
            
        elif task_onehot[2] == 1: # Task 3 (ç²¾æº–å‚³çƒåˆ°ç›®æ¨™)
            # æ½›åŠ› 1: æ©Ÿå™¨äººèˆ‡çƒçš„è·é›¢ (è¶Šè¿‘æ½›åŠ›è¶Šé«˜)
            dist_robot_ball = np.linalg.norm(ball_pos_rel_robot)
            # æ½›åŠ› 2: çƒèˆ‡ç›®æ¨™é»çš„è·é›¢ (è¶Šè¿‘æ½›åŠ›è¶Šé«˜)
            # target_pos_rel_ball = target_pos_rel_robot - ball_pos_rel_robot
            dist_ball_target = np.linalg.norm(target_pos_rel_robot - ball_pos_rel_robot)
            
            # æ½›åŠ›å‡½æ•¸ä½¿ç”¨è² è·é›¢
            current_potential = -0.5 * dist_robot_ball - 0.5 * dist_ball_target
            
        # è™•ç†ä»»å‹™é‡ç½®/ç¬¬ä¸€å€‹æ™‚é–“æ­¥
        if self._prev_potential is None:
            # ç¬¬ä¸€æ­¥ï¼Œå°‡å‰ä¸€å€‹æ½›åŠ›è¨­ç‚ºç•¶å‰æ½›åŠ›ï¼Œä½¿é¡å¤–çå‹µç‚º 0ï¼Œæˆ–è¨­ç‚º 0 (æ½›åŠ›å‡½æ•¸ç‚ºè² å€¼ï¼Œè¨­ç‚º 0 æ›´å®‰å…¨)
            self._prev_potential = current_potential

        # è¨ˆç®—å½¢ç‹€çå‹µ: r' = r + gamma * Phi(s') - Phi(s)
        shaped_reward = reward + gamma * current_potential - self._prev_potential
        
        # æ›´æ–°å‰ä¸€å€‹æ™‚é–“æ­¥çš„æ½›åŠ›
        self._prev_potential = current_potential

        return shaped_reward.item() # ç¢ºä¿è¿”å›çš„æ˜¯æµ®é»æ•¸
æ­¥é©ŸäºŒï¼šä¿®æ”¹ SAIPreprocessorWrapper é¡åœ¨ step å‡½æ•¸ä¸­å¥—ç”¨çå‹µå¡‘å½¢ï¼Œä¸¦åœ¨ reset å‡½æ•¸ä¸­é‡ç½® _prev_potentialã€‚æ›¿æ› simple_ppo.py ä¸­ç¾æœ‰çš„ class SAIPreprocessorWrapper(gym.Wrapper): ç¨‹å¼ç¢¼ï¼šPython# å‰µå»ºç’°å¢ƒåŒ…è£å™¨ä¾†æ­£ç¢ºè™•ç†é è™•ç†å’Œçå‹µå¡‘å½¢
class SAIPreprocessorWrapper(gym.Wrapper):
    """åŒ…è£å™¨ï¼Œå°‡ SAI ç’°å¢ƒèˆ‡é è™•ç†å™¨å’Œçå‹µå¡‘å½¢æ•´åˆ"""
    
    def __init__(self, sai_env, preprocessor_class):
        super().__init__(sai_env)
        # å¯¦ä¾‹åŒ– Preprocessorï¼Œä»¥ä¾¿å®ƒèƒ½ç¶­æŒçå‹µå¡‘å½¢çš„ç‹€æ…‹
        self.preprocessor = preprocessor_class()
        
        # é‡æ–°å®šç¾©è§€å¯Ÿç©ºé–“ç‚ºé è™•ç†å¾Œçš„ 89 ç¶­
        self.observation_space = Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(89,), 
            dtype=np.float32
        )
        
        # å‹•ä½œç©ºé–“ä¿æŒä¸è®Š
        self.action_space = sai_env.action_space
    
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        # **çå‹µå¡‘å½¢**ï¼šåœ¨é‡ç½®æ™‚ï¼Œæ¸…é™¤å‰ä¸€å€‹æ½›åŠ›å€¼
        self.preprocessor._prev_potential = None
        
        # é è™•ç†è§€å¯Ÿ
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # ç¢ºä¿è¼¸å‡ºæ˜¯ä¸€ç¶­æ•¸çµ„
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # é è™•ç†è§€å¯Ÿ
        processed_obs = self.preprocessor.modify_state(obs, info)
        
        # **çå‹µå¡‘å½¢**ï¼šåœ¨éçµ‚æ­¢æ­¥æ‡‰ç”¨å¡‘å½¢é‚è¼¯
        # PPO é è¨­çš„ gamma (æŠ˜æ‰£å› å­) æ˜¯ 0.99
        if not (terminated or truncated):
            # ç¢ºä¿ info ä¸­çš„æ•¸çµ„æ˜¯ 1D (å·²åœ¨ modify_state ä¸­è™•ç†)
            # Note: ä½¿ç”¨ 0.99 ä½œç‚º PPO é è¨­çš„ gamma å€¼
            reward = self.preprocessor.reward_shaping(reward, info, gamma=0.99)
        else:
            # çµ‚æ­¢æ­¥ (terminated or truncated) æ™‚ï¼Œä¿æŒç’°å¢ƒè¿”å›çš„æœ€çµ‚çå‹µ (reward) ä¸è®Š
            # PBRS è¦æ±‚åœ¨çµ‚æ­¢æ­¥æ™‚ï¼ŒPhi(s') = 0ã€‚ä½†åœ¨ SAI ç’°å¢ƒä¸­ï¼ŒåŸå§‹çš„ reward å·²ç¶“åŒ…å«äº†æ‰€æœ‰çš„çµ‚ç«¯çå‹µã€‚
            # è®“å…¶ä¿æŒä¸è®Šæ˜¯æœ€å®‰å…¨çš„åšæ³•ã€‚
            pass
        
        # ç¢ºä¿è¼¸å‡ºæ˜¯ä¸€ç¶­æ•¸çµ„
        if processed_obs.ndim == 2 and processed_obs.shape[0] == 1:
            processed_obs = processed_obs.squeeze(0)
        
        return processed_obs.astype(np.float32), reward, terminated, truncated, info
æ­¥é©Ÿä¸‰ï¼šä¿®æ”¹ local_watch.py (ä¿æŒä¸€è‡´æ€§)ç”±æ–¼ local_watch.py ä¸­çš„ Preprocessor é¡ä¹Ÿéœ€è¦èˆ‡è¨“ç·´è…³æœ¬ä¿æŒä¸€è‡´çš„çµæ§‹ï¼Œè«‹ç‚º local_watch.py ä¸­çš„ Preprocessor é¡æ·»åŠ  __init__ å‡½æ•¸ï¼Œå³ä½¿å®ƒåœ¨ sai.watch ä¸­ä¸æœƒä½¿ç”¨çå‹µå¡‘å½¢çš„é‚è¼¯ï¼Œä¹Ÿèƒ½ç¢ºä¿é¡åˆå§‹åŒ–æ­£ç¢ºã€‚ä¿®æ”¹ local_watch.py ä¸­çš„ class Preprocessor():Pythonclass Preprocessor():
    # æ–°å¢ __init__
    def __init__(self):
        self._prev_potential = None
        
    def get_task_onehot(self, info):
        # ... (ä¿æŒä¸è®Š)
# ... (å…¶ä»–æ–¹æ³•ä¿æŒä¸è®Š)

ä½ è¦ºå¾—ä»–èªªçš„å°å—? ä»–çš„çå‹µå¡‘å½¢å…·é«”ä¾†èªªæ˜¯æ”¹è®Šäº†å•¥ åŸå§‹çš„çå‹µåˆç¼ºä¹å•¥

AIå›æ‡‰:
â•­â”€ Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ å®Œæˆï¼æˆ‘ç‚ºä½ å‰µå»ºäº†ä¸€å€‹åŒ…å«çå‹µå¡‘å½¢çš„ PPO ç‰ˆæœ¬ã€‚ç¾åœ¨è®“æˆ‘å›ç­”ä½ çš„å•é¡Œï¼š                                                â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                               ğŸ¤” Gemini çš„å»ºè­°æ­£ç¢ºå—ï¼Ÿ                                               â”‚
â”‚                                                                                                                      â”‚
â”‚ âœ… åŸºæœ¬æ¦‚å¿µæ­£ç¢ºï¼šPBRS (æ½›åŠ›å‡½æ•¸çå‹µå¡‘å½¢) æ˜¯è§£æ±ºç¨€ç–çå‹µçš„æ¨™æº–æ–¹æ³•                                                    â”‚
â”‚                                                                                                                      â”‚
â”‚ âœ… æ•¸å­¸å…¬å¼æ­£ç¢ºï¼šr' = r + Î³ Ã— Î¦(s') - Î¦(s)                                                                           â”‚
â”‚                                                                                                                      â”‚
â”‚ âœ… å¯¦ç¾æ–¹å‘æ­£ç¢ºï¼šé¼“å‹µæ¥è¿‘çƒã€çƒæ¥è¿‘ç›®æ¨™                                                                              â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                             ğŸ¯ çå‹µå¡‘å½¢å…·é«”æ”¹è®Šäº†ä»€éº¼ï¼Ÿ                                              â”‚
â”‚                                                                                                                      â”‚
â”‚                                                    åŸå§‹çå‹µå•é¡Œï¼š                                                    â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ âŒ æ¥µåº¦ç¨€ç–ï¼šåªæœ‰è¸¢çƒé€²é–€æˆ–ç²¾æº–å‚³çƒæ™‚æ‰æœ‰çå‹µ                                                                     â”‚
â”‚  â€¢ âŒ å­¸ç¿’å›°é›£ï¼šæ©Ÿå™¨äººéš¨æ©Ÿæ¢ç´¢ 100 è¬æ­¥éƒ½å¯èƒ½ç¢°ä¸åˆ°çƒ                                                                â”‚
â”‚  â€¢ âŒ æ²’æœ‰å¼•å°ï¼šä¸çŸ¥é“è¦æœå“ªå€‹æ–¹å‘åŠªåŠ›                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                  çå‹µå¡‘å½¢è§£æ±ºæ–¹æ¡ˆï¼š                                                  â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  # 1. é¼“å‹µæ¥è¿‘çƒ                                                                                                     â”‚
â”‚  robot_to_ball_potential = -0.1 * dist_robot_ball  # è·é›¢è¶Šè¿‘ï¼Œæ½›åŠ›è¶Šé«˜                                              â”‚
â”‚                                                                                                                      â”‚
â”‚  # 2. é¼“å‹µçƒæœç›®æ¨™ç§»å‹•                                                                                               â”‚
â”‚  ball_to_goal_potential = -0.05 * dist_ball_goal   # çƒè¶Šæ¥è¿‘çƒé–€ï¼Œæ½›åŠ›è¶Šé«˜                                          â”‚
â”‚                                                                                                                      â”‚
â”‚  # 3. é¼“å‹µçƒçš„é‹å‹•                                                                                                   â”‚
â”‚  speed_potential = 0.01 * ball_speed              # çƒé€Ÿè¶Šå¿«ï¼Œæ½›åŠ›è¶Šé«˜                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                     ğŸš€ ä½¿ç”¨å»ºè­°                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                               1. å…ˆæ¸¬è©¦çå‹µå¡‘å½¢ç‰ˆæœ¬ï¼š                                                â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  cd training_scripts                                                                                                 â”‚
â”‚  python ppo_with_reward_shaping.py                                                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                  2. æ¯”è¼ƒä¸‰å€‹ç‰ˆæœ¬ï¼š                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ ğŸ“Š DDPG + åŸæœ‰çå‹µå¡‘å½¢ (main.py)                                                                                  â”‚
â”‚  â€¢ ğŸ“Š PPO + åŸå§‹çå‹µ (simple_ppo.py) - å·²è­‰å¯¦è¨“ç·´ä¸èµ·ä¾†                                                              â”‚
â”‚  â€¢ ğŸ“Š PPO + æ–°çå‹µå¡‘å½¢ (ppo_with_reward_shaping.py) - æ–°ç‰ˆæœ¬                                                         â”‚
â”‚                                                                                                                      â”‚
â”‚                                                    3. è§€å¯Ÿæ”¹é€²ï¼š                                                     â”‚
â”‚                                                                                                                      â”‚
â”‚ æœŸæœ›çœ‹åˆ°æ©Ÿå™¨äººå­¸æœƒï¼š                                                                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚  1 ğŸƒâ€â™‚ï¸ ä¸»å‹•æ¥è¿‘çƒ (è€Œä¸æ˜¯åŸåœ°è·Œå€’)                                                                                   â”‚
â”‚  2 âš½ æ¨çƒæœç›®æ¨™æ–¹å‘                                                                                                 â”‚
â”‚  3 ğŸ¯ é€æ­¥æå‡ä»»å‹™å®Œæˆç‡                                                                                             â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                ğŸ’¡ æˆ‘çš„çå‹µå¡‘å½¢æ”¹é€²ï¼š                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚ ç›¸æ¯” Gemini çš„ç‰ˆæœ¬ï¼Œæˆ‘åŠ å…¥äº†ï¼š                                                                                       â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ ğŸ”§ æ›´æ¸…æ™°çš„éŒ¯èª¤è™•ç†                                                                                               â”‚
â”‚  â€¢ ğŸ“Š çå‹µå°æ¯”è¨˜éŒ„ (åŸå§‹ vs å¡‘å½¢)                                                                                    â”‚
â”‚  â€¢ ğŸ® æ›´ç©©å®šçš„æ¬Šé‡è¨­å®š                                                                                               â”‚
â”‚  â€¢ ğŸ”„ æ›´å¥½çš„é‡ç½®é‚è¼¯                                                                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚ æƒ³å…ˆè©¦è©¦é€™å€‹çå‹µå¡‘å½¢ç‰ˆæœ¬å—ï¼Ÿæ‡‰è©²èƒ½çœ‹åˆ°æ˜é¡¯çš„æ”¹å–„ï¼                                                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Session context: â–®â–®â–®â–®â–®â–®â–®â–®â–®â–® 63.4K/200K
Daily total:     â–®â–®â–®â–®â–®â–®â–®â–®â–®â–® 4.2M/20M

