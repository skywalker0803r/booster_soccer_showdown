1.在雲端跑training_scripts\simple_ppo_standalone.py把模型訓練好了
然後把saved_models\ppo_standalone_20251117_105737.zip和runs\PPO_Standalone_20251117_105737
下載到本地端電腦

2.在本地電腦執行training_scripts\local_watch_english.py之後觀察表現後提交到排行榜

然後我有觀察到雲端GPU訓練過程中
1.reward從-5慢慢上升到-2
2.3episode_length從300慢慢上升到600

在本地端觀察模型表現發現他會站得比較久但是還是不會踢球
上傳到雲端之後分數-1.76