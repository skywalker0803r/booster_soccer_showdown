Final model saved to: ./saved_models/ppo_pbrs_20251118_032230_final.zip
╭─────────────── Traceback (most recent call last) ───────────────╮
│ /content/booster_soccer_showdown/training_scripts/ppo_with_pbrs │
│ .py:229 in <module>                                             │
│                                                                 │
│   226 │   callback = SaveBestModelCallback(save_path=save_path, │
│   227 │                                                         │
│   228 │   try:                                                  │
│ ❱ 229 │   │   model.learn(total_timesteps=args.total_timesteps, │
│   230 │   except KeyboardInterrupt:                             │
│   231 │   │   print("\nTraining interrupted by user.")          │
│   232 │   finally:                                              │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │         argparse = <module 'argparse' from                  │ │
│ │                    '/usr/lib/python3.12/argparse.py'>       │ │
│ │             args = Namespace(total_timesteps=15000000,      │ │
│ │                    n_envs=4, lr=0.0003, n_steps=2048,       │ │
│ │                    batch_size=64, gamma=0.99,               │ │
│ │                    clip_range=0.2, k1=1.0, k2=1.0)          │ │
│ │         callback = <__main__.SaveBestModelCallback object   │ │
│ │                    at 0x79035b4d9970>                       │ │
│ │              env = <stable_baselines3.common.vec_env.dummy… │ │
│ │                    object at 0x790361482b70>                │ │
│ │ final_model_path = './saved_models/ppo_pbrs_20251118_03223… │ │
│ │              gym = <module 'gymnasium' from                 │ │
│ │                    '/usr/local/lib/python3.12/dist-package… │ │
│ │          log_dir = './runs/PPO_PBRS_20251118_032230'        │ │
│ │            model = <stable_baselines3.ppo.ppo.PPO object at │ │
│ │                    0x790361d8d970>                          │ │
│ │               np = <module 'numpy' from                     │ │
│ │                    '/usr/local/lib/python3.12/dist-package… │ │
│ │               os = <module 'os' (frozen)>                   │ │
│ │           parser = ArgumentParser(prog='ppo_with_pbrs.py',  │ │
│ │                    usage=None, description='Train a PPO     │ │
│ │                    agent with Potential-Based Reward        │ │
│ │                    Shaping.', formatter_class=<class        │ │
│ │                    'argparse.HelpFormatter'>,               │ │
│ │                    conflict_handler='error', add_help=True) │ │
│ │              sai = <sai_rl.sai_client.SAIClient object at   │ │
│ │                    0x7903612affb0>                          │ │
│ │        save_path = './saved_models'                         │ │
│ │      save_prefix = 'ppo_pbrs_20251118_032230'               │ │
│ │        timestamp = '20251118_032230'                        │ │
│ │            torch = <module 'torch' from                     │ │
│ │                    '/usr/local/lib/python3.12/dist-package… │ │
│ │   wrapper_kwargs = {'gamma': 0.99, 'k1': 1.0, 'k2': 1.0}    │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /usr/local/lib/python3.12/dist-packages/stable_baselines3/ppo/p │
│ po.py:311 in learn                                              │
│                                                                 │
│   308 │   │   reset_num_timesteps: bool = True,                 │
│   309 │   │   progress_bar: bool = False,                       │
│   310 │   ) -> SelfPPO:                                         │
│ ❱ 311 │   │   return super().learn(                             │
│   312 │   │   │   total_timesteps=total_timesteps,              │
│   313 │   │   │   callback=callback,                            │
│   314 │   │   │   log_interval=log_interval,                    │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │            callback = <__main__.SaveBestModelCallback       │ │
│ │                       object at 0x79035b4d9970>             │ │
│ │        log_interval = 1                                     │ │
│ │        progress_bar = False                                 │ │
│ │ reset_num_timesteps = True                                  │ │
│ │                self = <stable_baselines3.ppo.ppo.PPO object │ │
│ │                       at 0x790361d8d970>                    │ │
│ │         tb_log_name = 'PPO'                                 │ │
│ │     total_timesteps = 15000000                              │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /usr/local/lib/python3.12/dist-packages/stable_baselines3/commo │
│ n/on_policy_algorithm.py:311 in learn                           │
│                                                                 │
│   308 │   ) -> SelfOnPolicyAlgorithm:                           │
│   309 │   │   iteration = 0                                     │
│   310 │   │                                                     │
│ ❱ 311 │   │   total_timesteps, callback = self._setup_learn(    │
│   312 │   │   │   total_timesteps,                              │
│   313 │   │   │   callback,                                     │
│   314 │   │   │   reset_num_timesteps,                          │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │            callback = <__main__.SaveBestModelCallback       │ │
│ │                       object at 0x79035b4d9970>             │ │
│ │           iteration = 0                                     │ │
│ │        log_interval = 1                                     │ │
│ │        progress_bar = False                                 │ │
│ │ reset_num_timesteps = True                                  │ │
│ │                self = <stable_baselines3.ppo.ppo.PPO object │ │
│ │                       at 0x790361d8d970>                    │ │
│ │         tb_log_name = 'PPO'                                 │ │
│ │     total_timesteps = 15000000                              │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /usr/local/lib/python3.12/dist-packages/stable_baselines3/commo │
│ n/base_class.py:423 in _setup_learn                             │
│                                                                 │
│   420 │   │   # Avoid resetting the environment when calling `` │
│   421 │   │   if reset_num_timesteps or self._last_obs is None: │
│   422 │   │   │   assert self.env is not None                   │
│ ❱ 423 │   │   │   self._last_obs = self.env.reset()  # type: ig │
│   424 │   │   │   self._last_episode_starts = np.ones((self.env │
│   425 │   │   │   # Retrieve unnormalized observation for savin │
│   426 │   │   │   if self._vec_normalize_env is not None:       │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │            callback = <__main__.SaveBestModelCallback       │ │
│ │                       object at 0x79035b4d9970>             │ │
│ │        progress_bar = False                                 │ │
│ │ reset_num_timesteps = True                                  │ │
│ │                self = <stable_baselines3.ppo.ppo.PPO object │ │
│ │                       at 0x790361d8d970>                    │ │
│ │         tb_log_name = 'PPO'                                 │ │
│ │     total_timesteps = 15000000                              │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /usr/local/lib/python3.12/dist-packages/stable_baselines3/commo │
│ n/vec_env/dummy_vec_env.py:78 in reset                          │
│                                                                 │
│    75 │   def reset(self) -> VecEnvObs:                         │
│    76 │   │   for env_idx in range(self.num_envs):              │
│    77 │   │   │   maybe_options = {"options": self._options[env │
│ ❱  78 │   │   │   obs, self.reset_infos[env_idx] = self.envs[en │
│    79 │   │   │   self._save_obs(env_idx, obs)                  │
│    80 │   │   # Seeds and options are only used once            │
│    81 │   │   self._reset_seeds()                               │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │       env_idx = 0                                           │ │
│ │ maybe_options = {}                                          │ │
│ │          self = <stable_baselines3.common.vec_env.dummy_ve… │ │
│ │                 object at 0x790361482b70>                   │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /content/booster_soccer_showdown/training_scripts/ppo_with_pbrs │
│ .py:110 in reset                                                │
│                                                                 │
│   107 │   │   obs, info = self.env.reset(**kwargs)              │
│   108 │   │   self.previous_potential = self._calculate_potenti │
│   109 │   │                                                     │
│ ❱ 110 │   │   processed_obs = self.preprocessor.modify_state(ob │
│   111 │   │   return processed_obs.astype(np.float32), info     │
│   112 │                                                         │
│   113 │   def step(self, action):                               │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │   info = {                                                  │ │
│ │          │   'length': 10.97,                               │ │
│ │          │   'width': 6.87,                                 │ │
│ │          │   'goal_width': 1.6,                             │ │
│ │          │   'goal_height': 1.9,                            │ │
│ │          │   'goal_depth': 1.6,                             │ │
│ │          │   'goal_team_0_rel_robot': array([[-4.2,  0. ,   │ │
│ │          -0.7]]),                                           │ │
│ │          │   'goal_team_1_rel_robot': array([[17.74,  0.  , │ │
│ │          -0.7 ]]),                                          │ │
│ │          │   'goal_team_0_rel_ball': array([[-2.2,  0. ,    │ │
│ │          0. ]]),                                            │ │
│ │          │   'goal_team_1_rel_ball': array([[19.74,  0.  ,  │ │
│ │          0.  ]]),                                           │ │
│ │          │   'ball_xpos_rel_robot': array([[-2. ,  0. ,     │ │
│ │          -0.7]]),                                           │ │
│ │          │   ... +16                                        │ │
│ │          }                                                  │ │
│ │ kwargs = {'seed': 1969789416}                               │ │
│ │    obs = array([  0.  ,   0.  ,   0.  ,   0.  ,   0.  ,     │ │
│ │          0.  ,   0.  ,   0.  ,                              │ │
│ │          │   │    0.  ,   0.  ,   0.  ,   0.  ,   0.  ,     │ │
│ │          0.  ,   0.  ,   0.  ,                              │ │
│ │          │   │    0.  ,   0.  ,   0.  ,   0.  ,   0.  ,     │ │
│ │          0.  ,   0.  ,   0.  ,                              │ │
│ │          │   │    5.  ,   0.  ,  -0.7 ,   0.  ,   0.  ,     │ │
│ │          0.  ,   0.  ,   0.  ,                              │ │
│ │          │   │    0.  ,  -5.97,   0.  ,  -0.7 , -10.97,     │ │
│ │          0.  ,   0.  ,  -5.97,                              │ │
│ │          │   │    0.  ,  -0.5 ,   0.  ,   0.  ,   0.  ],    │ │
│ │          dtype=float32)                                     │ │
│ │   self = <PotentialBasedRewardWrapper<Monitor<CustomEvalua… │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /content/booster_soccer_showdown/training_scripts/ppo_with_pbrs │
│ .py:52 in modify_state                                          │
│                                                                 │
│    49 │   │   project_gravity = self.quat_rotate_inverse(quat,  │
│    50 │   │                                                     │
│    51 │   │   # Combine all state information                   │
│ ❱  52 │   │   processed_obs = np.hstack([                       │
│    53 │   │   │   robot_qpos, robot_qvel, project_gravity, base │
│    54 │   │   │   info["robot_accelerometer"], info["robot_velo │
│    55 │   │   │   info["goal_team_0_rel_robot"], info["goal_tea │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │    base_ang_vel = array([[0., 0., 0.]], dtype=float32)      │ │
│ │            info = {                                         │ │
│ │                   │   'length': 10.97,                      │ │
│ │                   │   'width': 6.87,                        │ │
│ │                   │   'goal_width': 1.6,                    │ │
│ │                   │   'goal_height': 1.9,                   │ │
│ │                   │   'goal_depth': 1.6,                    │ │
│ │                   │   'goal_team_0_rel_robot':              │ │
│ │                   array([[-4.2,  0. , -0.7]]),              │ │
│ │                   │   'goal_team_1_rel_robot':              │ │
│ │                   array([[17.74,  0.  , -0.7 ]]),           │ │
│ │                   │   'goal_team_0_rel_ball': array([[-2.2, │ │
│ │                   0. ,  0. ]]),                             │ │
│ │                   │   'goal_team_1_rel_ball':               │ │
│ │                   array([[19.74,  0.  ,  0.  ]]),           │ │
│ │                   │   'ball_xpos_rel_robot': array([[-2. ,  │ │
│ │                   0. , -0.7]]),                             │ │
│ │                   │   ... +16                               │ │
│ │                   }                                         │ │
│ │             key = 'task_index'                              │ │
│ │             obs = array([[  0.  ,   0.  ,   0.  ,   0.  ,   │ │
│ │                   0.  ,   0.  ,   0.  ,   0.  ,             │ │
│ │                   │   │     0.  ,   0.  ,   0.  ,   0.  ,   │ │
│ │                   0.  ,   0.  ,   0.  ,   0.  ,             │ │
│ │                   │   │     0.  ,   0.  ,   0.  ,   0.  ,   │ │
│ │                   0.  ,   0.  ,   0.  ,   0.  ,             │ │
│ │                   │   │     5.  ,   0.  ,  -0.7 ,   0.  ,   │ │
│ │                   0.  ,   0.  ,   0.  ,   0.  ,             │ │
│ │                   │   │     0.  ,  -5.97,   0.  ,  -0.7 ,   │ │
│ │                   -10.97,   0.  ,   0.  ,  -5.97,           │ │
│ │                   │   │     0.  ,  -0.5 ,   0.  ,   0.  ,   │ │
│ │                   0.  ]], dtype=float32)                    │ │
│ │ project_gravity = array([[ 0.        , -0.        ,         │ │
│ │                   -1.00000006]])                            │ │
│ │            quat = array([[0.000000e+00, 0.000000e+00,       │ │
│ │                   9.999997e-01, 7.963267e-04]],             │ │
│ │                   │     dtype=float32)                      │ │
│ │      robot_qpos = array([[0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │                   0., 0., 0., 0.]], dtype=float32)          │ │
│ │      robot_qvel = array([[0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │                   0., 0., 0., 0.]], dtype=float32)          │ │
│ │            self = <__main__.Preprocessor object at          │ │
│ │                   0x79036097c260>                           │ │
│ │     task_onehot = array([[1., 0., 0.]], dtype=float32)      │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
│                                                                 │
│ /usr/local/lib/python3.12/dist-packages/numpy/_core/shape_base. │
│ py:364 in hstack                                                │
│                                                                 │
│    361 │   if arrs and arrs[0].ndim == 1:                       │
│    362 │   │   return _nx.concatenate(arrs, 0, dtype=dtype, cas │
│    363 │   else:                                                │
│ ❱  364 │   │   return _nx.concatenate(arrs, 1, dtype=dtype, cas │
│    365                                                          │
│    366                                                          │
│    367 def _stack_dispatcher(arrays, axis=None, out=None, *,    │
│                                                                 │
│ ╭────────────────────────── locals ───────────────────────────╮ │
│ │    arrs = (                                                 │ │
│ │           │   array([[0., 0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │           0., 0., 0.]], dtype=float32),                     │ │
│ │           │   array([[0., 0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │           0., 0., 0.]], dtype=float32),                     │ │
│ │           │   array([[ 0.        , -0.        ,             │ │
│ │           -1.00000006]]),                                   │ │
│ │           │   array([[0., 0., 0.]], dtype=float32),         │ │
│ │           │   array([[-1.3136856e-16,  4.3332978e-17,       │ │
│ │           -2.0261973e-15]], dtype=float32),                 │ │
│ │           │   array([[0., 0., 0.]], dtype=float32),         │ │
│ │           │   array([[-4.2,  0. , -0.7]]),                  │ │
│ │           │   array([[17.74,  0.  , -0.7 ]]),               │ │
│ │           │   array([[-2.2,  0. ,  0. ]]),                  │ │
│ │           │   array([[19.74,  0.  ,  0.  ]]),               │ │
│ │           │   ... +12                                       │ │
│ │           )                                                 │ │
│ │ casting = 'same_kind'                                       │ │
│ │   dtype = None                                              │ │
│ │     tup = [                                                 │ │
│ │           │   array([[0., 0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │           0., 0., 0.]], dtype=float32),                     │ │
│ │           │   array([[0., 0., 0., 0., 0., 0., 0., 0., 0.,   │ │
│ │           0., 0., 0.]], dtype=float32),                     │ │
│ │           │   array([[ 0.        , -0.        ,             │ │
│ │           -1.00000006]]),                                   │ │
│ │           │   array([[0., 0., 0.]], dtype=float32),         │ │
│ │           │   array([[-1.3136856e-16,  4.3332978e-17,       │ │
│ │           -2.0261973e-15]], dtype=float32),                 │ │
│ │           │   array([[0., 0., 0.]], dtype=float32),         │ │
│ │           │   array([[-4.2,  0. , -0.7]]),                  │ │
│ │           │   array([[17.74,  0.  , -0.7 ]]),               │ │
│ │           │   array([[-2.2,  0. ,  0. ]]),                  │ │
│ │           │   array([[19.74,  0.  ,  0.  ]]),               │ │
│ │           │   ... +12                                       │ │
│ │           ]                                                 │ │
│ ╰─────────────────────────────────────────────────────────────╯ │
╰─────────────────────────────────────────────────────────────────╯
ValueError: all the input arrays must have same number of 
dimensions, but the array at index 0 has 2 dimension(s) and the 
array at index 13 has 1 dimension(s)
/content/booster_soccer_showdown#

好的，我已經分析了 log.txt 的內容。

  這是一個 ValueError，發生在 ppo_with_pbrs.py 腳本的 Preprocessor.modify_state 函數中。

  錯誤原因：
  在組合最終的觀測狀態時，numpy.hstack 函數要求所有被連接的陣列 (array)
  都必須有相同的維度。日誌顯示，列表中的第 0 個陣列（來自觀測 obs）是 2 維的，但第 13 個陣列（來自 info 字典的
  player_team）卻是 1 維的，導致維度不匹配而報錯。

  我之前為了簡化程式碼而寫的維度檢查邏輯不夠周全，錯誤地將 player_team 排除在維度提升之外。

  修正計畫：
  我將修改 Preprocessor 類別中的 modify_state 函數，採用更明確、更穩健的方式來確保所有要傳遞給 hstack
  的陣列都具有正確的 2D 形狀。

  我現在就來執行這個修正。
 ╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
 │ ✔ Edit training_scripts\ppo_with_pbrs.py: class Preprocessor:... => class Preprocessor:...                 │
 │                                                                                                             │
 │    26       return a - b + c                                                                                │
 │    27                                                                                                       │
 │    28   def modify_state(self, obs, info):                                                                  │
 │    29 -     if len(obs.shape) == 1:                                                                         │
 │    29 +     # Ensure obs is 2D                                                                              │
 │    30 +     if obs.ndim == 1:                                                                               │
 │    31           obs = np.expand_dims(obs, axis=0)                                                           │
 │    32                                                                                                       │
 │    32 -     # Ensure all info fields are correctly shaped                                                   │
 │    33 -     for key in info:                                                                                │
 │    34 -         if isinstance(info[key], (np.ndarray, list)) and len(np.array(info[key]).shape)             │
 │       == 1:                                                                                                 │
 │    35 -              if key not in ['task_index', 'player_team']: # These can be 1D                         │
 │    36 -                 try:                                                                                │
 │    37 -                     info[key] = np.expand_dims(np.array(info[key]), axis=0)                         │
 │    38 -                 except (ValueError, TypeError):                                                     │
 │    39 -                     pass # Ignore if not applicable                                                 │
 │    33 +     # List of all keys we expect to use from the info dict                                          │
 │    34 +     info_keys = [                                                                                   │
 │    35 +         "robot_quat", "robot_gyro", "robot_accelerometer", "robot_velocimeter",                     │
 │    36 +         "goal_team_0_rel_robot", "goal_team_1_rel_robot", "goal_team_0_rel_ball",                   │
 │    37 +         "goal_team_1_rel_ball", "ball_xpos_rel_robot", "ball_velp_rel_robot",                       │
 │    38 +         "ball_velr_rel_robot", "player_team", "goalkeeper_team_0_xpos_rel_robot",                   │
 │    39 +         "goalkeeper_team_0_velp_rel_robot", "goalkeeper_team_1_xpos_rel_robot",                     │
 │    40 +         "goalkeeper_team_1_velp_rel_robot", "target_xpos_rel_robot",                                │
 │    41 +         "target_velp_rel_robot", "defender_xpos"                                                    │
 │    42 +     ]                                                                                               │
 │    43                                                                                                       │
 │    44 +     # Ensure all required info arrays are 2D                                                        │
 │    45 +     for key in info_keys:                                                                           │
 │    46 +         if key in info:                                                                             │
 │    47 +             val = np.asarray(info[key])                                                             │
 │    48 +             if val.ndim == 1:                                                                       │
 │    49 +                 info[key] = np.expand_dims(val, axis=0)                                             │
 │    50 +                                                                                                     │
 │    51       task_onehot = self.get_task_onehot(info)                                                        │
 │    42 -     if len(task_onehot.shape) == 1:                                                                 │
 │    ═════════════════════════════════════════════════════════════════════════════════════════════            │
 │    52 +     if task_onehot.ndim == 1:                                                                       │
 │    53           task_onehot = np.expand_dims(task_onehot, axis=0)                                           │
 │    54                                                                                                       │
 │    55       robot_qpos = obs[:, :12]                                                                        │
 │    56       robot_qvel = obs[:, 12:24]                                                                      │
 │    57       quat = info["robot_quat"]                                                                       │
 │    58       base_ang_vel = info["robot_gyro"]                                                               │
 │    49 -     project_gravity = self.quat_rotate_inverse(quat, np.array([0.0, 0.0, -1.0]))                    │
 │    ═════════════════════════════════════════════════════════════════════════════════════════════            │
 │    59 +     # Ensure the vector for quat_rotate_inverse is also 2D                                          │
 │    60 +     gravity_vec_2d = np.array([[0.0, 0.0, -1.0]])                                                   │
 │    61 +     project_gravity = self.quat_rotate_inverse(quat, gravity_vec_2d)                                │
 │    62                                                                                                       │
 │    63       # Combine all state information                                                                 │
 │    64       processed_obs = np.hstack([                                                                     │
 ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
✦ 我已經修正了 ppo_with_pbrs.py 腳本中的 Preprocessor 類別。

  剛剛的 ValueError 錯誤已經被解決。新的程式碼會確保所有觀測狀態的組成部分在組合前都具有正確的維度。

  您可以再次嘗試執行訓練。

  請使用以下指令來啟動訓練：

   1 python training_scripts/ppo_with_pbrs.py --total_timesteps=500000 --n_envs=4

  您也可以像之前一樣，依需求自訂學習率、k1、k2 等參數。